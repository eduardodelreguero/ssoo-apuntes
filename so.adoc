== Introducción

=== Funciones del sistema operativo

Es habitual cuando hablamos de un elemento tan complejo como un sistema
operativo que resulte más sencillo definirlo por lo qué hace que por lo
qué es. Por ello, comenzaremos viendo el papel del sistema operativo en
el conjunto de un sistema informático.

Un *sistema informático* cualquiera puede ser dividido en cuatro
componentes (véase la ): el hardware, el sistema operativo, los
programas de aplicación y los usuarios.

* El hardware —la CPU, la memoria, los dispositivos de entrada salida,
etc.— proporcionan los recursos computaciones del sistema.

* Los programas de aplicación —procesadores de textos, hojas de cálculo,
compiladores, navegadores de Internet—. Definen las diferentes formas en
las que los recursos de sistema son utilizados para resolver los
problemas informáticos de los usuarios.

* El sistema operativo _controla y coordina el uso de hardware por
parte de las diversas aplicaciones para los diferentes usuarios del
sistema_. Un sistema operativo no hace trabajo útil. Simplemente
_proporciona un entorno adecuado para que otros programas puedan
hacerlo._

Con el fin de entender cuáles son las funciones de un sistema operativo;
se puede explorar su papel desde dos puntos de vista: el del sistema
informático y el del usuario.

==== Punto de vista del sistema informático

Desde el punto de vista del sistema informático, el sistema operativo es
el programa más íntimamente relacionado con el hardware. En este
contexto, _el sistema operativo actúa como:_

1. _Gestor de los recursos de sistema informático_.

2. _Programa encargado del control de la ejecución de los programas de
usuario y del acceso a los dispositivos de E/S._

Un sistema informático tiene múltiples recursos tanto hardware como
software —tiempo de CPU, espacio de memoria, espacio de almacenamiento
de archivos, dispositivos de E/S, servicios de red, etc.—. El sistema
operativo, como gestor de estos recursos, los asigna a los diferentes
programas, resolviendo los conflictos en las peticiones y haciendo que
el sistema opere eficientemente y resuelva los problemas de los
usuarios.

Además, como programa encargado del control de la ejecución de los
programas de los usuarios, el sistema operativo tiene la tarea de
prevenir errores y el uso inadecuado del ordenador.

==== Punto de vista del usuario

Sin embargo, la función del sistema operativo desde el punto de vista
del usuario varía de acuerdo con la interfaz utilizada. Por ejemplo, los
usuarios que se sientan frente a un sistema de escritorio (véase el
apartado <<_sistemas_de_escritorio>>) disponen de monitor, teclado, ratón y
una unidad central. Estos sistemas se diseñan buscando la máxima
productividad en equipos donde un usuario monopoliza todos los recursos;
por lo que el sistema operativo se diseña considerando fundamentalmente
la facilidad de uso, poniendo algo de atención en el rendimiento y nada
en el aprovechamiento de los recursos.

En otros casos un usuario se sienta frente a un terminalfootnote:[Los
terminales son sistemas informáticos utilizados para la conexión de los
usuarios a un _mainframe_. Sólo suelen disponer de los recursos
necesarios para realizar esa tarea.] conectado a un _mainframe_ (véase
el <<_mainframe>>), mientras muchos otros acceden al
mismo sistema a través de otros terminales. Por tanto, todos los
usuarios comparten los recursos del sistema informático y pueden
intercambiar información. En esos casos el sistema operativo debe
maximizar el aprovechamiento de los recursos con el objetivo de
garantizar que toda la CPU, memoria y E/S sean empleadas de forma
eficiente, y que ningún usuario utiliza más que lo que le corresponde.

Otro ejemplo son los sistemas de mano (véase el apartado
<<_sistemas_de_mano>>), por ejemplo _tablets_ y teléfonos móviles. A
causa de las limitaciones de la interfaz, en el diseño del sistema
operativo debe primar la usabilidadfootnote:[La usabilidad es la medida
de la facilidad de uso de un producto o servicio, típicamente una
aplicación software o un aparato. Más información en
http://es.wikipedia.org/wiki/Usabilidad.], aunque el rendimiento por
tiempo de vida de la batería también es importante.

=== Definición de sistema operativo

No existe una definición universal de lo qué es un sistema operativo.
Hay quién considera que es lo que el fabricante nos vende cuando decimos
que queremos comprar un sistema operativo. Esta definición no parece muy
precisa puesto que las características incluidas pueden variar
enormemente de un sistema a otro. Por ejemplo, algunos sistemas apenas
alcanzan el megabyte de espacio, careciendo incluso de las aplicaciones
más básicas, como puede ser un editor, mientras que otros ocupan
gigabytes de espacio y están completamente basados en sistemas gráficos
de ventanas.

Una definición mucho más común es que _el sistema operativo es aquel
programa que se ejecuta continuamente en el ordenador_ —lo que
denominamos comúnmente _**kernel**_ o *núcleo*— siendo todo lo demás
programas del sistema y aplicaciones. Sin embargo, es indudable que en
algunos casos ésta definición no incluye como parte del sistema
operativo algunas características que intuitivamente solemos considerar
dentro del mismo. Por ejemplo, si aplicamos esta definición a los
sistemas operativos de estructura microkernel (véase el apartado
<<_microkernel>>), no podríamos decir que servicios como la
comunicación en red, la gestión del sistema de archivos y la gestión de
la memoria (véanse el <<_componentes_del_sistema>>) son proporcionados
por el sistema operativo. Aunque pueda parecer lo contrario, la cuestión
de qué incluye y qué no incluye un sistema operativo no carece de
importancia, como se demostró durante el caso del Departamento de
Justicia de los Estados Unidos contra Microsoft por la excesiva
inclusión de funcionalidades en sus sistemas operativosfootnote:[Más
información sobre el caso en http://goo.gl/u1tf.].

Parece evidente que un sistema operativo se define mejor por lo qué
«hace» —es decir, sus funciones— que por lo qué «es». Sin embargo, ésta
primera manera de definirlo también tiene sus dificultades. Por ejemplo,
el principal objetivo de los sistemas operativos de escritorio es la
facilidad de uso, mientras que en los _mainframe_ el objetivo
fundamental es la eficiencia en el aprovechamiento de los recursos.
Puesto que ambos objetivos pueden ser en ocasiones contradictorios,
resulta obvio que lo que tiene que «hacer» un sistema operativo para
alcanzar esos objetivos puede ser diferente en cada caso, lo que
dificulta el obtener una definición única.

_De lo que no hay duda es de que los sistemas operativos existen porque
es más fácil hacer un sistema informático usable con ellos que sin
ellos_. El objetivo fundamental de las computadoras es ejecutar
programas para resolver fácilmente los problemas de los usuario, por lo
que con ese objetivo se construye el hardware de los ordenadores. Puesto
que el hardware por si sólo resulta difícil de utilizar, es necesario
desarrollar programas de aplicación para que sean utilizados por los
usuarios. Sin embargo, estas aplicaciones necesitan realizar operaciones
comunes, como controlar los dispositivos de E/S o reservar porciones de
la memoria. Esas _funciones comunes de control y asignación de
recursos, que se corresponden con las funciones del sistema operativo
desde el punto de vista del sistema informático vistas en el
<<_punto_de_vista_del_sistema_informático>>, son la labor del sistema operativo_.

=== Tipos de sistemas operativos

==== Mainframe

Los *ordenadores centrales* o _**mainframes**_ fueron los primeros
computadores utilizados en muchas aplicaciones comerciales y
científicas. Se caracterizan no tanto por la potencia de su
CPUfootnote:[Generalmente se considera que las mayor diferencia entre
los superordenadores y los mainframes está en que los primeros se
centran en resolver problemas limitados por la velocidad de cálculo —lo
cual requiere miles de CPU de alto rendimiento— mientras que lo segundos
se centran en problemas limitados por la E/S y la fiabilidad —sólo
necesitan entre una y varias docenas de CPU—. Más información en
http://es.wikipedia.org/wiki/Ordenador_central.] como por: su gran
capacidad de memoria, su gran capacidad de almacenamiento secundario, la
gran cantidad de dispositivos de E/S y la rapidez de estos, así como por
su alta fiabilidad. Estas máquinas pueden funcionar durante años sin
problemas ni interrupciones y las reparaciones se realizan sin detener
su funcionamiento.

===== Sistemas de procesamiento por lotes

Los *sistemas de procesamiento por lotes* o *sistemas en __batch__*
fueron los primeros ordenadores (véase el apartado
<<_mainframe>>). Eran enormes máquinas operadas desde una consola
y conectados a lectores de tarjetas perforadasfootnote:[Mas información
sobre la forma de trabajo con tarjetas perforadas en
http://goo.gl/S9FTOk.], dispositivos de cinta e impresoras. El trabajo,
normalmente en tarjetas perforadas, era preparado por el programador y
entregado al operador. Para acelerar la ejecución el operador debía
agrupar los programas con requerimientos similares en lotes y mandarlos
a ejecutar según iba quedando disponible el ordenador. Finalmente, el
resultado de cada programa debía ser devuelto al programador
correspondiente.

* El sistema operativo permanecía siempre residente en memoria (véase la).

* La _única tarea del sistema operativo era transferir
automáticamente el control de un trabajo al siguiente_.

* _El mayor inconveniente de éste tipo de sistemas era que la CPU
permanecía mucho tiempo desocupada_ porque era y es varios ordenes de
magnitud más rápida que la E/S. Los programas necesitan realizar
operaciones de E/S para obtener los datos requeridos para sus cálculos
—por ejemplo guardados en tarjetas perforadas— por lo que se pierde
mucho tiempo esperando a que estén disponibles dichos datos.

===== Sistemas multiprogramados

Con la aparición de la tecnología de los discos magnéticos se pudo
mantener todos los trabajos en el disco y no en tarjetas perforadas
sobre la mesa del operador. Esto permitió que el sistema operativo
pudiera encargarse de escoger el orden de ejecución de los trabajos.

1. En disco se almacena una cola con todos los trabajos que deben ser
ejecutados.

2. El sistema operativo mantiene varios trabajos en memoria del
conjunto de trabajos en la cola en disco (véase la ).

3. El sistema operativo ejecuta en la CPU unos de los trabajos en
memoria.

4. Si el trabajo en la CPU requiere E/S, en lugar de mantener a la CPU
ocupada inútilmente, el sistema operativo escoge otro trabajo de entre
los que están en memoria y lo ejecuta en la CPU. _El nuevo programa en
la CPU no es interrumpido cuando el anterior termina de utilizar la
E/S_, sino que éste último debe esperar en la memoria una nueva
oportunidad para ser escogido.

5. Cuando un programa en la CPU termina, un hueco queda libre en la
memoria. Por lo tanto es necesario que el sistema operativo escoja un
trabajo de la cola en disco y lo cargue en la memoria.

6. El proceso se repite mientras hayan trabajos que ejecutar.

Para seguir un esquema como el anterior es necesario que el sistema
operativo realice tres tareas esenciales:

* La *planificación de trabajos*. Su responsabilidad es elegir cuál es
el siguiente trabajo que debe ser cargado para mantener llena la
memoria.

* La *planificación de la CPU*. Se encarga de elegir el siguiente
trabajo que debe ser ejecutado en la CPU de entre los disponibles en la
memoria (véase el <<_planificación_de_la_cpu>>).

* La *gestión de la memoria*. Es necesaria puesto que la memoria tiene
que ser repartida entre los trabajos que deben ser alojados en la misma
(véase el <<_componentes_del_sistema>>).

Un ejemplo de este tipo de sistemas operativos es el IBM OS/360 que fue
liberado en 1966 para utilizarlo en los _mainframes_ IBM System/360
(véase el <<segunda_generación>>).

===== Sistemas de tiempo compartido

La mayor parte de los sistemas actuales son *sistemas de tiempo
compartido*. Los sistemas anteriores ofrecían un uso eficiente de la
CPU pero no eran capaces de proporcionar interacción con el usuario. El
usuario se limitaba a entregar los trabajos al operador y a esperar a
que éste le devolviera los resultados.

Los sistemas de tiempo compartido se caracterizan por tener:

* Un sistema de interacción directa entre el usuario y el sistema. Por
ejemplo, un terminal.

* Un sistema multiprogramado dónde la conmutación es tan frecuente que
el usuario puede interactuar con cada programa mientras se ejecuta.

Utilizando esta estrategia un sistema de tiempo compartido puede
disponer de varios terminales de forma que múltiples usuarios puedan
utilizar la máquina simultáneamentefootnote:[A los sistemas que tienen
esta funcionalidad se los denomina sistemas multiusuario.]. Los usuarios
comparten la CPU y los otros recursos del sistema, sin embargo, la
sensación para cada uno es la de que el sistema completo está dedicado a
él en exclusiva. Realmente el sistema conmuta de un usuario a otro —o
para ser exactos de un programa a otro, pudiendo ser de usuarios
distintos— pero debido a la lentitud de la E/S interactivafootnote:[La
E/S interactiva incluye la salida de datos por pantalla y la entrada de
datos utilizando dispositivos como el teclado, el ratón, etc. La
velocidad de este tipo de E/S viene limitada por las capacidades
humanas, por lo que hay que tener en cuenta que lo que para los humanos
es rápido para una CPU resulta sumamente lento.] los usuarios no
perciben demora alguna.

Los sistemas de tiempo compartido significaron un salto importante en
complejidad por diversas razones:

* _Varios trabajos deben estar en memoria al mismo tiempo_ => el sistema
operativo requiere mecanismos de *gestión de la memoria* y *protección*
(véase el <<_componentes_del_sistema>>).

* _Para tener un tiempo de respuesta razonable los trabajos en memoria
deben poder ser guardados o recuperados desde el disco que sirve como
almacenamiento de respaldo_ => el sistema operativo puede utilizar
técnicas de *memoria virtual* (véase el <<_memoria_virtual>>)
para poder ejecutar trabajos que no están completamente cargados en
memoria.

* _La CPU debe ser compartida entre los trabajos_ => el sistema operativo
requiere mecanismos de *planificación de la CPU* (véase el apartado
<<_planificación_de_la_cpu>>).

* _La ejecución de los trabajos debe ser ordenada_ => el sistema
operativo debe proporcionar mecanismos de *sincronización* (véase el
apartado <<_sincronización>>) y *comunicación* (véase el apartado
<<_procesos_cooperativos>>).

* _El sistema debe disponer de un **sistema de archivos**_ (véase el tema
<<_gestión_del_almacenamiento>>), que a su vez debe residir en un conjunto de discos
=> el sistema operativo requiere mecanismos de *gestión de discos*.

Las primeras versiones de UNIX —liberado por primera vez en 1970— el
sistema operativo VMS —desarrollado en 1978— para los VAX de Digital
Equipment Corportation y el IBM OS/400 —introducido en 1988— utilizado
en los minicomputadores AS/400, son algunos ejemplos de sistemas
operativos de tiempo compartido (véase el apartado
<<tercera_generación>>).

==== Sistemas de escritorio

Los *sistemas de escritorio* aparecieron en los primeros años de la
década de 1970 y carecían de las características necesarias para ser
multiusuario y multitarea. A diferencia de los sistemas de entonces, los
sistemas operativos de escritorio actuales si tienen esas
características pero se siguen diseñando con un objetivo diferente al de
los _mainframe_. Como ya hemos comentado, mientras que en los sistemas
de tiempo compartido y los multiprogramados se persigue maximizar la
utilización eficiente de los recursos, en _los sistemas de escritorio
se debe maximizar la respuesta al usuariofootnote:[El tiempo de
respuesta al usuario se puede considerar como el intervalo de tiempo
entre un comando de un usuario —por ejemplo un click— y la respuesta del
sistema a dicho comando. En ocasiones este tiempo se minimiza a costa de
un uso menos eficiente de los recursos del sistema por lo que no es un
objetivo deseable para diseñar un mainframe. Mas información en el tema
<<_criterios_de_planificación>>.] y la facilidad de uso_.

Pese a estas diferencias _los sistemas operativos de escritorio se han
beneficiado del desarrollo de los sistemas operativos para mainframes_.
Por ejemplo, en un sistema diseñado para ser utilizado por un único
usuario no tiene sentido implementar un sistema de archivos con
permisos. Por eso los primeros sistemas operativos de escritorio
carecían de esta característica, que ya existía en los _mainframe_ de la
época. Sin embargo, hoy en día los sistemas de escritorio son
multiusuario e incluyen sistemas de archivos con permisos como medida de
protección de los datos de los usuarios.

Los ejemplos de este tipo de sistemas operativos van desde CP/M —lanzado
en 1977— hasta los actuales GNU/Linux, Microsoft Windows 7 y Apple
macOS, pasando por MS-DOS, IBM OS/2 y las diversas versiones de
Microsoft Windows (véase el <<cuarta_generación>>).

==== Sistemas distribuidos

En la actualidad es común el uso de redes —por ejemplo Internet o la red
de área local de una oficina— para interconectar ordenadores
individuales; cada uno equipado con su procesador, su memoria, sus
dispositivos de almacenamiento, su fuente de alimentación, etc. _En las
redes de ordenadores los procesadores de dichos ordenadores se comunican
con otros procesadores a través de líneas de comunicación_, como redes
Ethernet o líneas telefónicas. Estos sistemas son comúnmente denominados
*sistemas distribuidos*.

===== Tipos de sistemas informáticos distribuidos

Sin entrar en detalles los sistemas distribuidos pueden ser clasificados
en dos grandes tipos:

* _En los *sistemas cliente-servidor* existen ordenadores que
actúan como *servidores* encargados de satisfacer las peticiones
generadas por otros ordenadores que actúan como **clientes**_. Este
tipo de sistemas ha ido sustituyendo a los terminales conectados a
_mainframes_ debido a que los sistemas de escritorio son cada vez más
potentes y más baratos. Concretamente, los terminales han sido
sustituidos por los sistemas de escritorio que, al disponer de más
recursos, son capaces de realizar muchas de las funcionalidades que
anteriormente eran manejadas directamente por los _mainframes_. Al
mismo tiempo estos _mainframes_ se han reemplazado por servidores, no
muy diferentes a los sistemas de escritorios, pero preparados para
atender las peticiones de sus clientes. Ejemplos de este este tipo de
sistemas son los servidores de base de datos, que responden a las
consultas SQL de los clientes, o los servidores de archivos, que
proporcionan una interfaz de sistema de archivos con la que los clientes
pueden crear, leer, escribir y borrar archivos en el servidor.

* _En los *sistemas de redes entre iguales* o *P2P* (peer-to-peer)
clientes y servidores no se distinguen los unos de los otros. Todos
los nodos del sistema son iguales y cada uno puede actuar como cliente
y/o servidor dependiendo de cuándo piden o proporcionan un
servicio._ La ventaja fundamental de este tipo de sistemas es que en los
sistemas cliente-servidor el servidor es el cuello de
botellafootnote:[Un servidor puede ser el cuello de botella no solo por
su potencia sino también por el ancho de banda de su conexión a la red.
La potencia del servidor es lo de menos cuando se intenta distribuir en
Internet archivos de gran tamaño —por ejemplo imágenes de CD o DVD— pues
el problema es que varias descarga simultaneas pueden consumir todo el
ancho de banda del servidor durante largos periodos de tiempo. ], pero
en los sistemas de redes entre iguales la carga se distribuye entre los
diversos nodos de la red. Ejemplos de este tipo de sistemas son las
redes eDonkey y BitTorrent.

===== Sistemas operativos para sistemas distribuidos

Desde el punto de vista de los sistemas operativos para sistemas
distribuidos es necesario hacer la siguiente distinción:

* _Los *sistemas operativos de red* ofrecen a las aplicaciones que
corren sobre ellos servicios de acceso a redes de ordenadores._ Por
ejemplo, implementan algún mecanismo que permita a diferentes procesos
en diferentes ordenadores intercambiar mensajes. Además suelen
incorporar la opción de proporcionar algunos servicios de red, como la
compartición de archivos y dispositivos. Los ordenadores con sistemas
operativos de red son autónomos, aunque conocen la existencia de la red
y están en disposición de comunicarse con otros ordenadores de la misma.
Este tipo de sistemas operativos son los más utilizados en los tipos de
sistemas distribuidos comentados anteriormente.

* _Los *sistemas operativos distribuidos* crean en el usuario la
ilusión de estar en un sólo ordenador_, aunque en realidad el sistema
operativo controla todos los ordenadores de la red dando al usuario
acceso transparente a los recursos en todos los equipos de la misma. Con
este tipo de sistemas operativos el usuario no sabe en qué ordenador se
ejecutan sus procesos, ni dónde se almacenan sus archivos, ni qué equipo
tiene conectado los distintos periféricos a los que tiene acceso. Un
ejemplo de sistema operativo distribuido es __Amoeba__footnote:[Amoeba es
un sistema operativo de investigación distribuido de estructura microkernel (véase el <<_microkernel>>) escrito por Andrew S. Tanenbaum en Vrije Universiteit. Más información en http://www.cs.vu.nl/pub/amoeba/.].

==== Sistemas de tiempo real

Se utilizan cuando tenemos requerimientos rígidos de tiempo en la
ejecución de las tareas o en el procesamiento de flujos de datos. Por lo
tanto, se usa frecuentemente en dispositivos de control dedicados a una
tarea específica; dónde se deben tomar datos de uno o varios sensores,
para posteriormente analizar dichos datos y accionar algún mecanismo de
control dentro de unos márgenes rígidos de tiempo. Los sistemas de
tiempo real se suelen utilizar en: algunos sistemas de control
industrial, domótica, armamento, la inyección electrónica de combustible
en los automóviles, el procesamiento de imágenes médicas, etc..

Los sistema de tiempo real están muy relacionados con los _sistemas
empotrados_. Estos sistemas están tanto en el motor de los automóviles
y los robots que los fabrican, como en reproductores de DVD, microondas,
etc. Los sistemas empotrado realizan tareas muy específicas, sus
sistemas operativos tienen características muy limitadas y no suelen
tener interfaz de usuario.

Los sistemas de tiempo real pueden ser clasificados en _sistemas de
tiempo real estricto_ y _sistemas de tiempo real flexible_:

* _Los *sistemas de tiempo real estricto* o *hard real-time*
garantizan que las tareas serán realizadas dentro de unos márgenes
estrictos de tiempo_. Para ello todos los imprevistos que puedan
ocasionar retardos en el funcionamiento del sistema operativo deben
estar perfectamente limitados en tiempo. Por lo tanto, la memoria
virtual y otras facilidades que abstraen del funcionamiento real del
hardware no están presentes en este tipo de sistemas porque introducen
impredecibilidad. Los sistemas de tiempo real estricto no son
compatibles con los sistemas de tiempo compartido.

* _Los *sistemas de tiempo real flexible* o *soft real-time* son
útiles cuando hay tareas que tienen mayor importancia que el resto por
lo que deben ser realizadas con mayor prioridad y esta prioridad debe
ser conservada hasta que terminan_. El tiempo real flexible no sirve
cuando se tienen tareas con limitaciones precisas de tiempo porque no
hay manera de garantizar que dichas restricciones se van a cumplir. Sin
embargo si es útil para tareas relacionadas con la multimedia, la
realidad virtual, etc. Este tipo de tiempo real está disponible en la
mayor parte de los sistemas operativos de propósito general pues es
compatible con la memoria virtual y otras facilidades propias de los
sistemas de tiempo compartido.

==== Sistemas de mano

Los sistemas de mano incluyen a los _tablets_, lectores de libros
electrónicos y teléfonos móviles. Los desarrolladores de sistemas de
mano y aplicaciones para estos sistemas deben enfrentarse a diversos
desafíos. Muchos de ellos vienen originados por el tamaño limitado de
los dispositivos y la alimentación mediante el uso de baterías. Debido a
esas limitaciones muchos sistemas de mano tienen poca cantidad de
memoria, procesadores lentos y pantallas pequeñas.

=== Historia de los sistemas operativos

La historia de los sistemas operativos se puede dividir en 5 grandes
etapas o generaciones.

[[primera_generación]]
==== 1ª Generación (1945-55)

===== Características

* Sin sistema operativo.
* Sólo hardware, sin lenguajes de programación.

===== Ejemplos

* _Mainframe_ IBM 701 y 704.

[[segunda_generación]]
==== 2ª Generación (1955-64)

===== Características

* Sistemas operativos de procesamiento por lotes.
* Sistema operativo básico. Se utilizan lenguajes de programación.

===== Ejemplos

* El primer sistema operativo fue desarrollado por General Motors
Research Laboratory en 1956 para su _mainframe_ IBM 701 (véase la ) con
el fin de automatizar la carga de los trabajos.

[[tercera_generación]]
==== 3ª Generación (1965-1968)

===== Características

* Sistemas operativos multiprogramados.
* Más lenguajes de programación y multiprogramación.

===== Ejemplos

* *IBM OS/360*. Desarrollado por IBM para su _mainframe_ System/360.

    ** Fue el primero en hacer los dispositivos de almacenamiento de acceso aleatorio un requisito para poder operar.

    ** Anunciado en 1964, fue liberado en 1966 con un año de retraso. Los motivos fundamentales fueron ciertos problemas de organización interna de la compañía y la falta de experiencia en proyectos de tal envergadura, pues las previsiones iniciales eran de 1 millón de líneas de código y miles de componentes de software. La experiencia negativa del desarrollo del IBM OS/360 condujo al nacimiento de la _ingeniería del software_.

[[cuarta_generación]]
==== 4ª Generación

Esta generación abarca desde mediados de los años 60 hasta finales de la
década de los 70.

===== Características

* Sistemas operativos de tiempo compartido.
* Aparecen los programas interactivos y las máquinas virtuales.

===== Ejemplos

* *MULTICS*. Fue anunciado en 1964 como el primer sistema operativo de
propósito general fruto de la colaboración entre el MIT, General
Electrics y Bell Labs (véase la ).

    ** Primer sistema operativo en proporcionar un sistema de archivos
jerárquico, un intérprete de comandos implementado como programa de
usuario, listas de control de acceso individuales para cada archivo,
enlazado dinámico, etc.

    ** Eliminó la separación entre el espacio de direcciones de los procesos
y los archivos. En un sistema moderno eso sería como si cada archivo
estuviera mapeado en memoria (véase el <<_archivos_mapeados_en_memoria>>).

* *VM/CMS*. Es un sistema de IBM utilizado en los _mainframe_
System/360, System/370, System/390 y zSeries.

    ** El desarrollo comenzó en 1965 y la primera versión estuvo disponible
a primeros de 1966.

    ** VM es una máquina virtual que proporciona a cada usuario la sensación
de tener su propio _mainframe_ personal.

    ** CMS es un sistema monousuario diseñado para operar fundamentalmente
encima de VM.

* *UNIX*. Desarrollado originalmente por Bell Labs en 1970 para los
sistemas PDP-11/20.

    ** La autoría del mismo se le atribuye a un grupo de programadores, liderados por Ken Thompson, que decidieron rehacer el trabajo de MULTICS pero a menor escala después de que Bell Labs abandonara el proyecto en 1969. Inicialmente se llamó UNICS y fue desarrollado para los sistemas PDP-7 (véase la ).

    ** La primer versión, como muchos otros sistemas operativos anteriores,
estaba implementada en ensamblador. Dennis Ritchie y Brian Kernighan
diseñaron un nuevo lenguaje llamado «C» especialmente pensado para que
UNIX fuera escrito con él. Eso permitió que UNIX pudiera ser modificado
fácilmente para funcionar en otros ordenadores. Además el código era más
conciso y compacto, lo que se tradujo en el aumento de la velocidad de
desarrollo de UNIX.

    ** AT&T, la compañía matriz de Bell Labs, no podía competir en la
industria de los ordenadores por lo que puso el código fuente de UNIX a
disposición de universidades, compañías privadas y del gobierno de los
Estados Unidos.

    ** Una de las más importantes versiones de UNIX fue desarrollada por la
Universidad de California en Berkeley. Esta versión implementaba el
estándar de comunicaciones TCP/IP, el cual permitió convertir la cerrada
ARPANET en la abierta Internet.

    ** En la actualidad se puede considerar que hay dos grandes familias de
UNIX. Por un lado AT&T UNIX System V, del que derivan sistemas tales
como SCO OpenServer, Oracle/Sun Microsystems Solaris Operating
Environment y SCO UnixWare. Y por el otro, BSDfootnote:[La siglas BSD provienen de Berkeley Software Distribution.] del que derivan FreeBSD, NetBSD, OpenBSD, Darwin y DragonFly BSD, entre muchos otros.

* *VMS*. Es un sistema operativo diseñado originalmente por —ahora propiedad de HP— en 1978 para operar en sistemas VAX. Posteriormente fue portado a sistemas DEC Alpha
e Intel Itanium.

    * *IBM OS/400*. Es un sistema utilizado en la familia IBM AS/400 —ahora llamada iSeries—.

    ** OS/400 y AS/400 fueron introducidos en el mercado en 1988.

    ** La familia IBM AS/400 es una familia de minicomputadores. Este
termino en desuso hace referencia a máquinas multiusuario de rango
medio, entre los _mainframes_ y los sistemas de escritorio.

==== 5º Generación (años 1980, 1990 y 2000):

Esta generación abarca desde la década de los 80 hasta la actualidad.

===== Características

* Sistemas operativos de escritorio y ordenadores personales
(PC)footnote:[Se puede observar una muestra de la interfaz gráfica de
usuario de algunos estos sistemas en https://goo.gl/rDNHeS.].

* Monousuario, multitarea, sistemas distribuidos, sistemas paralelos,
sistemas de tiempo real, etc.

===== Ejemplos

* *CP/M*. Sistema operativo estándar para la primera generación de
microcomputadoresfootnote:[Una microcomputadora es un ordenador que
tiene un microprocesador. La primera generación de microcomputadoras
también fue conocida como computadoras domésticas.].

    ** Creado por Digital Research, Inc., fundada por Gary Kildall, para ser
el sistema operativo de los microordenadores basados en Intel 8080/85 y
Zilog Z80.

    ** La combinación del CP/M junto al bus S-100 en el MITS Altair
8800footnote:[El MITS Altair 8800 fue un microcomputador diseñado en
1975 basado en el procesador Intel 8080A. Hoy en día es considerado el
primer ordenador personal de la historia. Su bus de sistema, el S-100,
se convirtió en un estándar de facto y su primer lenguaje de
programación fue el producto que ayudó a fundar Microsoft, el Altair
BASIC.] fue el primer estándar industrial.

    * *MS-DOS*. Sistema operativo estándar para la segunda generación de
microcomputadores.
** Fue el primer sistema operativo del IBM PC —lanzado en 1981— y
durante mucho tiempo fue ampliamente utilizado en la plataforma PC
compatible. No era ni multitarea ni multiusuario.
** MS-DOS fue creado por Seattle Computer Products con el nombre de
86-DOS, pero era comúnmente conocido como QDOS (Quick and Dirty
Operating System). Microsoft adquirió el sistema y lo vendió a IBM con
el nombre de MS-DOS.

    ** Tanto IBM como Microsoft lanzaron versiones de DOS, aunque
originalmente IBM solamente validaba y empaquetaba el software de
Microsoft. Microsoft liberaba sus versiones bajo el nombre de «MS-DOS»,
mientras IBM las liberaba bajo el nombre de «PC-DOS».
* *OS/2*. Sistema operativo creado por Microsoft e IBM y
posteriormente desarrollado por IBM en exclusiva. Se creó como el
sistema operativo predilecto para la segunda generación de ordenadores
personales de IBM, equipados con procesador Intel 80286.

    ** OS/2 fue pensado como un sucesor con operación en modo dual (véase el
apartado <<_operación_en_modo_dual>>) de MS-DOS y Microsoft Windows 2.0.
** OS/2 1.0 fue anunciado en abril y liberado en diciembre de 1987 como
un sistema operativo en modo texto. La interfaz gráfica de usuario
prometida —denominada Presentation Manager— se introdujo en la versión
1.1 en noviembre de 1988.

    ** La colaboración entre IBM y Microsoft terminó en 1990 entre la
liberación de Windows 3.0 y la de OS/2 1.3. El aumento de popularidad de
Windows llevo a Microsoft a dejar de centrarse en el desarrollo de OS/2,
lo que llevó a IBM a preocuparse por los continuos retrasos en el
desarrollo de OS/2 2.0. Inicialmente ambas compañías acordaron que IBM
tomaría el mantenimiento de OS/2 1.0 y el desarrollo de OS/2 2.0,
mientras Microsoft continuaría desarrollando OS/2 3.0, que entonces era
conocido como «NT OS/2». Sin embargo, finalmente Microsoft decidió
renombrar NT OS/2 como Windows NT, dejando el futuro desarrollo de OS/2
en manos de IBM.

    ** OS/2 Warp 3, liberado en 1994, fue un sistema completo de 32-bit.

    ** OS/2 Warp 4, fue liberado en 1996. Poco después de su lanzamiento IBM
anunció que OS/2 desaparecería.

* *Windows 3.x*. La familia Windows 3.x de Microsoft Windows fue
desarrollada desde 1990 hasta 1994. La 3.0 fue la primera versión de
éxito de Windows, permitiendo a Microsoft competir con el Macintosh de
Apple Computer y el Commodore Amiga.

    ** En 1983 Microsoft anuncia el desarrollo de Windows, una interfaz
gráfica de usuario para su propio sistema MS-DOS, que estaba disponible
para los IBM PC y compatibles desde 1981.

    ** Windows 3.x requería una instalación previa de MS-DOS y era iniciado
como un programa más, que podía ser terminado en cualquier momento
devolviendo al usuario a la linea de comandos del MS-DOS. Este sistema
operativo le proporcionaba a Windows controladores de dispositivo para
ciertas tareas, como el acceso al CD-ROM o a la interfaz de red. Sin
embargo, Windows necesitaba de aplicaciones especificas, almacenadas en
un formato ejecutable mucho más complejo que el de los programas de
MS-DOS. Además, debido a que MS-DOS no aislaba a las aplicaciones del
hardware y no se protegía así mismo de los errores en dichas
aplicaciones, Windows disponía de múltiples controladores de dispositivo
propios, así como su propio sistema de gestión de la memoria. Es decir,
que Windows realmente no se ejecutaba sobre MS-DOS sino que hacía uso de
él. Por ello puede ser considerado un sistema operativo.

* *Windows 95, 98, Me*. Sistemas operativos híbridos gráficos de
16-bit/32-bit sucesores de Windows 3.x.

    ** Windows 95, liberado en 1995, fue el primer Windows unido a una
versión de MS-DOS específica; aunque este hecho se intentaba mantener
oculto. Entre las características de Windows 95 se pueden destacar:
mejoras significativas en la interfaz de usuario, nombres de archivo de
hasta 256 caracteres con conservación de mayúsculas y minúsculas y
multitarea expropiativa (véase el <<_planificación_expropiativa>>) para
las aplicaciones de 32-bit.

    ** Windows 98 fue liberado el 25 de junio de 1998.

    ** Windows Me, liberado el 14 de septiembre de 2000, fue la última
versión de la familia de sistemas operativos híbridos de 16-bit/32-bit
que sucedió a Windows 3.1.

* *Windows NT*. Sistema operativo de 32-bit antecesor del actual
Windows 7.

    ** Su desarrollo empezó en 1988 con el nombre de OS/2 3.0. Cuando
Windows 3.0 fue liberado en mayo de 1990 tuvo tanto éxito que Microsoft
decidió cambiar la APIfootnote:[Una interfaz de programación de
aplicaciones o API (del inglés _application programming interface_) es
el conjunto de funciones, procedimientos o métodos que ofrece el sistema
operativo para ser utilizado por las aplicaciones.] del aún en
desarrollo NT OS/2 —como era conocido en la época— pasando de ser una
versión extendida de la API de OS/2 a una versión extendida de la API de
Windows. Esta decisión causó tensión entre Microsoft e IBM y provocó que
finalmente la colaboración terminara.

    ** Microsoft contrató a un grupo de desarrolladores de Digital Equipment
Corporation para crear Windows NT, por lo que muchos de sus elementos
reflejan la experiencia anterior de DEC en VMS.

    ** Las API soportadas por Windows NT —por ejemplo Win32, POSIX y OS/2
2.1— son implementadas como subsistemas encima de un API nativo
públicamente no documentado. Esta estructura en subsistemas fue lo que
permitió la adopción tardía de la API de Windows, tal y como hemos
comentado anteriormente.

    ** Windows NT 3.1 —la primera versión de Windows NT, liberada el 13 de
julio de 1993— era un sistema operativo microkernel (véase el apartado
<<_microkernel>>) multiplataforma que corría sobre procesadores
Intel IA-32, DEC Alpha, MIPS R4000 y PowerPC.

    ** Windows NT 4.0 fue la última versión en soportar plataformas
distintas a Intel IA-32. Aunque el desarrollo de Windows 2000 para Alpha
continuó hasta 1999, cuando Compaq dejó de soportar Windows NT en esa
arquitectura. Además Windows NT 4.0 integró en el núcleo más funciones
—por ejemplo parte del subsistema gráfico— para obtener mayor
rendimiento.

* *Windows 2000, XP, Vista, 7*. Sistemas operativos sucesores de
Windows NT.

    ** Windows 2000 —o Windows NT 5.0, liberado el 17 de febrero de 2000—
fue el primer sistema operativo de la familia NT al que se le eliminaron
las siglas del nombre por motivos de marketing. El objetivo era
favorecer la unificación de las dos familias de sistemas operativos
Windows —Windows 9x y Windows NT— alrededor de la tecnología NT.

    ** Windows XP —o Windows NT 5.1— completó el
proceso de unificación de las dos familias de sistemas operativos
Windows, forzando la extinción de la familia Windows 9x al sustituirla
con una versión de Windows XP, denominada Windows XP Home Edition,
específica para la informática doméstica.

* *GNU/Linux*. Se trata del más famoso ejemplo de software libre y de
desarrollo de fuente abierta.

    ** El proyecto GNU se inició en 1983 con el fin de desarrollar un
sistema operativo estilo UNIX, incluyendo herramientas de desarrollo de
software y aplicaciones de usuario, hecho enteramente de software libre.

    ** El núcleo Linux fue inicialmente escrito como hobby por el estudiante
universitario finés Linus Torvalds mientras estudiaba en la Universidad
de Helsinki. Torvalds originalmente usaba Minix, un sistema operativo
simplificado escrito por Andrew Tanenbaum para enseñar diseño de
sistemas operativos. Sin embargo, el hecho de que Tanenbaum no diera
soporte a las mejoras de su sistema operativo introducidas por otros
desarrolladores, llevó a Torvalds a escribir un sustituto de Minix.

    ** En 1991, cuando se liberó la primera versión del núcleo Linux, el
proyecto GNU había desarrollado todos los componentes necesarios del
sistema excepto el núcleo. Torvalds y otros desarrolladores rápidamente
adaptaron Linux para que funcionara con los componentes de GNU, creando
un sistema operativo completamente funcional.

    ** El núcleo fue licenciado bajo la GNU General Public License (GPL)
pero no es parte del proyecto GNU. El proyecto GNU tiene su propio
kernel denominado Hurd, pero sigue en desarrollo.

* *Mach*. Es un núcleo de sistema operativo desarrollado en la
Universidad Carnegie-Mellon (CMU). El proyecto en CMU se desarrolló
desde 1985 hasta 1994.

    ** Mach explora el concepto que denominamos como microkernel (véase el
apartado <<_microkernel>>).

    ** En algún momento se pensó que Mach podría dominar el universo de los
sistema operativos debido a las ventajas de los sistemas microkernel. El
mayor esfuerzo para conseguirlo hasta la fecha es GNU/Hurd pero lleva
más de una década de retraso. Sin embargo, otros sistemas operativos
microkernel han tenido más éxito, como es el caso de QNX.

    ** Apple Computers seleccionó OpenStep como base para el sucesor de su
clásico Mac OS. OpenStep es realmente una versión actualizada de
NeXTSTEP, que era un sistema basado en un núcleo Mach 2.5 con porciones
del sistema BSD de la Universidad de Berkeley. Por lo tanto, la mezcla
de Mach con BSDfootnote:[A la base del sistema operativo macOS se la
denomina Darwin. Concretamente se trata de un sistema FreeBSD portado
para correr sobre el núcleo Mach.] de OpenStep es la base del sistema
operativo macOS de Apple.

== Estructura de los sistemas operativos

=== Organización de los sistemas operativos

El estudio de la organización interna de los sistemas operativos
requiere del análisis de tres aspectos diferentes:

1.  Los componentes del sistema
operativo y sus interconexiones (véase el apartado
<<_componentes_del_sistema>>).

2.  Los servicios que el sistema operativo proporciona a través del
funcionamiento coordinado de dichos componentes (véase la ).

3.  La interfaz de programación que el sistema operativo ofrece a
usuarios y programadores como forma de acceso a dichos servicios.

El estudio de los componentes del sistema operativo lo dejaremos para
más adelante, tras ver la forma usual en la que los programas acceden a
los servicios del sistema operativo y, por tanto, en la que se comunican
indirectamente con dichos componentes. Respecto a los servicios que el
sistema operativo proporciona, no entraremos en ello puesto que cada uno
ofrece servicios diferentes, aunque siempre es posible identificar unos
pocos tipos comunes a todos.

==== Interfaz de programación de aplicaciones

Un sistema operativo proporciona un entorno controlado para la ejecución
de programas. Dicho entorno debe proporcionar ciertos servicios que
pueden ser accedidos por los programas a través de una *interfaz de
programación de aplicaciones* o *API* (_Application Programming
Interface_). Algunas de las API disponibles para los desarrolladores de
aplicaciones son la API Win32 —en sistemas Microsoft Windows— y la API
POSIX para sistemas compatibles POSIXfootnote:[POSIX (_Portable
Operating System Interface for Unix_) es el nombre de una familia de
estándares que definen una interfaz de programación de aplicaciones para
sistemas operativos. Esto permite que un mismo programa pueda ser
ejecutado en distintas plataformas, siempre que sean compatibles con
POSIX. La práctica totalidad de los sistemas UNIX modernos son
compatibles POSIX ya que la especificación deriva de la interfaces
típicas en ese tipo de sistemas antes de la aparición del estándar
POSIX.] —como es el caso de los diferentes UNIX, Linux y macOS—.

Concretamente, _junto a cada intérprete o compilador de un lenguaje
de programación suele ir una *librería estándar* que ofrece clases y/o
funciones con las que los programas pueden acceder a los servicios del
sistema operativo y realizar las tareas más comunes_. Estas librerías
generalmente no forman parte del sistema operativo, sino de las
herramientas de desarrollo de cada lenguaje de programación, y
constituyen la _interfaz de programación de aplicaciones_ (API) del
lenguaje al que acompañan.

Las librerías estándar necesitan acceder a los
servicios del sistema operativo para, a su vez, dar servicio a los
programas que las usan. Es decir, cuando un programa invoca alguna
función o método de la librería estándar que lo acompaña, es muy
probable que ésta necesite invocar uno o más servicios del sistema
operativo para atender la petición convenientemente. Para ello las
librerías estándar utilizan la _librería del sistema_ —o librerías del
sistema, en el caso de que hayan varias— que acompaña al sistema
operativo.

[NOTE]
====
Por ejemplo, en el lenguaje de programación Python se debe utilizar la función `open()` de la librería estándar de Python para abrir o crear un archivo:

[source, python]
----
f = open("example.txt", "r")
----

Mientras que en el lenguaje de programación C la función equivalente de la librería estándar de dicho lenguaje es `fopen()`:

[source, c]
----
FILE* fp = fopen("example.txt", "r")
----

Ninguna de las dos funciones puede hacer su trabajo sin la ayuda del sistema operativo, porque sólo el sistema operativo tiene acceso a los discos duros y al resto de dispositivos del almacenamiento.
Así que ambas funciones están obligadas a solicitar la operación al sistema operativo, a través de la librería del sistema. Por ejemplo, en Microsoft Windows, tanto `open()` en Python como `fopen()` en C tiene que llamar internamente la función `CreateFile()` de la librería del sistema (véase la ).
====

_La *librería del
sistema* forma parte del sistema operativo y contiene un conjunto de
clases o funciones —generalmente más primitivas que las de la librería
estándar de los lenguajes de programación— que los programas deben
utilizar para acceder a los servicios del sistema operativo_. Es decir,
la librería del sistema constituye la _interfaz de programación de
aplicaciones_ del sistema operativo. Es muy común que esta interfaz esté
implementada para ser usada con programas en lenguaje C, lo que permite
que tanto los programas en C como en {cpp} la puedan utilizar
directamente. Sin embargo con otros lenguajes de programación esto no
suele ser posible, por lo que no queda más remedio que acceder a los
servicios del sistema operativo a través de la librería estándar del
lenguaje en cuestión.

[NOTE]
====
En Microsoft Windows, un programa en C puede usar tanto la función `fopen()` de su librería estándar como llamar directamente a la función `CreateFile()` de la librería del sistema. Con {cpp} ocurriría lo mismo que con C, pero otros lenguajes no tienen esa facilidad para llamar a la librería del sistema directamente. Por ejemplo, un programa en Python no puede llamar directamente a `CreateFile()` sino que debe usar la función `open()` de la librería estandar del lenguaje (véase la , destacado en rojo).

Usar directamente las funciones de la librería del sistema desde programas en C o {cpp} tiene la ventaja de que permite utilizar todas las características del sistema operativo. Por ejemplo, podríamos utilizar opciones adicionales de `CreateFile()`:

[source, c]
----
HANDLE WINAPI CreateFile(
  LPCTSTR lpFileName,           // Nombre del archivo
  DWORD dwDesiredAccess,        // Modo de acceso
  DWORD dwShareMode,            // Modo de compartición
  LPSECURITY_ATTRIBUTES lpSecurityAttributes,   // Permisos
  DWORD dwCreationDisposition,  // Acción si no existe el archivo
  DWORD dwFlagsAndAttributes,   // Atributos del archivo
  HANDLE hTemplateFile          // Archivo plantilla
);
----

que no `fopen()` no posee:

[source, c]
----
FILE* fopen(
  const char *path,             // Nombre del archivo
  const char *mode              // Modo de acceso
);
----

Sin embargo, debemos tener en cuenta que se pierde portabilidad pues `CreateFile()` solo está disponible en Microsoft Window, mientras que `fopen()` viene con la librería estándar de cualquier compilador de C.
====

Algunos de los servicios ofrecidos por la librería del sistema pueden ser implementados en la propia librería, pero en la mayor parte de los casos ésta debe solicitar dichos servicios al núcleo del sistema operativo que es quién supervisa y tiene acceso a los recursos de la máquina.

[NOTE]
====
En los sistemas UNIX o «estilo UNIX» las cosas son un poco más complejas.
El lenguaje C fue diseñado originalmente para implementar sistemas UNIX y por eso la librería estándar de C tiene mucho parecido con la librería del sistema de los sistemas UNIX.
Con el tiempo, al ir añadiendo más funcionaliades, la librería del sistema de los sistemas UNIX de los distintos fabricantes fue divergiendo, haciendo muy complicado desarrollar programas que usaran sus características más avanzadas y que a la vez pudieran usarse en varios de ellos.
Por eso el IEEE desarrollo el estándar POSIX, que define una interfaz de programación de aplicaciones (API) común para todos los UNIX y sistemas estilo UNIX modernos —como es el caso de Linux—.

Por su origen, la API POSIX es un superconjunto de la API de la librería estándar de C y por eso en estos sistemas la librería estándar de C es parte de la librería del sistema, en lugar de ser dos librerías separadas.
Como hemos comentado, otros lenguajes sí necesitan su propia librería estándar, que a su vez utiliza los servicios de la librería del sistema.

Por ejemplo, en {cpp} en Linux el método `open()` de la clase `std::ofstream` de la librería estándar utiliza a su vez la función `open()` de la librería del sistema para abrir o crear un archivo (véase la ).
La función `open()` no es parte del C estándar ni del {cpp} estándar, pero si del estándar POSIX.
Por lo tanto:

* Para crear programas para cualquier sistema operativo para el que tengamos un compilador de nuestro lenguaje, tenemos que limitarnos a usar funciones de la librería estándar de dicho lenguaje. Por ejemplo, `fopen()` en C o `std::ofstream` en {cpp}.

* Para usar `open()` y otras funciones POSIX directamente —desde lenguajes como C o {cpp}— no es suficiente con tener un sistema con un compilador estándar de C, sino que necesitamos un sistema operativo que siga el estándar POSIX, que es mucho más amplio.

En la librería del sistema de los sistemas POSIX los desarrolladores del sistema a veces añaden funciones no incluidas en el estándar, con el objeto de soportar algún tipo de funcionalidad avanzada del sistema:

* Un programa que sólo utilice la API POSIX podrá ejecutarse en cualquier sistema operativo compatible POSIX.

* Mientras que uno que utilice, por ejemplo, alguna funcionalidad adicional no POSIX de Linux, sólo podrá compilarse y ejecutarse en Linux.
====

La librería del sistema, al igual que la estándar y otras librerías utilizadas por el programa, se cargan dentro de la región de memoria asignada al proceso donde se ejecuta el programa que las utiliza.
Por lo tanto, la invocación de sus métodos y funciones se realiza como si fueran cualquier otro método o función del programa.
Sin embargo, el código del núcleo del sistema operativo suele estar en una ubicación
diferente que, desde el punto de vista de los programas, no es conocida y generalmente está protegida frente a accesos indebidos (véase el apartado <<_protección_de_la_memoria>>).
Eso significa que para que la librería del sistema invoque los servicios que necesita hace falta un procedimiento diferente, denominado *llamada al sistema*.

_Las *llamadas al sistema* proporcionan una interfaz con la que los procesos pueden invocar los servicios que el sistema operativo ofrece_.
Estas llamadas habitualmente se hacen mediante instrucciones en lenguaje ensamblador (véase la ) pero generalmente los programas no las utilizan directamente.
En su lugar invocan las funciones de la librería del sistema, que a su vez son las encargadas de hacer las llamadas al sistema necesarias.

En las figuras y se ilustra el papel de todos los elementos comentados con el ejemplo de programas en C, {cpp} y Python que invocan los métodos `fopen()`, `std::ofstream::open()` y `open()` de la librería estándar de estos lenguajes, respectivamente.
En todo los casos, al final se realiza una llamada al sistema que hace que el sistema operativo tome el control, deteniendo la ejecución del proceso que la solicita.
Entonces se realiza la tarea solicitada mediante el funcionamiento coordinado de los diferentes componentes del sistema (véase el apartado <<_componentes_del_sistema>>).

===== Invocación de las llamadas al sistema

Generalmente una
llamada al sistema se invoca mediante una instrucción específica en
lenguaje ensamblador que genera una **excepción**footnote:[Una
_excepción_ es una interrupción generada por software, que puede ser
debida a un error —por ejemplo una división por cero o un acceso no
válido a memoria— o a una llamada al sistema de un proceso para que se
ejecute un servicio del sistema operativo.] —por ejemplo la instrucción
_syscall_ en la — que es capturada por el sistema operativo, deteniendo
la ejecución del proceso que la invocó. Cuando se realiza la llamada es
necesario que el proceso identifique la operación que quiere que se
realice. Esto se suele hacer poniendo un número identificativo de la
llamada en un registro concreto de la CPU. Por ejemplo, el número de la
llamada al sistema `open` del ejemplo de la es 2footnote:[En GNU/Linux
se puede conocer el número correspondiente a cada llamada al sistema
soportada por el núcleo consultado el listado del archivo
`/usr/include/asm/unistd.h`.].

Sin embargo, una llamada al sistema suele requerir más información que
simplemente la identidad de la llamada. Si por ejemplo se quisiera leer
un bloque de datos desde un almacenamiento secundario, al menos se
debería indicar el archivo o dispositivo desde el que se desea realizar
la lectura, así como la dirección y tamaño de la región de la memoria
donde se quiere que los datos sean copiados. En concreto hay tres
métodos para pasar parámetros a una llamada al sistema:

* _En el paso de parámetros *por registros* se cargan los parámetros de
la llamada al sistema en los registros de la CPU antes de realizar la
llamada_. Este método es el más eficiente, pero limita el número de
parámetros al número de registros disponibles en la CPU. Es utilizado,
por ejemplo, en Linux para IA-32footnote:[IA-32 (_Intel Architecture,
32-bit_), conocida en la actualidad de manera genérica como x86 o i386,
es la arquitectura del conjunto de instrucciones de los procesadores
Intel de 32 bits. Concretamente es una extensión de 32 bits,
implementada por primera vez en el Intel 80386, para la arquitectura x86
original de 16 bits.] cuando la llamada al sistema tiene menos de seis
parámetros (véase la ).

* _En el paso de parámetros *por tabla en memoria* se copian los
parámetros de la llamada al sistema en una tabla en memoria_, de manera
que la dirección de dicha tabla debe ser cargada en un registro de la
CPU antes de la llamada al sistema. Evidentemente no limita el número de
parámetros que pueden ser pasados a una llamada al sistema. Por ejemplo,
es utilizado en Linux IA-32, cuando la llamada al sistema tiene más de
cinco parámetros, y en Microsoft Windows.

* _En el paso de parámetros *por pila* se insertan los parámetros de la
llamada al sistema en la pila del proceso_. En este caso el sistema
operativo es el encargado de extraer los parámetros de la pila durante
la llamada al sistema. Al igual que en el caso anterior tampoco se
limita el número de parámetros que pueden ser pasados. Es utilizando,
por ejemplo, en FreeBSD.

En cualquier caso, sea cual sea el método utilizado, _el sistema
operativo debe comprobar de manera estricta los parámetros pasados en la
llamada al sistema antes de realizar cualquier operación_, puesto que
nunca debe confiar en que los procesos hagan su trabajo correctamente. A
fin de cuentas una de las funciones del sistema operativo es el control
de dichos procesos.

==== Componentes del sistema

Como ya hemos comentado en diversas ocasiones en este tema, el sistema
operativo ofrece una serie de servicios a través del funcionamiento
coordinado de los diferentes componentes que lo forman. A fin de
cuentas, crear un software tan complejo como un sistema operativo no es
sencillo, por ello resulta más práctico dividirlo en piezas más pequeñas
especializadas en aspectos concretos de la gestión del sistema.

===== Gestión de procesos

La gestión de los procesos es un elemento central de todo sistema
operativo ya que el _proceso_ es la unidad de trabajo en cualquier
sistema operativo moderno:

* _Un *proceso* puede ser considerado como un programa en ejecución_,
es decir, cuando las instrucciones del programa son ejecutadas por una
CPU. Un proceso es un entidad activa que necesita recursos —CPU,
memoria, archivos, E/S— que se le asignan cuando es creado o cuando lo
solicita durante la ejecución. Cuando el proceso termina el sistema
operativo reclama de estos recursos aquellos que sean reutilizables.

* Un _**programa** no es un proceso, es una entidad pasiva; como el
contenido de un archivo en disco con las instrucciones que algún día una
CPU ejecutará_. Un programa no puede hacer ningún tipo de trabajo a
menos que sus instrucciones sean ejecutadas por una CPU pero si eso
ocurre, ya no sería un programa sino un proceso.

* La CPU ejecuta las instrucciones de cada proceso una detrás de otra,
de manera que para conocer la siguiente instrucción a ejecutar cada
proceso tiene un contador de programa que se lo indica a la CPU. Por
tanto, aunque dos procesos estén asociados al mismo programa no pueden
ser considerados el mismo proceso, ya que la secuencia de ejecución de
instrucciones puede ser distinta al tener cada uno un contador de
programa independiente.

Por el momento estamos considerando que _proceso_ y _trabajo_ (véase el
apartado <<_mainframe>>) hacen referencia al mismo concepto. Sin
embargo más adelante veremos que el segundo es mucho más general que el
primero puesto que un proceso puede colaborar con otros procesos para
desarrollar un trabajo determinado (véase el apartado
<<_operaciones_sobre_los_procesos>>).

====== Responsabilidades de la gestión de procesos

El sistema operativo es responsable de la siguientes actividades
relacionadas con la gestión de procesos:

* Crear y terminar procesos.
* Suspender y reanudar los procesos.
* Proporcionar mecanismos para la sincronización de procesos.
* Proporcionar mecanismos para la comunicación entre procesos.
* Proporcionar mecanismos para el tratamiento de interbloqueos.

===== Gestión de la memoria principal

La memoria principal es un recurso fundamental para las operaciones de
cualquier sistema operativo moderno. Esto es así porque generalmente es
el único almacenamiento al que la CPU tiene acceso directo, por lo que
para que un programa pueda ser ejecutado debe ser copiado a la memoria
principal. Además sirve de zona de intercambio de datos entre la CPU y
los dispositivos de E/S. Por ejemplo, para que la CPU pueda procesar los
datos de un archivo en disco, éstos primero deben ser transferidos a la
memoria principal.

Para mejorar el aprovechamiento de la CPU y la respuesta al usuario es
necesario tener copiados varios programas en la memoria al mismo tiempo.
Puesto que dichos programas deben compartir la memoria existe
automáticamente la necesidad de que el sistema operativo disponga de un
componente de gestión de la memoria principal.

====== Responsabilidad de la gestión de la memoria

El componente de gestión de la memoria debe asumir las siguientes
responsabilidades:

* Controlar qué partes de la memoria están actualmente en uso y cuáles no.

* Decidir que procesos añadir o extraer de la memoria cuando hay o falta espacio en la misma.

* Asignar y liberar espacio de la memoria principal según sea necesario.

===== Gestión del sistema de archivos

Los ordenadores pueden almacenar información en diferentes tipos de
medios físicos —por ejemplo en discos magnéticos, en CD/DVD-ROM, en
memorias de estado sólido, etc.— cada uno de los cuales tiene
características propias. El acceso a cada tipo medio es controlado por
un dispositivo —por ejemplo el controlador de disco, la unidad de
CD-ROM, etc.— que también tiene características propias. Para
simplificar todo esto el sistema operativo proporciona una visión lógica
uniforme de todos los sistemas de almacenamiento. Es decir, abstrae las
propiedades físicas de los dispositivos de almacenamiento para definir
una unidad de almacenamiento lógico, el _archivo_. _Un *archivo* o
fichero es una colección de información relacionada definida por su
creador_ —por ejemplo programas, imágenes, datos—. Los archivos
normalmente se organizan en directorios para facilitar su uso.

====== Responsabilidades de la gestión del sistema de archivos

El sistema operativo es responsable de la siguientes actividades
relacionadas con la gestión del sistema de archivos:

* Crear y borrar archivos.

* Crear y borrar directorios para organizar los archivos.

* Soportar __primitivas__footnote:[El término primitivas hace referencia a funciones que realizan operaciones muy básicas. Estas operaciones básicas pueden ser combinadas para realizar operaciones más complejas.] para la manipulación de archivos y directorios.

* Mapear en memoria archivos del almacenamiento secundario.

* Copias de seguridad de los archivos en sistemas de almacenamiento estables y seguros.

===== Gestión del sistema de E/S

_El *sistema de E/S* oculta las peculiaridades del hardware al resto
del sistema_. El sistema de E/S consta de:

* Un componente de gestión de memoria con soporte para servicios de
__buffering__footnote:[El buffering o uso de memoria intermedia es una
estrategia para leer datos desde un dispositivo de E/S. La CPU instruye
al dispositivo para que escriba bloques de datos en la memoria de forma
que la operación se realiza mientras la CPU está ocupada procesando los
bloques leídos anteriormente desde el dispositivo. Al escribir en un
dispositivo de E/S el proceso es análogo.], __caching__footnote:[En el
caching el sistema mantiene en la memoria principal una copia de los
datos almacenados en los dispositivos de E/S del sistema como, por
ejemplo, en los discos. Esto mejora la eficiencia del sistema puesto que
el acceso a la memoria principal es más rápido que el acceso a los
dispositivos de E/S. La memoria principal es de tamaño limitado, por lo
que sólo se mantiene copia de los datos utilizados con mayor
frecuencia.] y __spooling__footnote:[El spooling se utiliza en
dispositivos que no admiten el acceso simultaneo de varias aplicaciones
a vez, como es el caso de impresoras y unidades de cinta. Cuando varias
aplicaciones intentan enviar un trabajo a una impresora el sistema
operativo lo intercepta para copiar los datos enviados a un archivo
distinto para cada aplicación. Cuando una aplicación termina de enviar
el trabajo el archivo correspondiente es encolado para su impresión. Así
los archivos son impresos de uno en uno.]. Estos servicios son
habitualmente utilizados por el resto del sistema de E/S.

* _Una interfaz genérica de acceso a los controladores de
dispositivo_. Esta interfaz genérica hace que el acceso de los procesos
a los dispositivos sea a través de una interfaz similar, sin importar
las particularidades de cada dispositivo. Por ejemplo, una
característica de los sistemas UNIX es que cada dispositivo de E/S se
representa como un archivo en el sistema de archivos. Esto permite que
los procesos utilicen para acceder a los dispositivos de E/S las mismas
primitivas que emplean para manipular los archivos.

* _Controladores de dispositivo_ que son quiénes conocen las
peculiaridades específicas del dispositivo para el que ha sido creado.

===== Gestión del almacenamiento secundario

Los programas que se desean ejecutar deben estar en la memoria
principal, o _almacenamiento primario_, pero ésta es demasiado pequeña
para alojar todos los datos y todos los programas del sistema. Además,
incluso aunque pudiera ser así, los datos almacenados en la memoria
principal se perderían en caso de que fallara la alimentación. Por eso
los ordenadores deben disponer de un almacenamiento secundario para
respaldar a la memoria principal. Hoy en día lo más común es utilizar
discos duros para esa tarea.

====== Responsabilidades de la gestión del almacenamiento secundario

El sistema operativo es responsable de la siguientes actividades
relacionadas con la gestión del almacenamiento secundario:

* Gestión del espacio libre.
* Asignación del espacio de almacenamiento.
* Planificación del acceso al disco.

===== Gestión de red

_El componente de red se responsabiliza de la comunicación entre los
procesadores en sistemas interconectados mediante una red de
ordenadores_ —por ejemplo en Internet o la red de área local de una
oficina—.

===== Protección y seguridad

_*Protección* es cualquier mecanismo para controlar el acceso de los
procesos y usuarios a los recursos definidos por el sistema_. Estos son
necesarios cuando un sistema informático tiene múltiples usuarios y
permite la ejecución concurrente de varios procesos, pues así sólo
pueden utilizar los recursos aquellos procesos que hayan obtenido la
autorización del sistema operativo. Además _la *protección*
también permite mejorar la fiabilidad_ al permitir detectar los
elementos del sistema que no operan correctamente. Un recurso
desprotegido no puede defenderse contra el uso —o mal uso— de un usuario
no autorizado o incompetente.

Ejemplos típicos de mecanismos de protección son el hardware de
direccionamiento de memoria, que se utiliza para que los procesos se
ejecuten en su propio espacio de direcciones, y el temporizador, que
garantiza que ningún proceso toma el control de la CPU de manera
indefinida. Además los registros de los dispositivos de E/S suelen estar
protegidos del acceso directo de los usuarios, lo que protege la
integridad de los dispositivos, mientras que en algunos sistemas se
pueden establecer permisos sobre los archivos para garantizar que sólo
los procesos con la debida autorización tienen acceso.

En todo caso, un sistema puede tener la protección adecuada pero estar
expuesto a fallos y permitir accesos inapropiados. Por eso es necesario
disponer de _mecanismos de *seguridad* que se encarguen de defender el
sistema frente a ataques internos y externos_. Eso incluye a virus y
gusanos, ataques de denegación de serviciofootnote:[En los _ataques de
denegación_ de servicio se intentan utilizar todos los recursos de
sistema para evitar que éste pueda dar servició a los usuarios
legítimos.], robo de identidad y uso no autorizado del sistema, entre
muchos otros tipos de ataque.

==== Interfaz de usuario

Aunque cada sistema operativo ofrece servicios diferentes, vamos a
detenernos en uno común e importante para todos los sistemas que han
sido diseñados para que los usuarios interactúen con ellos directamente,
la interfaz de usuario.

Las interfaces de usuario pueden ser de diferentes tipos:

* _Interfaz de línea de comandos o intérprete de comandos_, que
permite que los usuarios introduzcan directamente los comandos que el
sistema operativo debe ejecutar. En algunos sistemas este tipo de
interfaz se incluye dentro del núcleo, pero en la mayor parte —como
MSDOS y UNIX— se trata de un programa especial denominado _shell_ que se
ejecuta cuando un usuario inicia una sesión.

* _Interfaz de proceso por lotes_, en la que los comandos y directivas
para controlar dichos comandos se listan en archivos que posteriormente
pueden ser ejecutados. Este tipo de interfaz es la utilizada en sistemas
no interactivos, como los de procesamiento por lotes y los
multiprogramados. También suele estar disponible en los sistemas de
tiempo compartido, junto con algún otro tipo de interfaz de usuario,
como es el caso de la _shell_ de los sistemas UNIX.

* _Interfaz gráfica de usuario_ o _GUI_ (_Graphical User Interface_)
que permite a los usuarios utilizar un sistema de ventanas y menús
controlable mediante el ratón.

Puesto que la interfaz de usuario puede variar de un sistema a otro, y
de un usuario a otro dentro del mismo sistema, no se suele incluir como
un componente básico del sistema operativo, pero si como un servicio
útil para los usuarios.

A parte de la interfaz de usuario, cualquier sistema operativo moderno
incluye una colección de programas del sistema. El papel de estos
programas del sistema es proporcionar un entorno conveniente para la
ejecución y desarrollo de programas. Entre los programas del sistema se
suelen incluir aplicaciones para manipular archivos y directorios,
programas para obtener información sobre el estado del sistema —como la
fecha y hora o la memoria y el espacio en disco disponible—,
herramientas de desarrollo —como intérpretes, compiladores, enlazadores
y depuradores—, programas de comunicaciones —como clientes de correo
electrónico y navegadores web—, etc.

Además, muchos sistemas operativos disponen de programas que son útiles
para resolver los problemas más comunes de los usuarios. Entre estos
programas se suelen incluir: editores de archivos de texto y
procesadores de texto, hojas de cálculo, sistemas de base de datos,
juegos, etc. Ha esta colección de aplicaciones se la suele conocer con
el término de _utilidades del sistema_ o _programas de aplicación_.

=== Operación del sistema operativo

_Los sistemas operativos modernos pertenecen a un tipo de software que
se dice que está controlado mediante interrupciones_:

* Si no hay ningún proceso que ejecutar ni ningún dispositivo de E/S
pide la atención del sistema, el sistema operativo debe permanecer
inactivo esperado a que algo ocurra.

* Los sucesos que requieren la activación del sistema casi siempre se
indican mediante una interrupción:

    ** Cuando un proceso comente un error —como una división por cero o un acceso a memoria no válido— o un programa solicita un servicio al sistema operativo a través de una llamada al sistema lo que se genera es una *excepción* —que no es más que una interrupción generada por software— que despierta al sistema operativo para que haga lo que sea más conveniente.

    ** Cuando los dispositivos de E/S requieren la atención del sistema operativo —por ejemplo porque se ha completado una transferencia de datos— se genera una interrupción que despierta al sistema operativo.

Dado que el sistema operativo y los procesos de usuarios comparten los
recursos del sistema informático, _necesitamos estar seguros de que un
error que se produzca en un programa sólo afecte al proceso que lo
ejecuta_. Por ejemplo, en los sistemas de tiempo compartido —y en
cualquier otro tipo de sistema operativo donde los programas tengan que
compartir la memoria, como es el caso de los sistema microprogramados—
un programa erróneo puede modificar el código de otro programa, los
datos de otro programa o el propio sistema operativo. Por eso es
necesario establecer mecanismos de protección frente a los errores en
los programas que se ejecutan en el sistema.

==== Operación en modo dual

Para evitar este tipo de problemas es necesario poder distinguir entre
la ejecución de código del sistema operativo y del código de los
programas de usuario. El método que utilizan la mayor parte de los
sistemas operativos consiste en _utilizar algún tipo de soporte en el
hardware que permita diferencia entre varios modos de ejecución y
restringir la utilización de las instrucciones peligrosas
—*instrucciones privilegiadas*— para que sólo puedan ser utilizadas en
el modo en el que se ejecuta el código del sistema operativo_.

Como mínimo son necesarios dos modos de operación diferentes:

* _En el *modo usuario* se ejecuta el código de las tareas de los
usuarios_. Si se hace un intento de ejecutar una instrucción
privilegiada en este modo, el hardware la trata como ilegal y genera una
excepción que es interceptada por el sistema operativo, en lugar de
ejecutar la instrucción.

* _En el *modo privilegiado*_ —también denominado *modo supervisor*,
*modo del sistema* o *modo kernel*— _se ejecuta el código de
las tareas del sistema operativo_. El hardware es el encargado de
garantizar que las instrucciones privilegiadas sólo pueden ser
ejecutadas en este modo.

El modo actual de operación _puede venir indicado por un *bit de modo*
que se añade al hardware de la computadora_, de forma que si por
ejemplo el bit está a 0, el código en ejecución opera en modo
privilegiado mientras que si el bit está a 1, el código en ejecución
opera en modo usuario.

Comúnmente en el grupo de las instrucciones privilegiadas se suelen
incluir:

* _Las instrucción para conmutar al modo usuario_.
* _Las instrucciones de E/S_.
* _Las instrucciones necesarias para la gestión de las
interrupciones_.

A continuación podemos ver el ciclo de vida de la ejecución de
instrucciones en un sistema con modo dual de operación:

1. _Inicialmente, al arrancar la computadora, el hardware se inicia en
el modo privilegiado_ —es decir, con el bit de modo a 0—. En este modo
se carga el sistema operativo e inicia su ejecución.

2. _El sistema operativo debe cambiar al modo usuario —poniendo el bit
de modo a 1— antes de ceder el control a un proceso de usuario_. Esto
ocurre cuando es necesario que un proceso de usuario continúe o inicie
su ejecución (véase el <<_el_asignador>>).

3. _El hardware conmuta a modo privilegiado cuando ocurre una
interrupción o una excepción_ —poniendo el bit de modo a 0— antes de
pasar el control al código del sistema operativo que se encargará de
tratarlas.

Esto último es importante pues, como ya hemos comentado, los sistemas
operativos están controlados mediante interrupciones. Al activarse el
modo privilegiado cada vez que ocurre una interrupción podemos estar
seguros de que las tareas del sistema operativo se ejecutará en modo
privilegiado.

Cuando se dispone de la protección del modo dual el hardware se encarga
de detectar los errores de ejecución y de notificarlo al sistema
operativo mediante excepciones, siendo responsabilidad de este último
realizar un tratamiento adecuado de los mismos. Por lo general, si un
programa falla de alguna forma, como por ejemplo intentando utilizar una
instrucciones ilegal o de acceder a una zona de memoria inválida, el
sistema operativo lo hace terminar de manera anormal.

==== Protección de la memoria

La memoria principal debe acomodar tanto el sistema operativo como a los
diferentes procesos de los usuarios. Por eso la memoria normalmente se
divide en dos partes:

1. __La primera parte sirve para albergar el sistema operativo
residente__footnote:[El termino sistema operativo residente hace
referencias a los componentes del sistema operativo que deben estar
permanentemente en la memoria. Comúnmente dicho conjunto de elementos
componen el núcleo del sistema.]. El sistema operativo puede estar
localizado tanto en la parte baja como en la parte alta de la memoria.
El factor determinante en la elección es la localización del vector de
interrupciones. Puesto que en la mayor parte de las arquitecturas éste
reside en la parte baja de la memoria, normalmente el sistema operativo
también se aloja en la parte baja.

2. _La segunda parte alberga los procesos de usuario_.

Sin embargo en los sistemas operativos modernos los procesos no tienen
acceso libre a toda memoria física con el objeto de proteger a los
procesos en ejecución y al sistema operativo de posibles errores en
cualquiera de ellos:

* _El sistema operativo proporciona a cada proceso una «vista»
privada de la memoria similar a la que tendrían si cada uno de ellos se
estuviera ejecutando en solitario_ (véase la ).

* A esa «vista» que tiene cada proceso de la memoria es a lo que se
denomina *espacio de direcciones virtual* del proceso y _está formado
por el conjunto de direcciones que puede generar la CPU para un proceso
dado_.

* Durante los accesos a la memoria principal en tiempo de ejecución
estas *direcciones virtuales* son convertidas en direcciones físicas
antes de ser enviadas a la memoria principal. Por tanto _las
*direcciones físicas* son las direcciones reales que ve la memoria,_
mientras que _el *espacio de direcciones físico* es el conjunto de
direcciones físicas que corresponden a un espacio de direcciones virtual
dado_.

_La conversión de una dirección virtual en una física la realiza en
tiempo de ejecución un dispositivo hardware denominado MMU
(Memory-Management Unit)_. Las ventajas de este dispositivo desde el
punto de vista de la protección de la memoria son que:

* _Permite el aislamiento de los procesos_, creando para cada uno la
ilusión de que toda la memoria es para él y evitando que un proceso
pueda acceder a la memoria de otros procesos.

* _Permite marcar los modos de acceso autorizados en las diferentes
regiones de la memoria_ —como por ejemplo lectura, escritura y
ejecución— evitando que el código ejecutado en modo usuario tenga acceso
a zonas a las que no debería tenerlo. El acceso a la memoria en un modo
no autorizado se considera una instrucción privilegiada, por lo que ese
tipo de acceso desde el modo usuario siempre genera una excepción.

==== El temporizador

_El *temporizador* se utiliza para poder estar seguros de que el
sistema operativo es capaz de mantener el control de la CPU_, puesto
que lo que no puede ocurrir es que un proceso entre en un bucle infinito
de manera que nunca devuelva el control al sistema operativo.

El temporizador se configura durante el arranque del sistema para
interrumpir a la CPU a intervalos regulares. Así, cuando el temporizador
interrumpe, el control se transfiere automáticamente al sistema
operativo. Entonces este puede: conceder más tiempo al proceso en
ejecución, detenerlo y darle más tiempo de CPU en el futuro o tratar la
interrupción como un error y terminar de manera anormal el programa.
Indudablemente las instrucciones que pueden modificar el contenido del
temporizador son instrucciones privilegiadas.

=== Sistemas operativos por su estructura

Ya hemos discutido anteriormente acerca de los componentes más comunes
en un sistema operativo (véase el <<_componentes_del_sistema>>). En
esta sección comentaremos su organización e interconexión dentro del
núcleo.

==== Estructura sencilla

_Los sistemas con *estructura sencilla* no tienen una estructura bien
definida_. Es decir, los interfaces y niveles de funcionalidad no están
bien separados.

Por ejemplo, en MSDOS los programas de aplicación podían acceder
directamente a la BIOS o al hardware para hace acceder a cualquier
dispositivo (véase la ). Disponiendo de esa libertad un programa erróneo
cualquiera podía corromper el sistema completo. Como el Intel 8086 para
el que fue escrito MSDOS no proporcionaba un modo dual de operación, los
diseñadores del sistema no tuvieron más opción que dejar accesible el
hardware a los programas de usuario.

Otro ejemplo es el de UNIX original, donde se combinaba un montón de
funcionalidad en un mismo nivel, el núcleo (véase la ). Es decir, todo
lo que estaba por encima del hardware y por debajo de las llamadas al
sistema era el núcleo. Este proporciona la planificación de CPU, la
gestión de la memoria, el soporte de los sistemas de archivos y muchas
otras funcionalidades del sistema operativo. En general se trata de una
enorme cantidad de funcionalidad que es difícil de implementar y
mantener en un mismo nivel. _Esa concentración de funcionalidad en el
núcleo define a los sistemas de estructura sencilla como sistemas de
núcleo *monolítico*_.

Tanto MSDOS como UNIX eran originalmente sistemas pequeños y simples,
limitados por la funcionalidades del hardware de su época, que fueron
creciendo más allá de las previsiones originales. Lo cierto es que con
mejor soporte del hardware se puede dividir el sistema operativo en
piezas más pequeñas y apropiadas que las del MSDOS y UNIX original.

==== Estructura en capas

_Un método para dividir el sistema operativo en piezas más pequeñas,
con el fin de hacerlo más modular, es partirlo en capas_. Las capas se
seleccionan de manera que cada una use sólo funciones y servicios de las
capas inferiores y de servicios sólo a las capas superiores. Cada capa
no tiene que saber como se implementan las funciones que utiliza de las
capas inferiores, sólo debe conocer qué es lo que hacen y como utilizar.
Por lo tanto cada capa tiene la responsabilidad de ocultar la existencia
de estructuras de datos, operaciones y hardware a las capas de nivel
superior. Este tipo de sistemas son los que se denominan con
*estructura en capas*.

Los sistemas con estructura en capas siguen concentrado la mayor parte
de la funcionalidad en el núcleo, por lo que _también son sistemas
**monolíticos**_ aunque el núcleo es más modular. Ejemplos de este tipo de
sistemas operativos son el IBM OS/2 y Microsoft Windows (véase la ).

Sin embargo esta forma de dividir los componentes del sistema operativo
no está libre de inconvenientes:

* _La mayor dificultad con los sistemas con estructura en capas es
definirlas_. Esto debe ser planificado cuidadosamente debido a la
restricción, comentada anteriormente, de que un capa sólo puede utilizar
los servicios de las capas inferiores. Por ejemplo, el planificador de
CPU suele tener información de los procesos que están en la memoria y
parte de esa información puede ser intercambiada con el disco para
aumentar la memoria principal disponible. Este planteamiento nos lleva a
pensar que la gestión del almacenamiento secundario debe ir en una capa
inferior a la del planificador de la CPU. Sin embargo el planificador
debe replanificar la CPU cuando el proceso que actualmente la ocupa
solicita alguna operación de E/S, por lo que la gestión del
almacenamiento secundario debe estar encima del planificador de la CPU
para que le pueda decir que replanifique. _Al final la solución de
compromiso es tender hacia sistemas con pocas capas donde cada una tiene
mucha funcionalidad_.

* _Esta estrategia es sin duda mucho menos eficiente que la de los
sistemas de estructura sencilla_. En cada capa los parámetros son
modificados y los datos necesarios deben de ser transferidos, por lo que
cada una añade cierto nivel de sobrecarga al funcionamiento del sistema.

==== Microkernel

_Los sistemas *microkernel* eliminan todos los componentes no
esenciales del núcleo y los implementa como programas de nivel de
usuario_. Aunque hay poco consenso, en general un núcleo microkernel
proporciona funciones mínimas de gestión de procesos y memoria, junto a
algún mecanismo de comunicación. En estos sistemas la función principal
del núcleo es precisamente proporcionar dicho mecanismo de comunicación
entre el programa cliente y los diversos servicios del sistema.
Generalmente esta comunicación se implementa mediante paso de mensajes
(véase el <<_procesos_cooperativos>>).

Entre los beneficios de estos sistemas operativos se incluyen:

* _Facilidad a la hora de añadir nuevas funcionalidades_. Los nuevos
servicios son añadidos como aplicaciones de nivel de usuario, por lo que
no es necesario hacer modificaciones en el núcleo.

* _Facilidad a la hora de portar el sistema a otras plataformas_.

* _Más seguridad y fiabilidad_. Puesto que los servicios se ejecutan a
nivel de usuario en procesos separados, un servicio que falla no puede
afectar a otros ni puede ser utilizado para ganar acceso a otros
servicios o al núcleo.

_El mayor inconveniente es su pobre rendimiento causado por la
sobrecarga que añade el mecanismo de comunicación_. Por ejemplo
Microsoft Windows NT nació con una estructura de microkernel en capas
donde una parte importante de los servicios eran proporcionados por unos
procesos de usuario llamados subsistemas. Además el sistema operativo
podía mostrar diferentes personalidades o _entornos operativos_ —OS/2,
POSIX y DOS— a través del uso de subsistemas ambientales, que también se
ejecutaban como procesos de usuario. Las aplicaciones de Microsoft
Windows NT se comunicaban con estos subsistemas utilizando una forma de
IPC (véase el <<_microkernel>>) denominada LPC (_Local
Procedure Call_), una forma local y optimizada de RPCfootnote:[La RPC
(_Remote Procedure Call_) es una mecanismo de llamada a procedimiento
diseñado para ser utilizado entre sistemas conectados por redes de
ordenadores, permitiendo que un proceso cliente llame a un procedimiento
en un proceso servidor, aunque ambos estén en equipos diferentes, y
ocultado los detalles de la comunicación que permiten que la llamada
tenga lugar.]. Con esta estructura la pérdida de rendimiento respecto a
Microsoft Windows 95 era tan importante que los diseñadores se vieron
obligados a mover más servicios al espacio del núcleo. En la actualidad
Microsoft Windows XP (véase la ) —que es un heredero directo de
Microsoft Windows NT— tiene una arquitectura más monolítica que
microkernelfootnote:[A las 280 llamadas al sistema de Microsoft Windows
XP —algo menos de 200 en Microsoft Windows NT 3.51— se deben sumar las
más de 650 del subsistema gráfico, alojado en el núcleo desde Microsoft
Windows NT 4.0.] ya que aunque muchos servicios siguen siendo
proporcionados por procesos de usuario, esto sólo ocurre con aquellos
donde el rendimiento no es un factor crítico.

Sin embargo varios sistemas operativos siguen utilizando núcleos
microkernel, como Tru64 UNIX y GNU/Hurd (véase la ). Ambos proporcionan
una interfaz UNIX implementada sobre un microkernel Mach. Otro ejemplo
es QNX, un sistema operativo de tiempo real con una gran aceptación que
basa en la estructura de microkernel su estabilidad como sistema para
tareas críticas. Además siguen existiendo algunos proyectos de
investigación dirigidos a resolver los problemas de rendimiento
asociados a los núcleos microkernel.

==== Estructura modular

Los sistemas de *estructura modular* tienen divido el núcleo en módulos,
cada uno de los cuales implementa funciones y servicios concretos, a
imagen y semejanza de las técnicas de programación orientada a objetos.
Quizás por eso sea la mejor metodología actual para diseñar sistemas
operativos. Además _se parecen a los sistemas con estructura en capas
en que cada módulo del núcleo tiene definidos interfaces protegidas,
pero a diferencia de estos todos los módulos pueden llamar a cualquier
otro módulo_.

Estos núcleos suelen disponer un pequeño conjunto de componentes
fundamentales que se cargan durante el arranque, aunque también pueden
enlazar dinámicamente servicios adicionales tanto durante la
inicialización del sistema como o en tiempo de ejecución. En este
aspecto se asemejan a los núcleos microkernel, ya que el módulo
principal sólo tiene funciones básicas, aunque es mucho más eficiente al
no necesitar un mecanismo de paso de mensajes, puesto que los
componentes se cargan directamente en la memoria destinada al núcleo.
Por lo tanto también _deben ser considerados como sistemas
**monolíticos**_.

Este tipo de estructura es la utilizada en los UNIX modernos, como
Oracle/Sun Microsystems Solaris, Linux (véase la ) y macOS.

== Gestión de procesos

=== Procesos

Los primeros sistemas informáticos sólo permitían que un programa se
ejecutara de cada vez. Dicho programa tenía un control completo sobre el
sistema y acceso a todos los recursos del mismo. Por el contrario, los
sistemas de tiempo compartido actuales permiten que múltiples programas
sean cargados y ejecutados concurrentemente. Obviamente esta evolución
requiere un control más fino y la compartimentación de los diversos
programas para que no interfieran unos con otros. Esto a su vez conduce
a la aparición de la noción de _proceso, que no es sino la unidad de
trabajo en un sistema operativo moderno de tiempo compartido_.

Por simplicidad, en este tema utilizaremos los términos trabajo y
proceso de forma indistinta. A fin de cuentas tanto los _trabajos_ en
los sistemas de procesamiento por lotes como los _procesos_ en los
sistemas de tiempo compartido son la unidad de trabajo en sus
respectivos sistemas y el origen de toda actividad en la CPU.

Por último, antes de continuar, no debemos olvidar que en un sistema
operativo hay:

* _Procesos del sistema_ ejecutando el código del sistema operativo
contenido en los _programas del sistema_, que generalmente realizan
tareas que es mejor mantener fuera del núcleo.

* _Procesos de usuario_ ejecutando código de usuario contenido en los
_programas de aplicación_.

Sin embargo en lo que resta de tema no estableceremos ningún tipo de
distinción entre ellos. Al fin y al cabo todos son simples procesos de
cara al resto del sistema.

==== El proceso

Como ya hemos comentado con anterioridad, _un *proceso* es un programa
en ejecución_. Sin embargo los procesos no son sólo el código del
programa, sino que _también suelen contener algunos_ otros elementos:

* *El código del programa*, conocido como la sección `text`.

* *La sección de datos* contiene las variables globales. Se divide entre
la sección `data`, donde se almacenan las variables inicializadas, y
la sección `bss`, donde se almacenan las variables sin inicializar.

* *La pila* contiene datos temporales como parámetros y direcciones de
retorno de las funciones y variables locales. Es conocida como la
sección `stack`.

* *El montón*, que es donde se aloja la memoria que se asigna dinámicamente durante la ejecución del proceso.

* *Información de la actividad actual*, como el _contador de programa_, los _registros de la CPU_, etc.

En la se puede observar la disposición de algunos de estos elementos en
la memoria.

En todo caso es importante recordar que _un *proceso* es una entidad
activa, con un contador de programa especificando la próxima instrucción
a ejecutar y un conjunto de recursos del sistema asociados_. Mientras
que _un *programa* no es un proceso ya que es una entidad pasiva, como
un archivo en disco que contiene el código que algún día será ejecutado
en la CPU_. Por lo tanto dos procesos pueden estar asociados al mismo
programa pero no por eso dejan de ser distintos procesos (véase el
apartado <<_componentes_del_sistema>>). Ambos tendrán la misma sección _text_
pero el contador de programas, la pila, la sección _data_, etc.
contendrán valores diferentes.

==== Estados de los procesos

Los procesos tienen un *estado* que cambia a lo largo de su ejecución y
está definido parcialmente por la actividad actual del propio proceso.
Los estados por los que puede pasar un procesos varían de un sistema
operativo a otro, aunque los siguientes son comunes a todos ellos:

* *Nuevo*. El proceso está siendo creado.

* *Ejecutando*. El proceso está siendo ejecutado puesto que ha sido
escogido por el planificador de la CPU. _Sólo puede haber un proceso en
este estado por CPU en el sistema_.

* *Esperando*. El proceso está esperando por algún _evento_, como
por ejemplo que termine alguna operación de E/S o que se reciba alguna
señal. Obviamente varios procesos pueden estar en este estado.

* *Preparado*. El proceso está esperando a que se le asigne la CPU.
Varios procesos pueden estar en este estado.

* *Terminado*. El proceso ha finalizado su ejecución y espera a que se
liberen los recursos que le fueron asignados.

El diagrama de estado de los procesos se muestra en la .

==== Bloque de control de proceso

_El *bloque de control de proceso* o *PCB* (Process Control Block) es
una estructura de datos que representa a cada proceso en el sistema
operativo_. Sirve de almacén para cualquier información que puede
variar de un proceso a otro.

* *Estado del proceso*. Por ejemplo: nuevo, preparado, esperando, etc.

* *Contador de programa*. Indica la dirección de la próxima
instrucción del proceso que debe ser ejecutada por la CPU.

* *Registros de la CPU*.

* *Información de planificación de la CPU*. Incluye la información
requerida por el planificador de la CPU. Por ejemplo la prioridad del
proceso, punteros a las colas de planificación donde está el proceso,
punteros al PCB del proceso padre y de los procesos hijos, etc.

* *Información de gestión de la memoria*. Incluye la información
requerida para la gestión de la memoria. Por ejemplo los valores de los
registros base y límite que definen el área de la memoria física que
ocupa el proceso, la tabla de páginas —en el caso de que se use
paginación (véase el <<_paginación>>)— o la tabla de
segmentos —en el caso de que se utilice segmentación— etc.

* *Información de registro*. Esta información incluye la cantidad de
CPU usada, límites de tiempo en el uso de la CPU, estadísticas de la
cuenta del usuario al que pertenece el proceso, estadísticas de la
ejecución del proceso, etc.

* *Información de estado de la E/S*. Incluye la lista de dispositivos
de E/S reservados por el proceso, la lista de archivos abiertos, etc.

==== Colas de planificación

En los sistemas operativos hay diferentes *colas de planificación* para
los procesos en distintos *estados*.

* *Cola de trabajo*. _Contiene a todos los procesos en el sistema_ de
manera que cuando un proceso entra en el sistema va a esta cola.

* *Cola de preparados*. _Contiene a los procesos que están cargados
en la memoria principal y están preparados para ser ejecutados_. La
cola de preparados es generalmente una lista enlazada de PCB donde cada
uno incluye un puntero al PCB del siguiente proceso en la cola.

* *Colas de espera*. _Contienen a los procesos que están esperando
por un evento concreto_, como por ejemplo la finalización de una
solicitud de E/S. Estas colas también suelen ser implementadas como
listas enlazadas de PCB y suele existir una por evento, de manera que
cuando ocurre algún evento todos los procesos en la cola asociada pasan
automáticamente a la cola de preparados.

* *Colas de dispositivo*. Son un caso particular de cola de espera.
Cada dispositivo de E/S tiene asociada una cola de dispositivo que
contiene los procesos que están _esperando_ por ese dispositivo en
particular.

Una manera habitual de representar la planificación de procesos es a
través de un diagrama de colas como el de la . Analizándolo podemos
tener una idea clara del flujo típico de los procesos dentro del
sistema:

. _Un nuevo proceso llega al sistema_. El proceso es colocado en la _cola de preparados_.
Allí espera hasta que es seleccionado para su ejecución y se le asigna la CPU. Mientras se ejecuta pueden ocurrir varias cosas:

    ** _El proceso puede solicitar una operación de E/S_ por lo que abandona la CPU y es colocado en la _cola de dispositivo_ correspondiente.
    No debemos olvidar que aunque en nuestro diagrama no exista más que una de estas colas, en un sistema operativo real suele haber una para cada dispositivo.

    ** _El proceso puede querer esperar por un evento_.
    Por ejemplo puede crear un subproceso y esperar a que termine. En ese caso el proceso hijo es creado mientras el proceso padre abandona la CPU y es colocado en una _cola de espera_ hasta que el proceso hijo termine.
    La terminación del proceso hijo es el evento que espera el proceso padre para continuar su ejecución.

    ** _El proceso puede ser sacado forzosamente de la CPU_ —como resultado de la interrupción del temporizador que indica que lleva demasiado tiempo ejecutándose— y colocado en la _cola de preparados_.

. _Cuando la espera concluye_ los procesos pasan del _estado de espera
al de preparado_ y son insertados en la _cola de preparados_.

. _El proceso repite este ciclo hasta que termina_.
En ese momento es eliminado de todas las colas mientras el PCB y los recursos asignados son liberados.

==== Planificación de procesos

Durante su ejecución, los procesos se mueven entre las diversas colas de
planificación a criterio del sistema operativo como parte de la tarea de
planificación. Este proceso de selección debe ser realizado por el
*planificador* adecuado:

* __En los sistemas de multiprogramados__footnote:[Los sistemas de tiempo
compartido como GNU/Linux, Microsoft Windows o cualquier sabor de UNIX
carecen de planificador de trabajos. En estos sistemas simplemente se
cargan los procesos en memoria para que sean ejecutados cuando el
usuario lo solicita.] el *planificador de largo plazo* —o *planificador
de trabajos*— selecciona los trabajos desde la cola de entrada en el
almacenamiento secundario, dónde están todos almacenados, y los carga en
memoria.

* _El *planificador de corto plazo* o *planificador de CPU*
selecciona uno de los procesos en la cola de preparados y lo asigna a la
CPU_. Obviamente este planificador es invocado cuando un proceso en
ejecución abandona la CPU por cualquiera de los motivos comentados.

* _Algunos sistemas operativos utilizan el *planificador de medio
plazo* para sacar procesos de la memoria y reintroducirlos
posteriormente_. A este esquema se le denomina *intercambio* —o
_swapping_— y puede ser necesario utilizarlo cuando escasea la
memoria.

==== Cambio de contexto

_El *cambio de contexto* es la tarea de asignar a la CPU un proceso
distinto al que la tiene asignada en el momento actual_. Esto implica
salvar el estado del viejo proceso en su PCB y cargar en la CPU el
estado del nuevo. Entre la información que debe ser preservada se
incluye:

* Los *registros de la CPU*.

* El *estado del proceso*.

* La *información de gestión de la memoria*. Por ejemplo la información referente al espacio de direcciones del proceso.

El cambio de contexto es sobrecarga pura puesto que no hace ningún
trabajo útil mientras se conmuta. Su velocidad depende de aspectos tales
como: el número de registros, la velocidad de la memoria y la existencia
de instrucciones especialesfootnote:[Algunas CPU disponen de
instrucciones especiales para salvar y cargar todos los registros de
manera eficiente. Esto reduce el tiempo que la CPU está ocupada en los
cambios de contexto. Otra opción es el uso de juegos de registros, como
es el caso de los procesadores Sun UltraSPARC e Intel Itanium. Con ellos
el juegos de registros de la CPU puede ser mapeado sobre un banco de
registros mucho más extenso. Esto permite que la CPU almacene de forma
eficiente el valor de los registros de más de un proceso.].

==== Operaciones sobre los procesos

En general los procesos pueden ser creados y eliminados dinámicamente,
por lo que los sistemas operativos deben proporcionar mecanismos para la
creación y terminación de los mismos.

===== Creación de procesos

_Un proceso —denominado *padre*— puede crear múltiples procesos —los
*hijos*— utilizando una llamada al sistema específica para la creación
de procesos_. En general cada proceso se identifica de manera unívoca
mediante un *identificador de proceso* o *PID* (_Process Identifier_),
que normalmente es un número entero. Puesto que cada nuevo proceso puede
a su vez crear otros procesos, al final se acaba obteniendo un **árbol
de procesos**footnote:[En los sistemas UNIX el proceso _init_ es el
proceso padre raíz de todos los procesos de usuario. Su PID siempre
es 1 ya que es el primer proceso creado por el sistema operativo al
terminar la inicialización del núcleo. Por lo tanto es el responsable de
crear todos los otros procesos que son necesarios para el funcionamiento
del sistema.].

Hay varios aspectos en la creación de los procesos que pueden variar de
un sistema operativo a otro:

* *¿Cómo obtienen los subprocesos los recursos que necesita para hacer
su trabajo?*

    ** _En algunos sistemas operativos los subprocesos sólo puede aspirar a obtener un subconjunto de los recursos de su padre_. Esto permite evitar, por ejemplo, que un proceso pueda sobrecargar el sistema creando demasiados procesos.

    ** Mientras que _en otros cada subproceso puede solicitar y obtener los recursos directamente del sistema operativo_.

* *¿Qué ocurre con los recursos de un proceso cuando decide crear
subprocesos?*

    ** _El proceso puede estar obligado a repartir sus recursos entre sus hijos_.

    ** _O puede que esté en disposición de compartir algunos recursos_ —como memoria y archivos— _con algunos de sus hijos_. Por ejemplo en POSIX todos los archivos abiertos por un proceso son _heredados_ en ese estado por sus hijos.

* *¿Cómo un proceso puede pasar parámetros de inicialización a sus procesos hijo?* Además de los diversos recursos que un proceso obtiene cuando es creado, el proceso padre suele poder pasar parámetros de inicialización a sus procesos hijo. Por ejemplo en C y {cpp} se puede obtener acceso a estos parámetros través de los argumentos `argc` y `argv` de la función `main()` del programa.

* *¿Qué ocurre con la ejecución de un proceso cuando crea un
subproceso?* Si eso ocurre se suelen contemplar dos posibilidades en
términos de la ejecución del padre:
** _El padre continúa ejecutándose concurrentemente con el hijo_.
** _El padre decide esperar a que algunos o todos sus hijos terminen_.
* *¿Cómo se construye el espacio de dirección de los subprocesos?* En
general hay dos posibilidades:
** _El espacio de direcciones del proceso hijo es un duplicado del que
tiene el padre_. Es decir, que inicialmente tiene el mismo código y
datos que el padre.
** _El espacio de direcciones del proceso hijo se crea desde cero y se
carga en él un nuevo programa_.

Para ilustrar la diferencia entre estos dos últimos casos, supondremos
que tenemos un proceso que quiere crear un subproceso. En los sistemas
operativos POSIX un nuevo proceso siempre se crea con la llamada
`fork()` que se encargar de crear el nuevo proceso con una copia del
espacio de direcciones del proceso original (véase la ). Esto facilita
la comunicación entre procesos puesto que al copiarse el espacio de
direcciones también se copia la tabla de archivos abiertos. No debemos
olvidar que muchos de los recursos de un sistema POSIX se muestran de
cara a los procesos que los utilizan como archivos. Por lo tanto, el
proceso hijo no sólo tiene acceso a los archivos abiertos por el padre
antes de la llamada al `fork()` sino también a tuberías (véase el
apartado <<_ejemplos_de_mecanismos_comunicación_entre_procesos>>), sockets (véase el apartado
<<_ejemplos_de_mecanismos_comunicación_entre_procesos>>) y regiones de memoria compartida (véase el
apartado <<_memoria_compartida>>), entre otros muchos recursos. Todos
estos son mecanismos que los procesos pueden utilizar para comunicarse.

Tanto padre como hijo continúan la ejecución en la siguiente instrucción
después del `fork()`. La diferencia es que para el padre el valor de
retorno de la llamada `fork()` es el identificador de proceso del hijo,
mientras que para el hijo el valor de retorno es cero. De esa forma
padre e hijo pueden saber quién es cada uno (véase la ).

En muchas ocasiones lo que se desea es iniciar la ejecución de un nuevo
programa. Como POSIX no dispone de una llamada al sistema para dicha
tarea, lo que se debe hacer es invocar la llamada `exec()` en el hijo
después del `fork()`. La llamada `exec()` carga un nuevo archivo
ejecutable en el espacio de direcciones del proceso actual e inicia su
ejecución, destruyendo la imagen del programa que realizó la llamada a
`exec()`. Mientras tanto el padre puede crear más hijos o si no tiene
nada más que hacer puede esperar. Para ello utiliza la llamada `wait()`
que mueve el proceso a una cola de espera hasta que termine el hijo
creado previamente (véase la ).

Respecto a la familia de sistemas operativos Microsoft Windows NT, se
supone que ambos modelos son soportados a través de la llamada al
sistema `NTCreateProcess()`. Es decir, que el espacio de direcciones
del padre puede ser duplicado o se puede indicar el nombre de un
programa para que el sistema operativo lo cargue en un nuevo proceso.
Sin embargo el hecho es que los programadores no tienen acceso directo a
`NTCreateProcess()` ya que se supone que debe ser utilizada a través de
la función `CreateProcess()` de la librería del sistema de la API Win32.
Dicha función no duplica el espacio de direcciones del padre, sino que
simplemente carga en un nuevo proceso el programa indicado en los
argumentosfootnote:[Esta decisión de diseño en el API principal de
Microsoft Windows estuvo motivada por el hecho de que la creación de
nuevos procesos tiene un coste alto en ese sistema operativo. En
Microsoft Windows la forma adecuada de realizar varias tareas al mismo
tiempo es utilizando hilos (véase el <<_hilos>>).].

===== Terminación de procesos

Un proceso termina cuando se lo indica al sistema operativo con la
llamada al sistema `exit()`. En ese momento puede devolver un valor de
estado a su padre, que este puede recuperar a través de la llamada al
sistema `wait()`. Cuando un proceso termina todos los recursos son
liberados incluyendo: la memoria física y virtual, archivos abiertos,
búferes de E/S, etc.

En todo caso un proceso puede provocar la terminación de otro proceso a
través de una llamada al sistema. Habitualmente el proceso que la invoca
es el padre ya que puede que sea el único con permisos para hacerla. Los
motivos pueden ser:

* _El hijo ha excedido el uso de algunos de los recursos reservados_.
Obviamente esto tiene sentido cuando los hijos utilizan un subconjunto
de los recursos asignados al padre.

* _La tarea asignada al hijo ya no es necesaria_.

* _El padre termina y el sistema operativo está diseñado para no permite
que el hijo pueda seguir ejecutándose si no tiene un padre._ Esto
provoca que el sistema operativo inicie lo que se denomina una
**terminación en cascada**footnote:[En UNIX si un proceso muere a su hijo
se le reasigna como padre el proceso _*init*_ —con PID 1—. Al menos en
GNU/Linux, los intentos de matar a este último son ignorados por el
sistema.], donde todos los procesos que cuelgan de uno dado terminan.

=== Procesos cooperativos

Desde el punto de vista de la cooperación podemos clasificar los
procesos en dos grupos:

* _Los *procesos independientes*, que no afectan o pueden ser
afectados por otros procesos del sistema._ Cualquier proceso que no
comparte datos —temporales o persistentes— con otros procesos es
independiente.

* _Los *procesos cooperativos*, que pueden afectar o ser afectados
por otros procesos ejecutados en el sistema_. Los procesos que
comparten datos siempre son cooperativos.

==== Motivaciones para la colaboración entre procesos

Hay diversos motivos para proporcionar un entorno que permita la
cooperación de los procesos:

* *Compartición de información*. Dado que varios usuarios pueden estar
interesados en los mismos bloques de información —por ejemplo en un
archivo compartido— el sistema operativo debe proporcionar un entorno
que permita el acceso concurrente a este tipo de recursos.

* *Velocidad de cómputo*. Para que una tarea se ejecute más rápido se
puede partir en subtareas que se ejecuten en paralelo. Es importante
destacar que la mejora en la velocidad sólo es posible si el sistema
tiene varios componentes de procesamiento —como procesadores o canales
E/S—.

* *Modularidad*. Podemos querer crear nuestro software de forma
modular, dividiendo las funciones del programa en procesos separados.
* *Conveniencia*. Incluso un usuario individual puede querer hacer
varias tareas al mismo tiempo. Por ejemplo editar, imprimir y compilar
en paralelo.

Las ejecución concurrente de procesos cooperativos requiere mecanismos
tanto para _comunicar unos con otros_ (véase los apartados
<<_memoria_compartida>> y <<_microkernel>>) como para _sincronizar
sus acciones_ (véase el <<_sincronización>>).

==== Memoria compartida

_La *memoria compartida* es una estrategia para comunicar procesos dónde
uno de ellos gana acceso a regiones de la memoria del otro_ (véase la ).
Puesto que normalmente el sistema operativo intenta evitar que un
proceso pueda tener acceso a la memoria de otro proceso (véase el
apartado <<_protección_de_la_memoria>>), para que pueda haber memoria
compartida es necesario que los dos procesos estén de acuerdo en
eliminar dicha restricción.

Dos procesos que comparten una región de la memoria pueden intercambiar
información leyendo y escribiendo datos en la misma, pero:

* _La estructura de los datos y su localización dentro de la región
compartida la determinan los procesos en comunicación_ y no el sistema
operativo.

* _Los procesos son responsables de sincronizarse_ (véase el apartado
<<_sincronización>>) para no escribir en el mismo sitio al mismo
tiempo, lo que que podría generar inconsistencias.

Las principales ventajas de la memoria compartida frente a otros
mecanismos de comunicación son:

* _**Eficiencia**, puesto que la comunicación tiene lugar a la
velocidad de la memoria principal._

* _**Conveniencia**, puesto que el mecanismo de comunicación sólo
requiere leer y escribir de la memoria_.

==== Comunicación entre procesos

_La *comunicación entre procesos* o *IPC* (Interprocess Communication)
es un mecanismo para que los procesos puedan compartir información y
sincronizar sus acciones sin necesidad de compartir el espacio de
direcciones_. Este mecanismo debe ser proporcionado por el sistema
operativo que, a diferencia de cuando se usa memoria compartida, se
encarga de la sincronización y así como de establecer el formato que
deben tener los datos. Es particularmente útil en entornos distribuidos
dónde los procesos a comunicar residen en ordenadores diferentes
conectados a una red. Por ejemplo se utiliza para comunicar un navegador
y un servidor Web en Internet.

La mejor forma de proporcionar IPC es utilizando un sistema de paso de
mensajes (véase la ).

===== Sistema de paso de mensajes

_La función de un *sistema de paso de mensajes* es permitir que los
procesos se comuniquen sin necesidad de recurrir a la compartición de
recursos_ —compartir memoria, archivos, etc.—.

El componente de IPC de cualquier sistema operativo debe proporcionar al
menos dos llamadas al sistema similares a:

* _send (message)_ para mandar mensajes a otro proceso.
* _receive (message)_ para recibir mensajes de otro proceso.

Además los diseñadores del sistema operativo deben escoger entre
implementar un componentes de IPC con _mensajes de tamaño fijo_ o
_mensajes de tamaño variable_.

* *Mensajes de tamaño fijo*. La implementación del sistema operativo
es sencilla pero la programación de aplicaciones es mucho más compleja.

* *Mensajes de tamaño variable*. La implementación del sistema
operativo es más compleja pero la programación de aplicaciones es más
simple.

Para que dos procesos se puedan comunicar es necesario que haya un
_enlace de comunicaciones_. No trataremos aquí la implementación
física del enlace —que por ejemplo puede ser mediante memoria
compartida, un bus hardware, o una red de comunicaciones— sino de su
implementación lógica.

En general existen varias opciones a la hora de implementar de manera
lógica un enlace y las correspondientes operaciones de envío y
recepción:

* _Comunicación directa o indirecta_.
* _Comunicación síncrona o asíncrona_.
* _Buffering explícito o automático_.

===== Referenciación

Los procesos que se quiera comunicar debe tener una forma de
referenciarse el uno al otro. Para ello puede utilizar la comunicación
directa o la indirecta.

====== Comunicación directa

En _la *comunicación directa* cada proceso debe nombrar explícitamente
al proceso destinatario o receptor de la información_. Por ejemplo:

* _send (P, message)_ para mandar un mensaje al proceso _P_.
* _receive (Q, message)_ para recibir un mensaje del proceso _Q_.

// TODO: Poner nota de que estas llamadas al sistema no existen.

El esquema anterior se de nomina *direccionamiento simétrico* pero
existe una variante de ese mismo esquema denominado *direccionamiento
asimétrico*.

* _En el *direccionamiento simétrico* tanto el proceso transmisor como
el receptor tienen que nombrar al otro para comunicarse_.

* _En el *direccionamiento asimétrico* sólo el transmisor nombra al
receptor, mientras que el receptor no tiene que nombrar al transmisor_.

    ** _send (P, message)_ para mandar un mensaje al proceso _P_.

    ** _receive (&id, message)_ para recibir un mensaje de cualquier
proceso. En este caso el sistema operativo asigna a la variable _id_ el
identificador del proceso transmisor del mensaje antes de volver de la
llamada al sistema.

La principal desventaja de este tipo de comunicación es que _cambiar el
identificador de un proceso requiere actualizar todas las referencias al
anterior identificador en todos los procesos que se comunican con el_.
En general cualquier técnica que requiera que los identificadores de los
procesos sean establecidos explícitamente en el código de los programas
no es deseable. Esto es así porque en muchos sistemas los
identificadores de los procesos cambian de una ejecución a otra. Por lo
tanto lo mejor sería disponer de una solución con un nivel adicional de
indirección que evite que los identificadores tenga que ser usados
explícitamente

====== Comunicación indirecta

En _la *comunicación indirecta* los mensajes son enviados a
*buzones*, *maillox* o *puertos* que son objetos dónde
los procesos pueden dejar y recoger mensajes_.

* _send (A, message)_ para mandar un mensaje al puerto _A_.
* _receive (A, message)_ para recibir un mensaje del puerto _A_.

Este tipo de comunicación da lugar a algunas situaciones que deben ser
resueltas. Por ejemplo, *¿qué pasa si los procesos P, Q y R comparten
el puerto A, P manda un mensaje, y Q y R invocan la llamada receive() en
A?*. La respuesta correcta dependerá de cuál de los siguientes métodos
escogieron los diseñadores del sistema:

* _No permitir que cada enlace esté asociado a más de dos
procesos_.

* _No permitir que más de un proceso puedan ejecutar receive()
al mismo tiempo_. Por ejemplo, haciendo que sólo el proceso que crea
el puerto tenga permiso para recibir de él. Los sistemas que optan por
esta solución suelen disponer de algún mecanismos para transferir el
permiso de recibir a otros procesos.

* _O permitir que el sistema operativo escoja arbitrariamente quién
recibe el mensaje_ si dos o más procesos ejecutan `receive()` al mismo
tiempo. La elección puede ser aleatoria o mediante algún algoritmo, por
ejemplo por turnos.

===== Buffering

Los mensajes intercambiados durante el proceso de comunicación residen
en una cola temporal. Básicamente hay tres formas de implementar dicha
cola:

* _Con *capacidad cero* o *sin buffering* la cola tiene una capacidad
máxima de 0 mensajes_, por lo tanto no puede haber ningún mensaje
esperando en el enlace. En este caso el transmisor debe bloquearse hasta
que el receptor recibe el mensaje.

* _Con **buffering automático**_:

    ** _Con *capacidad limitada* la cola tiene una capacidad limitada a N
mensaje_, por que si la cola no se llena el transmisor no espera. Sin
embargo si la cola se llena, el transmisor debe bloquearse a la espera
de haya espacio en la cola.

    ** _Con *capacidad ilimitada* la cola es de longitud
potencialmentefootnote:[Las colas de longitud real infinita son
imposibles puesto que los recursos son limitados. La longitud de estas
colas viene determinada por la memoria principal disponible, que suele
ser lo suficientemente grande para que podamos considerar que las colas
son infinitas.] infinita_, lo que permite que el transmisor nunca
espere.

===== Sincronización

La comunicación entre dos procesos tiene lugar por medio de las llamadas
`send()` y `receive()`; de tal forma que generalmente la primera se
bloquea cuando la cola de transmisión se llena —en función del tipo de
buffering— mientras que la segunda lo hace cuando la cola de recepción
está vacía.

Sin embargo existen diferentes opciones de diseño a la hora de
implementar cada una de estas primitivas en función de si se pueden
bloquear o no. Por tanto, el paso de mensajes puede ser *con bloqueo* o
*sin bloqueo*, o lo que es lo mismo *síncrono* u *asíncrono*.

* _Cuando el envío es sin bloqueo,_ el proceso transmisor nunca se
bloquea. En caso de que la cola de mensaje esté llena, la solución más
común es que la llamada `send()` vuelva con un código de retorno que
indique que el proceso debe volver a intentar el envío más tarde.

* _Cuando el envío es con bloqueo_, el proceso transmisor se bloquea
cuando no queda espacio en la cola de mensajes y hasta que pueda
depositar el mensaje en la misma.

* _Cuando la recepción es sin bloqueo_, el proceso receptor nunca se
bloquea. En caso de que la cola de mensajes esté vacía, el sistema
operativo puede indicar al proceso que lo intente más tarde a través de
un código de retorno o devolviendo un mensaje nulo.

* _Cuando la recepción es con bloqueo_, el receptor se bloquea cuando
no hay mensajes en la cola.

Diferentes combinaciones de `send()` y `receive()` son posibles. Es
decir, transmisión y recepción pueden ser síncronas o asíncronas de
manera independiente.

==== Ejemplos de mecanismos comunicación entre procesos

===== Tuberías

Las *tuberías* son un mecanismo de IPC de _comunicación indirecta_ que
está incluido en muchos sistemas operativos. La comunicación es de
_capacidad cero_ y _síncrona,_ aunque en algunos sistema operativos
también puede ser _asíncrona_.

Conceptualmente cada tubería tiene dos extremos. Un extremo permite al
proceso en ese extremo escribir en la tubería, mientras el otro extremo
permite a los procesos leer de la tubería.

Existen dos tipos de tuberías:

* _Las *tuberías anónimas* sólo existen en el espacio de direcciones
del proceso que las crea_.

    ** Los procesos hijo pueden heredar las tuberías abiertas por el proceso
padre. Usando esa capacidad de herencia _se puede comunicar un proceso
padre con sus hijos de manera privada_ (véase la ).

* _Las *tuberías con nombre* son públicas al resto del sistema_, por
lo que teóricamente son accesibles por cualquier proceso.

    ** Se suelen utilizar en aplicaciones _cliente — servidor_ dónde un
proceso servidor ofrece algún servicio a otros procesos cliente a través
de la tubería.

    ** En POSIX se denominan _FIFO_ y tienen presencia en el sistema de
archivos como archivos especiales.

** En Windows las tuberías con nombre son bidireccionales.

Por simplicidad las tuberías son tratadas de forma similar a los
archivos por lo que en ambos casos se utilizar las mismas primitivas POSIX de
E/S —`read()` y `write()`—.

// TODO: Nota sobre el caso de Windows y Linux.

===== Señales en sistemas operativos POSIX

En POSIX la forma más sencilla de comunicar dos procesos del mismo
sistema es mediante el envío de una *señal* de uno al otro.

Los procesos pueden mandar señales utilizando la llamada al sistema
`kill()`, que sólo requiere el identificador del proceso de destino y
el número de la señal. Por tanto, _estamos hablando de un mecanismo de
comunicación directa_. Cada señal tiene un efecto particular por
defecto —que por lo general es matar al proceso— en el proceso
que las recibe. Sin embargo cada proceso puede declarar un _manejador de
señales_ que redefina la acción por defecto para una señal determinada.
Un manejador de señales no es más que una función que es ejecutada
asíncronamente cuando la señal es recibida. En ese sentido _las
*señales* en POSIX puede interpretarse como una forma de interrupción
por software_.

_Las señales fueron diseñadas originalmente como un mecanismo para que
el sistema operativo notificara a los programas ciertos errores y
sucesos críticos_, no como un mecanismo de IPC. Por ejemplo:

* La señal `HUP` o `SIGHUP` es enviada a cada proceso iniciado desde una sesión
de terminal cuando dicha sesión termina.

* La señal `INT` o `SIGINT` es enviada al proceso que está enganchado a la
consola cuando el usuario pulsa el carácter de interrupción —frecuentemente la combinación de teclas kbd:[CTRL+C]—.

// TODO: Poner el ejemplo de SIGUP.

Sin embargo esto no evita que las señales puedan ser útiles como mecanismo de IPC.
No en vano el estándar POSIX incluye dos señales —`USR1` y `USR2`— especialmente indicadas para este uso.
Además las señales son utilizadas frecuentemente como medio de control de los __demonios__footnote:[Un demonio es un proceso no interactivo que se ejecuta en segundo plano en vez de ser controlado directamente por el usuario. Este tipo de programas se ejecutan de forma continua y proporcionan servicios específicos, como por ejemplo es el caso de los servidores de correo electrónico, servidores de páginas Web o de bases de datos.] del sistema.
Por ejemplo permiten que un administrador —u otro proceso— le indique a un demonio que debe reinicializarse, empezar a realizar el trabajo para el que fue diseñado o escribir su estado interno en un sitio conocido del almacenamiento.

===== Sockets

_Un *socket* es un punto final en una comunicación bidireccional entre
procesos_. Para que una pareja de procesos se pueda comunicar son
necesarios dos _sockets_ —uno para cada proceso— de manera que cada uno
de ellos representa un extremo de la conexión.

La API de _sockets_ fue creada por la Universidad de Berkeley para ser
la que abstrajera el acceso a la familia de protocolos de Internet
(TCP/IP) en el UNIX desarrollado por esa misma universidad. Sin embargo
rápidamente se convirtió en el estándar de facto para la comunicación en
red, por lo que todos los sistemas operativos modernos —incluidos los
sistemas POSIX y Microsoft Windows— tienen una implementación de la
misma.

Pese a sus orígenes, _los sockets se diseñaron para ser independientes
de la tecnología de red subyacente_. Por ejemplo:

* En las redes TCP/IP para crear un _socket_ es necesario indicar la
dirección IP y el número de puerto en el que debe de escuchar o desde el
que se debe conectar a otro _socket_. Mientras que en el momento de
establecer una conexión con ese otro _socket_, se debe indicar la
dirección IP y el número de puerto donde el _socket_ debe estar
escuchando. Esto es así porque la tecnología de red TCP/IP subyacente
establece que cada máquina tiene una IP y que los procesos se comunican
a través de los puertos en las mismas.

* En los sistemas POSIX es habitual el uso de _sockets de dominio UNIX_
para comunicar procesos dentro de un mismo sistema. Estos no son más que
_sockets_ locales identificados mediante un nombre de archivo y que, por
tanto, están representados en el sistema de archivos. Su principal
utilidad están en las aplicaciones que siguen el modelo cliente-servidor
pero donde no es interesante —o seguro— que el servicio esté disponible
a través de la red. Por ejemplo se suelen utilizar para conectar
gestores de bases de datos con aplicaciones Web servidas desde el mismo
equipo.

Si comparamos los ejemplos anteriores, podemos observar que existen grandes diferencias en cuanto a la tecnología de comunicación empleada cuando se trata de comunicar procesos en redes TCP/IP o en un mismo equipo mediante _sockets_ de dominio UNIX.
Sin embargo para ambos casos la API de _sockets_ siempre es la misma.

Los _sockets_ implementan _buffering automático_ y admiten tanto _comunicación síncrona_ como _asíncrona_, aunque el comportamiento final de la interfaz depende de la tecnología de red utilizada.

=== Hilos

Hasta el momento el modelo de proceso que hemos descrito asume que
tenemos un sólo *hilo* de ejecución, es decir, que se ejecuta en la CPU
una única secuencia de instrucciones. Un proceso con un hilo de
ejecución sólo puede realizar una tarea a la vez. Por ejemplo, en un
procesador de textos con un sólo hilo de ejecución el usuario nunca
podría escribir al mismo tiempo que se comprueba la ortografía. Por eso
muchos sistemas operativos modernos han extendido el concepto de proceso
para permitir múltiples hilos de ejecución en cada uno. Los procesos con
varios hilos pueden realizar varias tareas a la vez.

==== Introducción

_El hilo es la unidad básica de uso de la CPU en los sistemas
operativos multihilo_. De los recursos de un proceso es privado a cada
hilo (véase la ):

* *El identificador del hilo* lo identifica en el sistema de la misma
manera que lo hace el identificador de proceso con el proceso.

* *El contador de programa* indica la dirección de la próxima
instrucción del proceso que debe ser ejecutada por la CPU.

* *Los registros de la CPU*.

* *La pila* contiene datos temporales como parámetros y direcciones de
retorno de las funciones y variables locales.

Sin embargo todos los hilos de un mismo proceso comparten (véase la ):

* *El código del programa*.

* *Otras secciones de datos*, como el montón.

* Y *otros recursos del proceso* como archivos abiertos y señales.

===== Beneficios

Muchos son los beneficios que aporta la programación multihilo:

* *Respuesta*. Una aplicación multihilo interactiva puede continuar
ejecutándose aunque parte de la misma esté bloqueada o realizando una
operación lenta, mejorando la _respuesta al usuario_ de la misma. Por
ejemplo, un navegador Web multihilo puede gestionar la interacción del
usuario a través de un hilo mientras el contenido solicitado se descarga
en otro hilo.

* *Compartición de recursos*. Por defecto los hilos comparten la
memoria y los recursos del proceso al que pertenecen. El compartir el
código es lo que permite a una aplicación tener varios hilos que
realizan diferentes actividades dentro del mismo espacio de direcciones.

* *Economía*. Reservar memoria y otros recursos para la creación de un
proceso es costoso. Puesto que los hilos comparten los recursos de los
procesos a los que pertenecen es más económico crearlos. También es más
económico el cambio de contexto entre ellos ya que hay que guardar y
recuperar menos información. Por ejemplo en Oracle/Sun Microsystems
Solaris crear un proceso es 30 veces más lento que crear un hilo; y el
cambio de contexto es 5 veces más lento.

// Estadísticas más modernas ¿Linux? ¿Windows?

* *Aprovechamiento de las arquitecturas multiprocesador*. En esas
arquitecturas diferentes hilos pueden ejecutarse en paralelo en
distintos procesadores. Por el contrario un proceso monohilo sólo se
puede ejecutar en una CPU a la vez, independientemente de cuantas estén
disponibles para ejecutarlo.

===== Soporte multihilo

_Las *librerías de hilos* proporcionan al programador la API para crear
y gestionar los hilos de su proceso_. Hay dos formas fundamentales de
implementar una librería de hilos:

* La primera forma es _implementar la librería enteramente en el
espacio de usuario, sin requerir el soporte del núcleo_:

    ** Los hilos así gestionados no existen para el núcleo. Sólo existen en el espacio de usuario dentro del proceso que los ha creado.
    Por ese motivo se los denomina *hilos de usuario*.

    ** El código y los datos de la librería residen en el espacio de usuario, por lo que invocar una función de la misma se reduce a una simple llamada a una función, evitando el coste de hacer llamadas al sistema.

* La segunda forma es _implementar la librería en el núcleo_.

    ** Los hilos así gestionados son soportados y gestionados por el núcleo, quien se encarga de planificarlos en la CPU. Por ese motivo se los denomina *hilos de núcleo*.

    ** El código y los datos de la librería residen en el espacio del núcleo, por lo que invocar una función de la misma requiere frecuentemente hacer una llamada al sistema.

En la actualidad en los diferentes sistemas operativos se pueden
encontrar librerías de ambos tipos. Por ejemplo, la librería de hilos
del API Win32 es del segundo tipo mientras que la librería de hilos
POSIX Threads —frecuentemente utilizada en los sistemas POSIX— puede ser
de ambos tipos, dependiendo solamente del sistema donde se
implementefootnote:[POSIX Threads se implementa en el núcleo en los
sistemas Linux y en la mayor parte de los UNIX actuales.].

==== Modelos multihilo

Las distintas formas de implementar los hilos comentadas anteriormente
—en espacio de usuario o en el núcleo— no son excluyentes ya que en un
sistema operativo concreto se pueden implementar ambas, una de las dos o
ninguna —esto último en el caso de los sistemas operativos que no
soportan de ninguna forma múltiples hilos de ejecución—. Así que en
general debe existir una relación entre los hilos de usuario y los del
núcleo. A continuación veremos tres formas de establecer dicha relación.

===== Muchos a uno

_En un sistema operativo cuyo núcleo no soporta múltiples hilos de
ejecución la única posibilidad es utilizar una librería de hilos
implementada en el espacio de usuario. El planificador de dicha librería
se encarga de determinar que hilo de usuario se ejecuta en cada momento
en el proceso, mientras este es planificado en la CPU por el núcleo,
obviamente elegido cuando le corresponda de entre todos los procesos del
sistema._

A efectos prácticos un proceso «sin hilos» se puede
interpretar como un proceso con «un único hilo» de
ejecución en el núcleo. Por eso se dice que _en el modelo *muchos a
uno* se mapean los múltiples hilos de usuario de un proceso en el único
hilo de núcleo del mismo_ (véase la ).

Las principales características de este modelo son:

* _La gestión de hilos se hace con una librería en el espacio de
usuario_, por lo que puede ser muy eficiente. Como hemos visto
anteriormente la invocación de las funciones de la librería se hace por
medio de simples llamadas a funciones.

* _El proceso entero se bloquea si un hilo hace una llamada al sistema
que deba ser bloqueada_. Por ejemplo operaciones de E/S a archivos,
esperar a que suceda un evento, etc.

* Como sólo un hilo de usuario puede ser asignado al hilo de núcleo,
_los hilos de un mismo proceso no se pueden ejecutar en paralelo en
sistemas multiprocesador_. El planificador de la librería de hilos es
el encargado de determinar que hilo de usuario es asignado al único hilo
de núcleo del proceso y este sólo puede ejecutarse en una única CPU al
mismo tiempo.

El problema del bloqueo de procesos puede ser evitado sustituyendo las
funciones de la librería del sistema, de manera que las llamadas al
sistema que se pueden bloquear sean sustituidas por versiones con
llamadas equivalentes pero no bloqueantes. Por ejemplo, las llamadas al
sistema de E/S se pueden reemplazar por llamadas de E/S asíncrona, que
retornan inmediatamente aunque la operación no haya sido completada.
Después de cada una de estas llamadas asíncronas al sistema, la librería
del sistema invoca al planificador de la librería de hilos para que
bloquee el hilo que ha realizado la llamada y asigne el hilo de núcleo a
un nuevo hilo de usuario. Obviamente el planificador de la librería de
hilos debe estar al tanto de cuando las operaciones asíncronas son
completadas para poder volver a planificar los hilos de usuario
bloqueados. Este procedimiento es a todas luces bastante complejo y
requiere versiones no bloqueantes de todas las llamadas al sistema, así
como modificar las funciones bloqueantes de la librería del sistema para
implementar el comportamiento descrito.

// TODO: Destacar que esto hay que comprobarlo.

Ejemplos de implementaciones este modelo de hilos son la Green Threads,
una de las implementaciones de hilos para Solaris y Java, Stackless
Pythonfootnote:[Más información de Stackless Python:
http://www.stackless.com/] y GNU Portable Threadsfootnote:[Más
información de GNU Pthreads: http://www.gnu.org/software/pth/.]. Estas
implementaciones son muy útiles en los sistemas monohilo, de cara a
poder ofrecer cierto soporte de hilos a las aplicaciones, pero también
en los sistemas multihilo, ya que debido a su bajo coste en recursos y a
su alta eficiencia son ideales cuando la cantidad de hilos a crear —el
nivel de concurrencia— va a ser previsiblemente muy alta .

===== Uno a uno

_Si el núcleo del sistema operativo soporta hilos de ejecución, lo más
común es que estos sean visibles directamente en el espacio de usuario.
Por lo tanto se dice que _en el modelo *uno a uno* se mapea cada hilo
de usuario en exactamente un hilo de núcleo_ (véase la ).

Las principales características de este modelo son:

* _Permite a otro hilo del mismo proceso ejecutarse aun cuando un hilo
hace una llamada al sistema que debe bloquearse_, ya que el núcleo se
encarga de ponerlo en espera y planificar en la CPU a otro de los hilos
preparados para ejecutarse de entre todos los existentes en el sistema.

* _Permite paralelismo en sistemas multiprocesador_, ya que diferentes
hilos pueden ser planificados por el núcleo en distintos procesadores.

* Crear un hilo de usuario requiere crear el correspondiente hilo de
núcleo. Debido a que la cantidad de memoria disponible para el núcleo
suele estar limitada, _muchos sistemas restringen la cantidad máxima de
hilos soportados_.

* _Las gestión de los hilos se hace con una librería en el espacio de
núcleo_, lo que requiere utilizar llamadas al sistema.

Este modelo se utilizar en la mayor parte de los sistemas operativos
multihilo modernos. Linux, Microsoft Windows 95/98/NT/2000/XP y
superiores, y Solaris 9 y superiores, son ejemplos de sistemas
operativos que los utilizan.

===== Muchos a muchos

_En teoría debería ser posible aprovechar lo mejor de los dos modelos
anteriores. Por eso _en el modelo *muchos a muchos* se mapean los hilos
de usuario en un menor o igual número de hilos de núcleo del proceso_
(véase la ). Así los desarrolladores pueden utilizar la librería de
hilos en el espacio de usuario para crear tantos hilos como quieran. El
planificador de la librería de hilos se encarga de determinar que hilo
de usuario es asignado a que hilo de núcleo. Mientras que el
planificador de la CPU asigna la CPU a alguno de los hilos de núcleo del
sistema.

* _Los hilos de núcleo pueden ser ejecutados en paralelo en sistemas
multiprocesador_.

* _Permite a otro hilo del mismo proceso ejecutarse cuando un hilo hace
una llamada al sistema que debe ser bloqueada_, puesto que si un hilo
de usuario realiza una llamada al sistema que debe ser bloqueada, el
correspondiente hilo de núcleo quedará bloqueado. Sin embargo, el resto
de los hilos de usuario pueden seguir ejecutándose en los otros hilos de
núcleo.

Existe una variación del modelo muchos a muchos donde, además de hacer
lo comentado anteriormente, se permite que un hilo de usuario quede
ligado a un único hilo de núcleo. Esta variación se denomina en
ocasiones modelo de *dos niveles* (véase la ) y es soportada en sistemas
operativos como Solaris 8 y anteriores, IRIX, HPUX y Tru64 UNIX.

Tanto en el modelo _muchos a muchos_ como en el de _dos niveles_ es
necesario cierto grado de coordinación entre el núcleo y la librería de
hilos del espacio de usuario. Dicha comunicación tiene como objetivo
ajustar dinámicamente el número de hilos del núcleo para garantizar la
máxima eficiencia. Uno de los esquemas de comunicación se denomina
*activación del planificador* y consiste en que el núcleo informa a la
librería de hilos en espacio de usuario del bloqueo de un hilo de un
proceso. Antes de dicha notificación el núcleo se encarga de crear un
nuevo hilo de núcleo en el proceso, de manera que el planificador de la
librería pueda encargarse de asignarle alguno de los otros hilos de
usuario. Así es como se ajusta el número de hilos dinámicamente de
manera que el proceso nunca quede bloqueado.

Debido a la complejidad del mecanismo descrito anteriormente y a la
dificultad de coordinar el planificador de la libraría de hilos con el
de la CPU para obtener un rendimiento óptimo, sistemas como Linux y
Solaris —a partir de la versión 9— han optado por el modelo uno a uno.
Con el objetivo de evitar las penalizaciones de dicho modelo, los
desarrolladores de Linux han preferido concentrar sus esfuerzos en
conseguir un planificador de CPU más eficiente, así como en reducir los
costes de la creación de hilos de núcleo.

==== Sincronización

Hemos comentado anteriormente que los hilos comparten el espacio de
direcciones del proceso al que pertenecen. Al mismo tiempo distintos
procesos pueden compartir regiones de la memoria con el objeto de
cooperar en las tareas que deben desempeñar. Ambas posibilidades
introducen algunos riesgos, puesto que el acceso concurrente a los datos
compartidos puede ocasionar inconsistencias. En este apartado vamos a
discutir _cómo se puede asegurar la ejecución ordenada de hilos o
procesos cooperativos que comparten regiones de la memoria, con el fin
de mantener la consistencia de los datos_.

// TODO: Usar una nota para decir que cuando hablemos de hilos nos referimos tambien a procesos en sistemas monohilo.

===== El problema de las secciones críticas

_Llamamos *condición de carrera* a la situación donde varios procesos o hilos pueden acceder y manipular los mismos datos concurrentemente, y donde el resultado de la ejecución depende del orden particular en el que tienen lugar dichos accesos_.
Estas situaciones ocurren frecuentemente en los sistemas operativos puesto que diferentes componentes del mismo manipulan los mismos recursos interfiriendo unos con otros.

Para ilustrarlo, supongamos que dos hilos comparten una región de la memoria que contiene un vector de elementos y un contador con el número de elementos del vector:

* El primer hilo realiza varias tareas que no entraremos a describir.
Sin embargo, como resultado de esas tareas en ocasiones añade un elemento al vector e incrementa el contador que indica el número de elementos en el vector.
Es decir, el primer hilo actúa como un *productor* de elementos del vector.
A continuación mostramos una porción de la función del productor:
+
[source, c]
----
while (count == VECTOR_SIZE);

// añadir un elemento al vector
vector[count] = item;
++count;
----

* El segundo hilo también realiza varias tareas que no describiremos.
Pero para realizar esas tareas en ocasiones debe tomar un elemento del vector compartido y decrementar el contador, porque ahora habrá un elemento menos en el vector.
Es decir, el segundo hilo actúa como un *consumidor* de elementos del vector.
A continuación mostramos una porción de la función del consumidor:
+
[source, c]
----
while (count == 0);

// quitar un elemento del vector
--count;
item = vector[count];
----

Aunque las funciones del productor y del consumidor son correctas cuando no coinciden en el tiempo, no funcionan adecuadamente cuando si lo hacen.
El motivo es que los distintos hilos comparten la variable `count` y las sentencias `++count` y `--count` no tiene porque tener una instrucción en lenguaje máquina equivalente.
Por ejemplo, `++count` podría ser traducida de la siguiente manera por el compilador:

.++count
[source, c]
----
registro1 = count;
registro1 = registro1 + 1;
count = registro1;
----

Donde `registro1` representa un registro de la CPU.
De forma parecida la sentencia `--count` puede ser implementada de la siguiente manera:

.--count
[source, c]
----
registro2 = count;
registro2 = registro2 - 1;
count = registro2;
----

Donde nuevamente `registro2` representa un registro de la CPU.
Realmente, aunque `registro1` y `registro2` pueden ser el mismo registro físico, el contenido de los registros se guarda y se recupera durante los cambios de contexto de un hilo al otro, por lo que cada uno ve sus propios valores y no los del otro.

La ejecución concurrente de las sentencias `++count` y `--count` es similar a la ejecución secuencial, pero las instrucciones de lenguaje máquina de ambas sentencias en ambos hilos o procesos pueden ser entrelazadas en algún orden aleatorio.
No olvidemos que la ejecución concurrente se puede dar bien porque:

* Tenemos un sistema multiprocesador, donde ambos hilos se ejecutan a la vez en procesadores diferentes.
* O bien porque tenemos un sistema monoprocesador, donde uno de los hilos puede ser interrumpido por el sistema operativo en cualquier momento (véase el <<_planificación_expropiativa>>) para asignar la CPU al otro.

Un posible entrelazado de las instrucciones en lenguaje máquina entre hilos, suponiendo que inicialmente `count = 5`, podría ser el siguiente:

[source, c]
----
// Entra ++count
registro1 = count;          // registro1 = 5
registro1 = registro1 + 1;  // registro1 = 6
// Sale ++count y entra --count
registro2 = count;          // registro2 = 5
registro2 = registro2 - 1;  // registro2 = 4
// Sale --count y entra ++count
count = registro1;          // count = 6 <2>
// Entra --count
count = registro2;          // count = 4 <1><2>
----
<1> Llegamos al resultado incorrecto `count = 4`, indicando que hay 4 elementos en el vector cuando realmente hay 5.
<2> Si invertimos el orden de las sentencias obtendríamos el resultado, también incorrecto, `count = 6`.

Como se puede apreciar, hemos llegado a estos valores incorrectos porque hemos permitido la manipulación concurrente de la variable `count`.
Según como se entrelacen las instrucciones de `++count` y `--count` en la CPU, el resultado final podría ser: 4, 5 o 6.
Pero el único resultado correcto es 5, que es el que obtendríamos si ejecutamos las sentencias secuencialmente.

Para evitar que estas situaciones lleven a la corrupción de los datos y a caídas de servicios y sistemas debemos asegurarnos que sólo un hilo en cada momento puede manipular recursos y variables compartidas.
Por tanto, necesitamos algún tipo de mecanismo de sincronización para que mientras se ejecuta `++count` no se pueda ejecutar `--count` ni viceversa.

Una forma de controlar el acceso a los recursos compartidos es definiendo en nuestro código _secciones críticas_.
Una *sección crítica* es una porción del código dónde se accede a variables, tablas, listas, archivos y otros recursos compartidos que no deben ser accedidos al mismo tiempo por otros hilos de ejecución.
El acceso a las secciones críticas es controlado de manera que _cuando un hilo se esté ejecutando en una sección de este tipo ningún otro pueda hacerlo en la suya correspondiente para manipular los mismos recursos_.
En estos casos se dice que la ejecución es _mutuamente exclusiva_ en el tiempo.

===== Semáforos, _mutex_ y _spinlocks_

La exclusión mutua en las secciones críticas se asegura utilizando adecuadamente una serie de recursos que para ese fin proporciona el sistema operativo.
Estos recursos utilizan internamente instrucciones y otras características de la CPU incluidas por los diseñadores para resolver este tipo de problemas.
Ese es el caso de los _semáforos_.

_Los *semáforos* son un tipo de objetos del sistema operativo que nos permite controlar el acceso a una sección crítica_, por medio de dos primitivas: `wait()`
y `signal()` —o `acquire()` y `release()`, según el libro de texto—.
A continuación describimos el mecanismo de funcionamiento:

[source, cpp]
----
semaphore S(10);    // <1>

S.wait()            // <2>

 ...                // <3>

S.signal();         // <4>
----
<1> Crear el semáforo `S` inicializado a 10. Un semáforo contiene fundamentalmente un contador con el número máximo de hilos que pueden estar ejecutando el código de la sección crítica al mismo tiempo. Los semáforos con contadores inicializados a 1 se denominan *mutex* o *semáforos binarios*.

<2> Intentar entrar en la sección crítica:

    * Si el contador interno del semáforo es mayor que 0, `wait()` lo decrementa y retorna para que la ejecución continue.

    * Si el contador interno del semáforo es igual a 0, `wait()` saca al hilo de la CPU y lo pone en una cola de espera, suspendiendo así su ejecución. Básicamente, hay demasiados hilos dentro de la sección crítica.

<3> Código protegido con el semáforo. Aquí iría el código de la sección crítica en sí.

<4> Salir de la sección crítica:

    * Si el contador interno del semáforo es mayor que 0, `signal()` lo incrementa y retorna para que la ejecución continue.

    * Si el contador interno del semáforo es igual a 0, `signal()` lo incrementa y saca a uno de los hilos en la cola de espera, donde los puso `wait()`, para meterlo en la cola de preparados, dejándolo listo para entrar en la CPU. Cuando ocurra, ese hilo decrementará el contador interno del semáforo y saldrá de `wait()`, donde hasta a hora estaba atrapado. Mientras tanto `signal()` retorna y la ejecución del hilo que sale del sección crítica continua.

[NOTE]
====
Para que funcione correctamente, el semáforo S debe ser el mismo para todos los hilos que tengan secciones críticas cuya ejecución deber ser _mutuamente exclusiva_. Es decir, el semáforo S debe estar compartido entre los hilos de la misma manera que las estructuras de datos, variables y otros recursos que protege.
====

Como hemos comentado anteriormente la implementación del `wait()` y el `signal()` del semáforo debe realizarse utilizando las características proporcionadas por el hardware, de forma que el incremento, decremento y comparación del contador interno se pueda realizar de forma atómicafootnote:[Una operación o conjunto de operaciones es atómica o no interrumpible si de cara al resto del sistema parece que la operación ocurre de forma instantánea e indivisible.].

// TODO: Explicar lo de atómica y porqué.

Por otro lado existen dos alternativas desde el punto de vista de la forma en la que se implementa la espera de los hilos dentro de `wait()`:

* _El hilo puede cambiar su estado a esperado y moverse a una cola de espera asociada al semáforo_, tal y como explicamos antes.
Entonces el planificador de la CPU escogerá a otro proceso para ser ejecutado.

* _El hilo puede iterar comprobado constantemente el contador, esperando a que sea
incrementado_.
Este tipo de *espera ocupada* sólo se utiliza en el caso de esperas previsiblemente cortas, puesto que se desperdician ciclos de CPU que otro hilo podría utilizar de forma más productiva.
Por eso, para evitar que las esperas ocupadas sean demasiado largas, los sistema operativos nunca expulsan de la CPU (véase el <<_planificación_expropiativa>>) a hilos que se estén ejecutando dentro de secciones críticas controladas por semáforos con este tipo de espera.

A estos semáforos con *espera ocupada* también se los denomina *spinlocks*.
Los *spinlocks* son utilizados frecuentemente para proteger las estructuras del núcleo en los sistemas multiprocesador, cuando la tarea a realizar dentro de la sección crítica en el núcleo requiere poco tiempo y es mayor el tiempo de CPU que se pierde si se saca al hilo en espera para ejecutar otro en su lugar.

==== Otras consideraciones sobre los hilos

===== Datos específicos de hilo

Los hilos de un mismo proceso comparten los datos del mismo, siendo este
uno de los principales beneficios de la programación multihilo. Por
ejemplo todas las variables globales del programa son compartidas por
todos los hilos. Sin embargo en algunas ocasiones puede interesar
definir ciertos datos como privados a cada hilo. A esos datos se los
denomina *TSD* o _thread-specific data_ y son soportados por muchas
librerías de hilos, incluyendo el API Win32 y Pthreads, aunque no es
común que sean soportados directamente por los distintos lenguajes de
programación.

// TODO: Un ejemplo.

===== Cancelación de hilos

_La *cancelación* es la operación de terminar un hilo antes de que
termine su trabajo_. Por ejemplo, en un navegador web un hilo se puede
encargar de la interfaz de usuario mientras otros hilos se encargan de
descargar las páginas y las imágenes de la misma. Si el usuario pulsa el
botón _cancelar_ es necesario que todos los hilos que intervienen en la
descarga sean cancelados. Esto puede ocurrir de dos maneras:

* _En la *cancelación asíncrona* un hilo puede terminar inmediatamente
la ejecución de otro_. Esto puede causar problemas al no liberarse los
recursos reservados al proceso por parte del hilo —no se cierran los
archivos abiertos, no se libera la memoria, etc.—. Además si el hilo que
termina estaba modificando estructuras de datos que compartía con otros
hilos, estas podrían quedar inconsistentes.

* _En la *cancelación en diferido* el hilo comprueba periódicamente
cuando debe terminar_. Esto da al hilo una oportunidad de terminarse
así mismo de forma ordenada y en un punto dónde es seguro hacerlo. En la
terminología de Pthreads a estos puntos se los denomina *puntos de
cancelación* —o _cancellation points_— y muchas llamadas al sistema lo
son por si mismas.

// TODO: C++ no incluye mecanismos de cancelación.
// TODO: Ejemplo en Pthread y API Win 32.

===== Funciones reentrantes y seguras en hilos

A la hora de utilizar una librería en un programa multihilo es necesario
que tengamos en cuenta los conceptos de reentrante y de seguridad de
hilos:

* _Una funciónfootnote:[De ahora en adelante, cuando usemos el
término función nos estaremos refiriendo a cualquier
procedimiento, función, método, subprograma, subrutina o rutina del
programa.] es *reentrante* si puede ser interrumpida en medio de su
ejecución y mientras espera puede volver a ser llamada con total
seguridad_. Obviamente las funciones recursivas deben ser reentrantes
para poder llamarse a sí mismas una y otra vez con total seguridad. +
En el contexto de la programación multihilo ocurre una reentrada cuando,
durante la ejecución de una función por parte de un hilo, este es
interrumpido por el sistema operativo para planificar posteriormente a
otro del mismo proceso que invoca la misma función. En general una
función es reentrante si:

    ** No modifica variables estáticas o globales. Si lo hiciera sólo puede hacerlo mediante operaciones _leer-modificar-escribir_ que sean ininterrumpibles —es decir, atómicas—.

    ** No modifica su propio código y no llama a otras funciones que no sean reentrantes.

* _Una función es *segura en hilos* o *thread-safe* si al manipular
estructuras compartidas de datos lo hace de tal manera que se garantiza
la ejecución segura de la misma por múltiples hilos al mismo tiempo_.
Obviamente estamos hablando de un problema de secciones críticas, por lo
que se resuelven sincronizando el acceso a estos datos mediante el uso
de semáforos, _mutex_ u otros recursos similares ofrecidos por el
sistema operativo.

En ocasiones ambos conceptos se confunden porque es bastante común que
el código reentrante también sea seguro en hilos. Sin embargo es posible
crear código reentrante que no sea seguro en hilos y viceversa. Por
ejemplo, una función que manipule _datos específicos de hilo_
seguramente no será reentrante aunque si segura en hilos. Mientras que
una función que sólo utilice variables locales y que no invoque a otras
funciones seguramente será reentrante y segura en hilos.

===== Las llamadas al sistema fork() y exec() en procesos multihilo

¿Qué debe ocurrir si un hilo de un proceso multihilo ejecuta la llamada
`fork()`?:

* ¿El nuevo proceso debe duplicar todos los hilos?.
* ¿O el nuevo proceso debe tener un único hilo copia del que invocó a `fork()`?.

Como hemos comentado anteriormente la llamada al sistema `exec()` sustituye el programa en ejecución con el programa indicado y este inicia su ejecución en `main()`.
Esto incluye liberar toda la memoria reservada y la destrucción de todos los hilos del programa original, por lo que duplicar los hilos en el proceso hijo creado por `fork()` parece algo innecesario.

El estándar POSIX establece que si se utiliza `fork()` en un programa multihilo, el nuevo proceso debe ser creado con un sólo hilo, que será una réplica del que hizo la llamada, así como un duplicado completo del espacio de direcciones del proceso.
Sin embargo algunos sistemas UNIX tienen una segunda llamada no estándar, denominada `forkall()`, capaz de duplicar todos los hilos del proceso padre.
Obviamente sólo resulta conveniente emplearla si no se va a utilizar la llamada `exec()` a continuación.

// TODO: Nota con tener cuidado si exec() falla.

===== Manejo de señales en procesos multihilo

Una señal se utiliza en UNIX para informar a un proceso cuando un evento
a ocurrido. Existen dos tipos de señales:

* Las _**señales síncronas** se deben a alguna acción del propio
proceso_. Ejemplos de señales de este tipo son las originadas por
accesos ilegales a memoria o divisiones por 0. Las señales síncronas son
enviadas al mismo proceso que las origina.

* Las _**señales asíncronas** son debidas a procesos externos_. Un
ejemplo de este tipo de señales es la terminación de procesos con teclas
especiales como kbd:[CTRL+C] o kbd:[CTRL-D]

Las señales que llegan a un proceso pueden ser interceptadas por una función definida por el programador —que se denominada _manejador de señal_-.
En caso de que esta función no haya sido definido, se utiliza un manejador por defecto cuya acción depende del tipo de evento.

La pregunta entonces es: ¿cuándo se tienen múltiples hilos cuál de ellos debe ser interrumpido para que ejecute el manejador de señales?

* Obviamente las señales síncronas, por su propia naturaleza, deben ser enviadas al hilo que las genera.

* Con las señales asíncronas —las que vienen de fuentes externas— la cosa no está tan clara. Dependiendo del caso algunas deben ser capturadas por un sólo hilo, mientras que otras —como aquellas que ordenan terminar el proceso— deberían ser enviadas a todos para que sepan lo que va a ocurrir.

// TODO: Y las asíncronas.

La mayor parte de los UNIX multihilo permiten especificar qué señales
acepta cada hilo y cuáles no. Por lo tanto una señal asíncrona sólo será
entregada a aquellos hilos que no la bloquean. Puesto que generalmente
las señales necesitan ser manejadas una sola vez, normalmente sólo
llegan al primer hilo al que se le asigna la CPU y que no las esté
bloqueando.

=== Planificación de la CPU

_El *planificador de la CPU* o *planificador de corto plazo* selecciona
de la cola de preparados el siguiente proceso o hilo del núcleo a
ejecutar_. En dicha cola suelen estar los PCB de todos los procesos que
esperan una oportunidad para usar la CPU. Aunque se suelen pensar en la
cola de preparados como una cola FIFO, como veremos más adelante, no
tiene por qué ser así. En cualquier caso, sea cual sea el algoritmo de
planificación utilizado, éste no debe ser excesivamente lento ya que es
ejecutado con mucha frecuencia; aproximadamente una vez cada 100
milisegundos.

// ¿Invertir el orden? ¿Hablar de hilos apartir de ahora?

*Aunque a lo largo de este tema hablaremos de planificar procesos en la
CPU, en los sistemas operativos multihilo se planifican los hilos de
núcleo y no los procesos*. Por ello todo lo que comentemos a partir de
ahora se aplica de la misma manera a los hilos de núcleo, en aquellos
sistemas operativos que los soportan.

==== Planificación expropiativa

Las decisiones de planificación _se deben tomar necesariamente_ en los
siguientes casos:

. _Cuando un proceso pasa de *ejecutando* a **esperando**_. Por
ejemplo, por solicitar una operación de E/S, esperar a que un hijo
termine, etc.

. _Cuando un proceso termina_.

Cuando el planificador es invocado en alguno de los casos anteriores
decimos que tenemos un sistema operativo con *planificación cooperativa*
o *no expropiativa*.

En la planificación cooperativa cuando la CPU es asignada a un proceso,
dicho proceso la acapara hasta terminar o pasar al estado de
_esperando_. La planificación cooperativa no requiere de ningún
hardware especial, por lo que en algunas plataformas puede ser la única
opción. Por ello estaba presente en los sistemas operativos más
antiguos, como Microsoft Windows 3.1 y Mac OS.

// TODO: Indicar que hablamos del antiguo sistema operativo de MAC.

Sin embargo, las decisiones de planificación _también pueden ser
tomadas en otros casos_:

. _Cuando ocurre una interrupción del temporizador_.

. _Cuando un proceso pasa de *esperando* a **preparado**_. Por ejemplo
porque para un proceso ha terminado la operación de E/S por la que
estaba esperando.

Cuando el planificador es invocado en los cuatro casos decimos que
tenemos planificación *expropiativa* o *apropiativa*. La planificación
expropiativa si requiere de un soporte adecuado por parte del hardware,
por lo que se utiliza en la mayor parte de los sistemas operativos
modernos. Ejemplos de estos sistemas son Microsoft Windows
9x/NT/2000/XP, macOS, GNU/Linux y los UNIX modernos.

// TODO: Actualizar lo de las versiones de Windows.

La utilización de un planificador expropiativo introduce algunas
dificultades adicionales:

* Puesto que un proceso puede ser expropiado en cualquier momento, el
sistema operativo debe proporcionar _mecanismos de sincronización_
(véase el <<_sincronización>>) para coordinar el acceso a
datos compartidos que podrían estar siendo modificados por el proceso
que abandona la CPU.

* ¿Qué pasa si un proceso va a ser expropiado cuando se está ejecutando
una llamada al sistema? No debemos olvidar que generalmente dentro del
núcleo se manipulan datos importantes que deben permanecer consistentes
en todo momento. Para resolver esta cuestión los diseñadores pueden
optar por _impedir la expropiación dentro del núcleo_. Es decir, antes
de hacer el cambio de contexto, que sacaría al proceso de la CPU, se
espera a que la llamada se complete o se bloquee pasando el proceso al
estado de _esperando_. Esto permite núcleos simples y garantiza que
las estructuras del mismo permanezcan consistentes, pero es un modelo
pobre en sistemas de tiempo real o multiprocesador. Exploraremos otras
soluciones más adelante (véase el <<_planificación_de_tiempo_real>>).

==== El asignador

_El *asignador* es el componente que da el control de la CPU al proceso
seleccionado por el planificador de corto plazo_. Esta tarea implica
realizar las siguientes funciones:

* Cambiar el contexto.

* Cambiar al modo usuario.

* Saltar al punto adecuado del programa para continuar con el proceso.

Puesto que el _asignador_ es invocado para cada conmutación entre
procesos, es necesario que el tiempo que tarda en detener un proceso e
iniciar otro sea lo más corto posible. _Al tiempo que transcurre desde
que un proceso es escogido para ser planificado en la CPU hasta que es
asignado a la misma se lo denomina **latencia de asignación**_.

==== Criterios de planificación

Los diferentes algoritmos de planificación de la CPU tienen diversas
propiedades que pueden favorecer a una clase de procesos respecto a
otra. Por ello es interesante disponer de algún criterio para poder
comparar dichos algoritmos y determinar cual es el mejor. Se han
sugerido muchos criterios para comparar los algoritmos de planificación
de CPU pero la elección de uno u otro puede crear una diferencia
sustancial a la hora de juzgar cual es el mejor. A continuación
presentamos los criterios más comunes.

===== Criterios a maximizar

* *Uso de CPU*: Un buen _planificador debería mantener la CPU lo más
ocupada posible_. El uso de CPU es la proporción de tiempo que se usa
la CPU en un periodo de tiempo determinado. Se suele indicar en tanto
por cierto.
+
[stem]
++++
bb "uso de CPU" = "tiempo que la CPU permanece ocupada" / "tiempo durante el que se toma la medida" "%"
++++

* *Tasa de procesamiento*: Cuando la CPU está ocupada es porque el
trabajo se está haciendo. Por tanto _una buena medida del volumen de
trabajo realizado puede ser el número de tareas o procesos terminados
por unidad de tiempo. _A dicha magnitud es a la que denominamos como
_tasa de procesamiento_.
+
[stem]
++++
bb "tasa de procesamiento" = "numero de procesos terminados" / "tiempo durante el que se toma la medida" "procesos/s"
++++

===== Criterios a minimizar


* *Tiempo de ejecución*: Es el _intervalo de tiempo que transcurre
desde que el proceso es cargado hasta que termina_.

* *Tiempo de espera*: Es la _suma de tiempos que el proceso permanece
a la espera en la cola de preparados_. Evidentemente esta medida de
tiempo no incluye el tiempo de espera debido a las operaciones de E/S.

* *Tiempo de respuesta*: Es _el intervalo de tiempo que transcurre
desde que se le lanza un evento —se pulsa una tecla, se hace clic con el
ratón o llega un paquete por la interfaz de red— hasta que se
produce la primera respuesta del proceso_. Evidentemente esto mide el
tiempo que se tarda en responder y no el tiempo de E/S, mientras que el
tiempo de ejecución sí suele estar limitado por la velocidad de los
dispositivos E/S.

===== Elección del criterio adecuado

En función del tipo de sistema o de la clase de trabajos que se van a
ejecutar puede ser conveniente medir la eficiencia del sistema usando un
criterio u otro. Esto a su vez beneficiará a unos algoritmos de
planificación frente a otros, indicándonos cuáles son los más eficientes
para nuestra clase de trabajos en particular.

En general podemos encontrar dos clases de trabajos para los que puede
ser necesario evaluar la eficiencia del sistema de manera diferente.:

* En los sistemas interactivos —ya sean sistemas de escritorio o
_mainframes_ de tiempo compartido— los procesos pasan la mayor parte
del tiempo esperando algún tipo de entrada por parte de los usuarios. En
este tipo de sistemas el tiempo de ejecución no suele ser el mejor
criterio para determinar la bondad de un algoritmo de planificación, ya
que vendrá determinado en gran medida por la velocidad de la entrada de
los usuarios. Por el contrario se espera que el sistema reaccione lo
antes posible a las órdenes recibidas, lo que hace que _el tiempo
de respuesta se el criterio más adecuado_ para evaluar al planificador
de la CPU. Además el tiempo de respuesta se reduce generalmente cuando
el tiempo que pasan los procesos interactivos en la cola de preparados
también lo hace —tras haber sido puestos ahí por la ocurrencia de algún
evento— por lo que también _puede ser una buena idea utilizar como
criterio el tiempo de espera_. Esta selección de criterios no sólo es
adecuada para los sistemas interactivos, ya que existen muchos otros
casos donde es interesante seleccionar un planificador de la CPU que
minimice el tiempo de respuesta. Esto por ejemplo ocurre con algunos
servicios en red como: sistemas de mensajería instantánea, chats,
servidores de videojuegos, etc.

* Por el contrario en los _mainframes_ de procesamiento por lotes y
multiprogramados, en los superordenadores que realizan complejas
simulaciones físicas y en los grandes centros de datos de proveedores de
Internet como Google, lo de menos es el tiempo de respuesta y lo
realmente importante es realizar cada tarea en el menor tiempo posible.
Por eso en ese tipo de sistemas _es aconsejable utilizar criterios
tales como el tiempo de ejecución o la tasa de procesamiento_.

Obviamente estos criterios varían de un proceso a otro, por lo que
normalmente lo que se busca es optimizar los valores promedios en el
sistema. Sin embargo no debemos olvidar que _en muchos casos puede ser
más conveniente optimizar el máximo y mínimo de dichos valores antes que
el promedio_. Por ejemplo, en los sistemas interactivos es más
importante minimizar la varianza en el tiempo de respuesta que el tiempo
de respuesta promedio, puesto que para los usuarios un sistema con un
tiempo de respuesta predecible es más deseable que uno muy rápido en
promedio pero con una varianza muy alta.

==== Ciclo de ráfagas de CPU y de E/S

El éxito de la planificación de CPU depende en gran medida de la
siguiente propiedad que podemos observar en los procesos: _La ejecución
de un proceso consiste de ciclos de CPU y esperas de E/S, de forma que
alternan entre estos dos estados. La ejecución empieza con una ráfaga de
CPU, seguida por una ráfaga de E/S, que a su vez es seguida por otra de
CPU y así sucesivamente. Finalmente la última ráfaga de CPU finaliza con
una llamada al sistema —generalmente exit()— para terminar la ejecución
del proceso_.

La curva que relaciona la frecuencia de las ráfagas de CPU con la
duración de las mismas tiende a ser exponencial o hiper-exponencial
(véase la ) aunque varía enormemente entre procesos y sistemas
informáticos distintos. Esto significa que los procesos se pueden
clasificar entre aquellos que presentan un gran número de ráfagas de CPU
cortas o aquellos con un pequeño número de ráfagas de CPU largas.
Concretamente:

* Decimos que un _proceso es *limitado por la E/S* cuando presenta
muchas ráfagas de CPU cortas, debido a que si es así pasa la mayor parte
del tiempo esperando por la E/S_.

* Decimos que un _proceso está *limitado por la CPU* cuando presenta
pocas ráfagas de CPU largas, debido a que si es así hace un uso
intensivo de la misma y a penas pasa tiempo esperando por la E/S_.

Esta distinción entre tipos de procesos puede ser importante en la
selección de un algoritmo de planificación de CPU adecuado. En general:

* _El algoritmo escogido debe favorecer
—planificándolos antes— a los procesos limitados por la E/S_, evitando
así que los procesos limitados por la CPU —que son los que tienden a
usarla más tiempo— la acaparen. Si eso ocurriera, los procesos limitados
por la E/S se acumularían en la cola de preparados, dejando vacías las
colas de dispositivos. A este _fenómeno tan negativo que provoca una
infrautilización de los dispositivos de E/S se lo denomina **efecto
convoy**_.

* Además planificar primero a los procesos limitados por la E/S tiene
dos efectos muy positivos:

    ** _Los procesos interactivos son generalmente procesos limitados por la E/S, por lo que planificarlos primero hace que mejore el tiempo de respuesta_.

    ** __Generalmente el tiempo de espera promedio se reduce cuando se planifican primero los procesos con ráfagas de CPU cortas__footnote:[En la literatura sobre algoritmos de planificación de la CPU se indica que SJF (_Shortest-Job First_) y SRTF (_Shortest-Remaing-Time First_) son los óptimos respecto al tiempo de espera promedio precisamente porque siempre escogen al proceso con la ráfaga de CPU más corta de entre los que esperan en la cola de preparados.], Según las definiciones anteriores, estos procesos son precisamente los limitados por la E/S.

==== Planificación

Hasta el momento hemos
considerado la cola de preparados como una estructura donde los procesos
que están preparados para ser ejecutados se ordenan y se escogen según
el criterio del algoritmo de planificación. Aunque a lo largo de todo el
tema <<_gestión_de_procesos>> se puede haber intuido que dicha cola es de tipo
FIFO —lo que se conoce como algoritmo de planificación FCFS o _First
Come, First Served_— ya al principio del <<_planificación_de_la_cpu>>
indicamos que no tiene porqué ser así pues existen muchos otros
algoritmos —SJF o _Shortest-Job First_, SRTF o _Shortest-Remaing-Time
First_, RR o _Round-Robin_, por prioridades, etc.— que pueden ser
preferibles en función del criterio que utilicemos para evaluar la
eficiencia de los mismos.

Sin embargo en los sistemas operativos modernos realmente las cosas son
un poco más complejas ya que generalmente se utiliza algún tipo de
*planificación con colas multinivel*. _En este tipo de
planificación _no existe una única cola de preparados sobre la que se
utiliza un único algoritmo de planificación sino que_:

* _La cola de preparados se divide en varias colas separadas_ y los
procesos son asignados a alguna de dichas colas en base a
características de los mismos.

* _Cada cola puede tener un algoritmo de planificación de la CPU
distinto_. Es decir, alguno de los que hemos mencionado anteriormente y
que se estudiarán en las clases de problemas.

* _Mediante un algoritmo determinado se debe seleccionar la cola que
debe escoger al siguiente proceso a ejecutar._

Precisamente una cuestión interesante es la indicada en éste último
punto ¿cómo seleccionar la cola que debe escoger al siguiente proceso
que debe ser ejecutado?.

===== Prioridad fija

Aunque existen muchas maneras de clasificar los procesos entre las
diferentes colas, lo más común en los sistemas operativos modernos es
hacerlo en base a la prioridad de los procesos (véase la ):

* _A cada proceso se le asigna una prioridad_.

* _En la cola de preparados hay una cola para cada nivel de
prioridad_.

* _Los procesos, al entrar en la cola de preparados, son insertados en
aquella cola que coincide con su prioridad_.

* _El planificador escoge primero siempre la cola de prioridad más alta
que no esté vacía_.

====== Definición de las prioridades

Las prioridades se suelen indicar con números enteros en un rango fijo.
Por ejemplo [0-7], [0-31], [0-139] o [0-4095]. En algunos sistemas
operativos los números más grandes representan mayor prioridad, mientras
que en otros son los procesos con números más pequeños los que se
planifican primero. _En éste curso utilizaremos la convención de que a
menor valor mayor prioridad_.

En los sistemas con prioridad fija:

* Una vez se asigna una prioridad a un proceso ésta nunca cambia.

* _Las prioridades normalmente vienen determinadas por criterios ajenos al sistema operativo_.
Por ejemplo: la importancia del proceso, la cantidad de dinero pagada para el uso del sistema u otros factores políticos.
_A este tipo de prioridades se las denomina definidas externamente_.

====== Planificación expropiativa o cooperativa

La planificación con prioridades puede ser expropiativa o cooperativa.
_En el caso expropiativo cuando un proceso llega a la cola de
preparados su prioridad es comparada con la del proceso en ejecución, de
manera que el segundo es expulsado si la prioridad del primero es
superior a la suya_. Obviamente en la planificación cooperativa los
nuevos procesos simplemente son insertados en la cola que les
corresponde en base a su prioridad, independientemente de si tienen o no
mayor prioridad que el que se esté ejecutando.

====== Planificación entre procesos con la misma prioridad

Cada cola en cada nivel de prioridad puede tener cualquier algoritmo de
planificación de CPU, lo que virtualmente significa que el abanico de
posibilidad es muy amplio. Sin embargo lo más común es que los
diseñadores del sistema opten por utilizar o bien el planificador FCFS o
bien el RRfootnote:[Los algoritmos FCFS y RR se pueden combinar de
múltiples maneras. En algunos sistemas todas las colas son o bien FCFS o
bien RR, mientras que en otros unas colas pueden ser de un tipo y otras
del otro. Por ejemplo, en el núcleo Linux las prioridades más altas —las
etiquetadas como de tiempo real— tienen tanto una cola FCFS como una
cola RR. En cada prioridad primero se planifican los procesos de la cola
FCFS y después lo de la cola RR.].

En la planificación *FCFS* (_First
Come, First Served_) o _primero que llega, primero servido_ la cola es
FIFO:

* _Los procesos que llegan se colocan al final de la cola que les
corresponde_.

* _El proceso asignado a la CPU se coge siempre del principio de la
cola seleccionada_.

El algoritmo *RR* (_Round-Robin_) es similar
al FCFS pero utilizando el temporizador para expropiar la CPU a los
procesos a intervalos regulares, alternando así entre ellos de manera
que se da a todos los procesos la oportunidad de ejecutarse. Como se
puede intuir, fue diseñado para los sistemas de tiempo compartido,
siendo ampliamente utilizado en cualquier sistema operativo de propósito
general moderno.

El algoritmo RR requiere los siguientes elementos:

* _Se define una ventana de tiempo o *cuanto*_, generalmente entre 10
y 100 ms.

* _La cola RR se define como una cola circular dónde el planificador
asigna la CPU a cada proceso en intervalos de tiempo de hasta un
cuanto_.

Cuando se utilizar la planificación RR el tamaño del cuanto es un factor
clave en la eficiencia del planificador:

* _Cuando se reduce el tiempo del cuanto, el tiempo de respuesta y el
tiempo de espera promedio tienden a mejorar_. Sin embargo el número de
cambios de contexto será mayor, por lo que la ejecución de los procesos
será mas lenta. Además es importante tener en cuenta que interesa que el
tiempo del cuanto sea mucho mayor que el tiempo del cambio de contexto;
pues si por ejemplo el tiempo del cambio de contexto es un 10% del
tiempo del cuanto, entonces alrededor del 10% de CPU se perdería en
cambios de contexto.

* _Cuando se incrementa el tiempo del cuanto, el tiempo de espera
promedio se incrementa_ dado que entonces el RR tiende a comportarse
como un FCFS, que suele tener grandes tiempos de espera promedio. Además
se puede observar experimentalmente que el tiempo de ejecución promedio
generalmente mejora cuantos más procesos terminan su próxima ráfaga de
CPU dentro del tiempo del cuantofootnote:[Por ejemplo, dados tres
procesos con una duración cada uno de ellos de 10 unidades de tiempo y
cuanto igual a 1, el tiempo de ejecución promedio será de 29 unidades.
Sin embargo si el cuanto de tiempo fuera 10, el tiempo de ejecución
promedio caería a 20 unidades de tiempo.]. Por lo tanto nos interesan un
cuanto grande para que más procesos terminen su siguiente ráfaga dentro
del mismo.

La _regla general que siguen los diseñadores es intentar que el 80% de
las ráfagas de CPU sean menores que el tiempo de cuanto_. Se busca así
equilibrar los criterios anteriores, evitando que el tiempo de cuanto
sea demasiado grande o demasiado cortofootnote:[De manera práctica
actualmente se utilizan tiempos de cuanto de entre 10 y 100 ms. Estos
tiempos son mucho mayores que los tiempos de cambios de contexto, que
generalmente son inferiores a 10µs.].

// TODO: Ejemplo con Windows

====== Muerte por inanición y otros inconvenientes

El principal problema de este tipo de planificación es el _bloqueo
indefinido_ o *muerte por inanición*, puesto que el algoritmo puede
dejar a los procesos de baja prioridad esperando indefinidamente si hay
un conjunto de procesos de mayor prioridad demandando CPU continuamente.

Además, como vimos en el <<_ciclo_de_ráfagas_de_cpu_y_de_e_s>>, es conveniente
favorecer a los procesos limitados por la E/S frente a los procesos
limitados por la CPU para evitar el _efecto convoy_ y para mejorar los
tiempos tanto de espera como de respuesta promedio. Lamentablemente este
tipo de planificación con _prioridad fija no es capaz de hacerlo ya que
la prioridad de los procesos viene determinada exclusivamente por
criterios externos al funcionamiento del sistema operativo_.

===== Prioridad dinámica

La mayor parte de los sistemas operativos modernos de propósito
generalfootnote:[Microsoft Windows, macOS, Oracle/Sun Microsystems
Solaris, las versiones de Linux anteriores a la 2.6.23 y, en general,
casi la totalidad de los sistemas operativos modernos de propósito
general utilizan este tipo de planificación de prioridades dinámicas con
RR como planificador en cada prioridad.] _solucionan los inconvenientes
de la planificación con prioridad fija permitiendo que la prioridad de
los procesos se ajuste dinámicamente_ bajo su propio criterio:

* Por ejemplo, _una solución al problema de la muerte por inanición es
utilizar un mecanismo de **envejecimiento**_ que aumente gradualmente la
prioridad de los procesos mientras están esperando en la cola de
preparados —por ejemplo 1 nivel de prioridad cada 15 minutos—. De esta
manera los procesos de baja prioridad tarde o temprano tendrán
oportunidad de ejecutarse. Con este mecanismo una vez consiguen
ejecutarse, se les restablece su prioridad original.

* _Para favorecer en la planificación a los procesos limitados por la
E/S el sistema puede añadir o quitar prioridad a los procesos, respecto
a su prioridad fija, en función de medidas internas del sistema
operativo_. Por ejemplo se puede tomar en consideración: límites de
tiempo, necesidades de memoria, número de archivos abiertos, la
proporción entre el tiempo de ráfaga de E/S promedio y el de ráfaga de
CPU promedio del proceso, etc. Obviamente el objetivo suele ser mejorar
el rendimiento del sistema priorizando unos procesos respecto a otros.

El resultado de estas políticas es que la prioridad que finalmente
utiliza el sistema operativo para planificar los procesos en un valor
calculado dinámicamente a partir de intereses externos y medidas
internas. Por lo tanto los procesos pueden cambiar múltiples veces de
cola durante su tiempo de vida. _A la planificación de múltiples
niveles donde los procesos pueden cambiar de una cola a otra se la
denomina **planificación con colas multinivel realimentadas**_.

// TODO: Más del ejemplo de Windows.

===== Planificación por reparto proporcional

Hasta el momento hemos hablado de planificadores que se concentran en
cuál es el proceso más importante que debe ser ejecutado en cada
instante. Sin embargo otra opción, desde el punto de vista de la
planificación ,es repartir el tiempo de CPU entre los procesos a un
ritmo controlado. Esto es precisamente lo que hace _la *planificación
equitativa* (Fair Scheduling) que intenta repartir por igual el tiempo
de CPU entre los procesos de la cola de preparados_. Por ejemplo, si 4
procesos compiten por el uso de la CPU, el planificador asignará un 25%
del tiempo de la misma a cada uno. Si a continuación un usuario iniciase
un nuevo proceso, el planificador tendría que ajustar el reparto
asignando un 20% del tiempo a cada uno. El algoritmo de planificación
equitativa es muy similar al algoritmo RR pero, a diferencia de este
último en el que se utiliza un cuanto de tamaño fijo, _la ventana de
tiempo se calcula de dinámicamente para garantizar el reparto equitativo
de la CPU_.

Al igual que en los algoritmos anteriores, en ocasiones puede ser
interesante priorizar unos procesos frente a otros, tanto por motivos
ajenos al sistema operativo como por motivos internos. Por ejemplo se
puede querer favorecer a los procesos limitados por la E/S para mejorar
la eficiencia del sistema, tal y como comentamos en el apartado
<<_ciclo_de_ráfagas_de_cpu_y_de_e_s>>. La _planificación equitativa_ resuelve este
problema asignando proporcionalmente más tiempo de CPU a los procesos
con mayor prioridad. __A esta generalización del planificador equitativo
se la conoce como **planificador equitativo ponderado**__footnote:[Linux
desde la versión 2.6.23 utiliza un tipo de *planificador equitativo
ponderado* denominado *CFS* (_Completely Fair Scheduler_) o
*planificador completamente justo.*].

// TODO: Ejemplo de Linux.

==== Planificación de tiempo real

En el <<_sistemas_de_tiempo_real>> discutimos la importancia de los
sistemas de tiempo real. A continuación, describiremos las
funcionalidades necesarias para soportar la ejecución de procesos en
tiempo real dentro de un sistema operativo de propósito general.

===== Tiempo real estricto

Los sistemas de *tiempo real estricto* son necesarios para realizar tareas críticas que deben
ser completadas dentro de unos márgenes de tiempo preestablecidos.
Generalmente las tareas son entregas al sistema operativo junto con una
declaración de las restricciones de tiempo —periodicidad y límite de
tiempo— y la cantidad de tiempo que necesitan para ejecutarse. El
planificador sólo admitirá las tareas si puede garantizar el
cumplimiento de las restricciones de tiempo, rechazándolas en caso
contrario. El proporcionar estas garantías requiere que el planificador
conozca exactamente el tiempo máximo que se tarda en realizar todas y
cada una de las funciones del sistema operativo. Esto es imposible en
sistemas con almacenamiento secundario o memoria virtual, ya que
introducen variaciones no controladas en la cantidad de tiempo necesario
para ejecutar una tarea. Por tanto, el _tiempo real estricto no es
compatible con los sistemas operativos de propósito general_, como los
de tiempo compartido.

===== Tiempo real flexible

La ejecución de procesos de *tiempo real flexible* es menos
restrictiva. Tan sólo requiere que los procesos críticos reciban mayor
prioridad que los que no lo son. Esto es compatible con los sistemas de
tiempo compartido, aunque _puede generar excesos en la cantidad de
recursos asignados a los procesos de tiempo real, así como inanición y
grandes retardos en la ejecución del resto de los procesos_. Sin
embargo esto nos permite conseguir sistemas de propósito general que
soporten multimedia, videojuegos y otras tareas que no funcionarían de
manera aceptable en un entorno que no implementara tiempo real flexible.
Por ello la mayor parte de los sistemas operativos modernos soportan
este tipo de tiempo real.

Implementar el soporte de tiempo real flexible en un sistema operativo
de propósito general requiere:

    * Sistema operativo con planificación con prioridades. __Los procesos de
tiempo real deben tener la mayor prioridad. Además, no deben ser
afectados por ningún mecanismo de envejecimiento o
bonificación__footnote:[Linux, Microsoft Windows y la mayor parte de los
sistemas operativos modernos de propósito general dividen el rango de
prioridades en dos partes. El conjunto de prioridades más altas son
prioridades de tiempo real y por tanto son fijas. Mientras que el grupo
de prioridades más bajas son de tiempo no real y dinámicas. Además el
planificador se implementa de tal manera que un proceso con prioridad
dinámica nunca puede alcanzar el rango de prioridades de tiempo
real.], que sí puede afectar a los procesos de tiempo no real.

    * _Baja latencia de asignación_. Cuanto menor es la latencia más
rápido comenzará a ejecutarse el proceso de tiempo real después de ser
seleccionado por el planificador de la CPU.

Mientras que el primer
requerimiento es bastante sencillo de conseguir, el segundo es mucho más
complejo. Muchos sistemas operativos tienen un núcleo no expropiable.
Estos núcleos no pueden realizar un cambio de contexto mientras se está
ejecutando código del núcleo —por ejemplo debido a una llamada al
sistema— por lo que se ven obligados a esperar hasta que la tarea que se
esté realizando se termine antes de asignar la CPU a otro proceso. Esto
aumenta la _latencia de asignación_ dado que algunas llamadas al
sistema pueden ser muy complejas y requerir mucho tiempo para ser
completadas. Con el objetivo de resolverlo existen diversas
alternativas:

====== Puntos de expropiación

Una posibilidad es _hacer que el código del núcleo sea expropiable_.
Esto se consigue introduciendo *puntos de expropiación* en diversos
lugares _seguros_ dentro del código. En dichos puntos se comprueba si
algún proceso de prioridad más alta está en la cola de preparados. En
caso de que sea así se expropia la CPU al proceso actual y se le asigna
al proceso de más alta prioridad.

Debido a la función que realizan los puntos de expropiación, sólo pueden
ser colocados en lugares seguros del código del núcleo. Es decir, sólo
pueden estar situados allí donde no se interrumpe la modificación de
estructuras de datos. Sin embargo esto limita el número de puntos que
pueden ser colocados, por lo que la latencia de asignación puede seguir
siendo muy alta para algunas tareas muy complejas del código del núcleo.

// TODO: Ejemplo de Linux.

====== Núcleo expropiable

Otra posibilidad es _diseñar un núcleo completamente expropiable_.
Puesto que en este caso la ejecución de cualquier tarea en el núcleo
puede ser interrumpida en cualquier momento por procesos de mayor
prioridad —que el que actualmente tiene asignada la CPU— es necesario
proteger las estructuras de datos del núcleo con mecanismos de
sincronización, lo que hace que el diseño de un núcleo de estas
características sea mucho más complejo.

Supongamos que un proceso de baja prioridad es interrumpido, porque hay
un proceso de alta prioridad en la cola de preparados, mientras accede a
una importante estructura de datos del núcleo. Durante su ejecución el
proceso de alta prioridad podría intentar acceder a la misma estructura
que manipulaba el proceso de baja prioridad cuando fue interrumpido.
Debido al uso de mecanismos de sincronización el proceso de alta
prioridad tendría que abandonar la CPU a la espera de que el de baja
libere el acceso. Sin embargo este tardará en ser asignado a la CPU
mientras haya algún otro proceso de alta prioridad en la cola de
preparados. Además otros procesos puede irse añadiendo a la cola de
espera del mecanismo de sincronización que regula el acceso a la
estructura de datos del núcleo. Al hecho de que un proceso de alta
prioridad tenga que esperar por uno de baja se le conoce como
*inversión de la prioridad*. Para resolverlo se utiliza un *protocolo
de herencia de la prioridad* dónde un proceso de baja prioridad hereda
la prioridad del proceso de más alta prioridad que espera por un recurso
al que el primero está accediendo. En el momento en que el proceso de
baja prioridad libere el acceso a dicho recurso, su prioridad retornará
a su valor original.

Linux 2.6, Solaris y Microsoft Windows NT/2000/XP son algunos ejemplos
de sistemas operativos con núcleos expropiables. En el caso concreto de
Solaris la latencia de asignación es inferior a 1 ms. mientras que con
la expropiación del núcleo desactivada ésta puede superar los 100 ms.

Lamentablemente el _conseguir baja latencia de asignación no tiene
coste cero_. El hecho de que el núcleo sea expropiable aumenta el
número de cambios de contexto, lo que reduce el rendimiento del sistema
a cambio de una mejor respuesta. Por ello resulta muy interesante para
aplicaciones de tiempo real, multimedia y sistemas interactivos pero es
poco adecuado para servidores y computación de alto rendimiento. Es por
eso que Linux 2.6 permite escoger entre tener un núcleo expropiativo,
usar puntos de expropiación o nada de lo anterior. De esta forma Linux
está preparado tanto para servidores como para sistemas de escritorio o
de tiempo real.

// TODO: Ejemplo de Linux

==== Planificación en sistemas multiprocesador

Para tratar el problema de la planificación en los sistemas
multiprocesador nos limitaremos al caso de los __sistemas
homogéneos__footnote:[Un ejemplo de lo contrario —de sistema heterogéneo— se puede observar en los PC modernos donde muchos disponen tanto de una CPU como de una GPU
especializada en el procesamiento de gráficos y en las operaciones de
coma flotante.]. En dichos sistemas los procesadores son idénticos,
por lo que cualquiera de ellos puede ejecutar cualquier proceso. Esto es
bastante común y simplifica el problema de la planificación. Aun así no
debemos olvidar que incluso en el caso de los sistemas homogéneos pueden
aparecer limitaciones en la planificación. Por ejemplo:

    * Un dispositivo de E/S puede estar conectado mediante un bus privado a
un procesador en particular. En ese caso los procesos que quieren
utilizar ese dispositivo deben ejecutarse en dicho procesador.

* Los procesadores
SMTfootnote:[El _HyperThreading_ disponible en algunos procesadores de
Intel es una implementación de la tecnología _Simultaneous
Multithreading_.] (_Simultaneous Multithreading_) permiten la
ejecución concurrente de varios hilos como si de varias CPU se tratara.
Sin embargo, al no disponer cada hilo de una CPU completa es posible que
algunos deban esperar a que algún otro libere unidades de la CPU que le
son necesarias. Eso debe ser tenido en cuenta por el planificador con el
fin de optimizar el rendimiento del sistema.

Al margen de estas cuestiones, existen diversas posibilidades a la hora
de enfrentar el problema de la planificación en un sistema
multiprocesador:

    * Cuando utilizamos **multiprocesamiento asimétrico**footnote:[En los
sistemas de _multiprocesamiento asimétrico_ hay una CPU maestra y varias
esclavas a quienes la primera entrega el trabajo. En ocasiones las CPU
esclavas se distinguen por haber sido diseñadas para realizar algún tipo
de trabajo de forma eficiente —como es el caso las GPU, que no son sino
CPU diseñadas para el procesamiento de gráficos— o por el hardware al
que están conectadas —como por ejemplo las CPU unidas a discos para
gestionarlos—.] todas las decisiones de planificación, procesamiento de
E/S y otras actividades son gestionadas por un único procesador, el
_servidor_ o _maestro_. El resto de procesadores se limitan a ejecutar
el código de usuarios que les es asignado. Este esquema _es sencillo
puesto que evita la necesidad de compartir estructuras de datos entre el
código que se ejecuta en los procesadores_.

* Cuando utilizamos **multiprocesamiento simétrico**footnote:[En los
sistemas de _multiprocesamiento simétrico_ o _SMP_ (_Symmetric
Multiprocessing_) todos los procesadores son iguales. Todos comparten
los mismos recursos, pueden acceder a los mismos dispositivos y cada uno
ejecuta una copia del núcleo del sistema operativo. Por lo tanto el
sistema operativo debe saber compartir los recursos y repartir la carga
entre las CPU. Casi todos los sistemas multiprocesador modernos son de
este tipo.] o _SMP_ cada procesador ejecuta su propia copia del núcleo
del sistema operativo y se auto-planifica mediante su propio
planificador de CPU. En estos sistemas nos podemos encontrar con varias
alternativas:

    ** Algunos sistemas disponen de _una cola de preparados común para
todos los procesadores_. Puesto que se mira en una única cola, _todos
los procesos pueden ser planificados en cualquier procesador_. Este
esquema requiere el uso mecanismos de sincronización debido a que hay
estructuras de datos que se comparten entre todos los núcleos. En caso
contrario varios procesadores podrían escoger y ejecuta el mismo proceso
a la vez.

    ** Por el contrario otros sistemas disponen de _una cola
de preparados para cada procesador_. El mayor inconveniente de esta
solución es que puede generar desequilibrios entre los procesadores, ya
que un procesador puede acabar desocupado —con la cola de preparados
vacía— mientras otro está muy ocupado.

Muchos sistemas operativos modernos implementan el esquema SMP con una
cola de preparados común. Esto incluye Microsoft Windows NT/2000/XP,
Solaris, macOS y versiones anteriores a Linux 2.6. Sin embargo, esta
solución presenta algunos inconvenientes:

* La posibilidad de que un proceso se pueda ejecutar en cualquier CPU
—aunque parezca beneficiosa— es negativa desde el punto de vista de que
dejan de ser útiles las cachés de los procesadores, penalizando
notablemente el rendimiento del sistema. Por eso realmente la mayoría de
los sistemas operativos de este tipo intenta evitar la migración de
procesos de un procesador a otro. A esto se lo conoce con el nombre de
*afinidad al procesador*.

* Los mecanismos de sincronización requeridos para controlar el acceso a
la cola de preparados pueden mantener a los procesadores mucho tiempo
desocupados —mientras esperan— en sistemas con un gran número de
procesadores y con muchos procesos a la espera de ser ejecutados.

Cada vez más sistemas modernos —incluido Linux 2.6— están optando por
utilizar el esquema SMP con una cola de preparados por procesador. De
esta manera, al no utilizar mecanismos de sincronización, se eliminan
los tiempos de espera para acceder a la cola de preparados y escoger un
nuevo proceso. Sin embargo, con el fin de mantener la carga de trabajo
equilibrada entre todos los procesadores es necesario disponer de
algunos mecanismos de *balanceo de carga*. Por ejemplo:

* En la *migración comandada* o _push migration_ un tarea
específica —que se ejecuta con menor frecuencia que el planificador de
la CPU— estima la carga de trabajo de cada CPU y en caso de encontrar
algún desequilibrio mueve algunos procesos de la cola de preparados de
unos procesadores a la de los otros
* En la *migración solicitada* o _pull migration_ un procesador inactivo
extrae de la cola de preparados de un procesador ocupado alguna tarea
que esté esperando.

Tanto el planificador de Linux 2.6 como el planificador ULE, disponible
en los sistemas FreeBSD, implementan ambas técnicas. Mientras que en
Microsoft Windows, a partir de Windows Vista, sólo se hace uso de la
_migración solicitada_.

// TODO: Para integrar con lo anterior.

Para ilustrar los visto hasta el momento sobre la planificación de la
CPU en sistemas operativos modernos, vamos a estudiar las principales
características de las últimas versiones de Microsoft Windows a este
respecto.

Las actuales versiones de sistemas operativos Windows se agrupan dentro
de la familia Microsoft Windows NT; que nació con el sistema operativo
Windows NT 3.1 en 1993 y que llega hasta hoy en día con Microsoft
Windows 8.1 y Windows Server 2012 R2 —que se corresponden con la versión
6.3 de dicha familia Windows NT—

El núcleo de la familia _Windows NT_ es multihilo e internamente
implementa un algoritmo de planificación expropiativa con colas
multinivel realimentadas basado en prioridades:

Como cualquier sistema operativo moderno, el núcleo de Windows
es expropiable —lo que sabemos que ofrece latencias de
asignación más bajas que si no lo fuera— y soporta tiempo
real flexible:

Respecto a esto último, en Windows los programadores o administradores
del sistema pueden utilizar el API para establecer la prioridad de los
hilos. Sin embargo sobre estas preferencias el núcleo aplica ciertas
bonificaciones para obtener la prioridad real; combinando
diferentes criterios para reducir la latencia, mejorar la
respuesta —obviamente a través de beneficiar a los hilos limitados
por E/S— evitar la muerte por inanición y la inversión
de prioridad. Estas bonificaciones pueden ocurrir en los siguientes
casos:

Respecto al tiempo de cuanto, desde Windows Vista —NT 6.0— no se usa el
temporizador para controlarlo sino un contador de ciclos de reloj de la
CPUfootnote:[Desde el Intel Pentium las CPU de la familia x86 incorporan
un contador de marca de tiempo (Time Stamp Counter o TSC) de 64 bits que
indica el número de ciclos transcurridos desde el último _reset_ del
procesador.]. Así el sistema puede determinar con precisión el tiempo
que se hay estado ejecutando un hilo, sin incluir los tiempos dedicados
a otras cuestiones, como por ejemplo a manejar interrupciones.

En Windows los hilos se insertan en la cabeza de su cola —no
en el final— y conservan lo que les queda de cuanto, cuando son
expropiados. Mientras que se insertan por el final con el valor de
cuanto reiniciado, cuando abandonan la CPU por haber agotado el cuanto
anterior.

En Windows las prioridades de los procesos se pueden ver desde dos
perspectivas: la del API de Windows y la del núcleo. Esta última es la
que hemos estudiado en el apartado anterior. Mientras que el API
tiene una organización muy diferente que en última instancia debe
ser mapeada a las prioridades numéricas del núcleo de
Windows.

El API organiza los procesos por clases de prioridad: Tiempo real
(15), Alta (10), Arriba de lo normal (9), Normal (8), Debajo de lo
normal (7), Baja (6) y Reposo (1) . Al tiempo que cada hilo
tiene una prioridad relativa: De tiempo crítico (15), Más alta (2),
Arriba de lo normal (1), Normal (0), Debajo de lo normal (—1), Más baja
(—2) y Reposo (—15). Por lo que la prioridad interna de cada
hilo, desde el punto de vista del núcleo, es el resultado de sumar la
prioridad base obtenida a partir de la clase de prioridad del proceso
con la prioridad relativa del hilo en cuestión.

== Gestión de la memoria

=== Memoria principal

La memoria es un recurso central para el funcionamiento de un sistema
operativo moderno, puesto que es el único medio de almacenamiento al que
la CPU puede acceder directamente. Por ello, para que un programa pueda
ser ejecutado debe ser cargado en la memoria, desde el disco, y creadas
o modificadas las estructuras internas del sistema operativo necesarias
para convertirlo en un proceso. Además, dependiendo de la forma en la
que se gestiona la memoria, los procesos o partes de los mismos pueden
moverse de la memoria al disco y viceversa durante su ejecución, con el
objetivo de ajustar las necesidades de memoria manteniendo la
utilización de la CPU lo más alta posible.

Como ya comentamos en el aparatado <<_mainframe>>, en los
*sistemas multiprogramados* existe una *cola de entrada* que se define
_como aquella formada por el conjunto de procesos en disco que esperan
para ser cargados en la memoria para su ejecución_.

Por tanto, el procedimiento normal de ejecución de un programa en dichos
sistemas es:

. _Seleccionar un proceso de la cola de entrada y cargarlo en la
memoria_.

. _Mientras el proceso se ejecuta, éste accede a instrucciones y
datos de la memoria_.

. _Finalmente el proceso termina y su espacio en memoria es marcado
como disponible_.

En los *sistemas de tiempo compartido* no existe *cola de entrada*,
por lo que los programas se cargan inmediatamente en memoria cuando su
ejecución es solicitada por los usuarios. Excepto por eso, el
procedimiento normal de ejecución de un programa es el mismo que para
los _sistemas multiprogramados_.

=== Reubicación de las direcciones

La mayor parte de los sistemas permiten que un proceso de usuario resida
en cualquier parte de la memoria física. Así, aunque el espacio de
direcciones del sistema comience en `0x000000`, la primera dirección del
proceso de usuario no tiene porque ser esa. En la mayor parte de los
casos, un programa de usuario debe pasar por diferentes etapas —algunas
de las cuales son opcionales— antes de ser ejecutado (véase la ). En
cada una de ellas las direcciones pueden representarse de formas
distintas, por lo que en cada paso es necesario reubicar las direcciones
usadas en una etapa en direcciones de la siguiente. Por ejemplo, en el
código fuente de un programa las direcciones son generalmente
_simbólicas_, como los nombres de las variables y las funciones. A
continuación, un compilador suele reasignar esas direcciones simbólicas
en _direcciones reubicables_ del estilo de "120 bytes desde el comienzo
del módulo". Finalmente, el enlazador o el cargador convierte esas
direcciones reubicables en _direcciones absolutas_ como `0x210243`.

Por tanto, en cada etapa se mapean las direcciones de un espacio de
direcciones en el siguiente. Sin embargo, para que al final el programa
pueda ser ejecutado es necesario que tanto a los datos como a las
instrucciones se les reasignen direcciones absolutas de la memoria. Esto
realmente puede ocurrir en cualquiera de las siguientes etapas:

* En *tiempo de compilación*. Si durante la compilación o el enlazado
se conoce el lugar de la memoria donde va a ser ejecutado el proceso, se
puede generar directamente código con _direcciones absolutas_, o
*código absoluto*. Si en algún momento la dirección de inicio donde es
cargado el programa cambia, es necesario recompilar el código fuente del
programa. Los programas con formato COM del MS-DOS son un ejemplo de
este tipo de programas.

// TODO: Poner lo de los como COM nota.

* En *tiempo de carga*. Si no se conoce durante la compilación el
lugar donde va a residir un programa cuando sea ejecutado, el compilador
debe generar *código reubicable*. En este tipo de código se utilizan
_direcciones reubicables_, de manera que se retrasa la reubicación a
direcciones absolutas hasta el momento de la carga del programa. Esto
permite a muchos sistemas operativos que un proceso pueda residir en
cualquier parte de la memoria física, cargando los procesos donde más
convenga para maximizar el aprovechamiento de la misma.

* En *tiempo de ejecución*. Si un proceso puede ser movido durante su
ejecución de un lugar de la memoria a otro, la reubicación de
direcciones debe ser retrasada hasta el momento de la ejecución de cada
instrucción del programa. Para que esto sea posible necesitamos disponer
de hardware especial que suele estar presente en la mayor parte de las
CPU modernas, por lo que la inmensa mayoría de los sistemas operativos
modernos de propósito general utilizan este método.

En el <<_protección_de_la_memoria>> vimos en lo sistemas operativos
modernos, como medida de protección, los procesos no tienen acceso libre
a la memoria física. En lugar de eso el sistema operativo —asistido por
la MMU (Memory-Management Unit)— proporciona a cada proceso un _espacio
de direcciones virtual_ que ofrece una «vista» privada de la memoria
similar a la que tendrían si cada uno de los procesos estuviera siendo
ejecutando en solitario (véase la ). Es durante los acceso a la memoria
principal en tiempo de ejecución cuando estas _direcciones virtuales_
son convertidas en las _direcciones física_ con las que realmente se
accede a la memoria.

El mecanismo de protección descrito _es una forma muy común de
reubicación de las direcciones en tiempo de ejecución_ que está
presente en la mayor parte de los sistemas operativos modernos de
propósito general. A parte de la protección, algunas de las
características de dicho mecanismo son:

* _Los programas pueden ser cargados en cualquier zona libre de la
memoria física e incluso movidos de una región a otra durante la
ejecución de los procesos_, puesto que la transformación (reubicación)
de las direcciones virtuales en direcciones físicas se realiza durante
la ejecución de cada instrucción.

* _La reubicación de las direcciones virtuales —es decir, la asignación
de direcciones virtuales a las direcciones del programa— puede hacerse
en tiempo de compilación_ puesto que de antemano se sabe que todo el
espacio de direcciones virtual va a estar disponible. Lo común es que
los programas se ubiquen en la parte baja del espacio de direcciones
virtual, por ejemplo en empezando en la dirección `0x00000000`.

* _Se puede reducir el consumo de memoria principal compartiendo las
regiones de memoria física asignadas al código y los datos de sólo
lectura de los procesos de un mismo programa_. El código de un programa
suele contener direcciones tanto para los saltos como para el acceso a
los datos. Al ubicar los programas siempre en las mismas regiones de los
espacios de direcciones virtuales nos estamos asegurando de que el
código en memoria de los procesos de un mismo programa siempre es el
mismo, por lo que se puede compartir la memoria física que ocupan.

=== Enlazado dinámico y librerías compartidas

Fundamentalmente existen dos tipos de enlazado:

* _En el *enlazado estático*, las librerías del sistema y otros
módulos son combinados por el enlazador para formar la imagen binaria
del programa que es almacenada en disco_. Algunos sistemas operativos,
como MS-DOS, sólo soportan este tipo de enlazado.

* _En el *enlazado dinámico*, éste se pospone hasta la carga o la
ejecución_ (véase la ).

Generalmente el enlazado dinámico ocurre durante la carga del programa:

. _Durante la carga del módulo ejecutable se comprueban las
dependencias del mismo_. Estas se almacenan en el mismo archivo en
disco que dicho módulo.

. _Las librerías a enlazar se cargar y ubican en el espacio de
direcciones virtual creado para el nuevo proceso_.

. Finalmente, _las referencias del programa a las funciones de cada
una de las librerías cargadas se actualizan con la dirección en memoria
de las mismas_. Así la invocación de las funciones por parte del
programa se puede realizar de forma transparente, como si siempre
hubieran formado parte del mismo.

Cuando el enlazado se va a realizar en tiempo de ejecución se habla de
_enlazado dinámico con carga diferida_. En ese caso el procedimiento
es el siguiente.

. _Durante el enlazado estático del módulo ejecutable se pone un stub
a cada referencia a alguna función de la librería que va a ser
enlazada dinámicamente_.

. _Si durante la ejecución alguna de dichas funciones es invocada,
se ejecuta el stub_. El _stub_ es una pequeña pieza de código que sabe
como carga la librería, si no ha sido cargada previamente, y como
localizar la función adecuada en la misma.

. Finalmente, _el stub se sustituye a si mismo con la dirección de la
función y la invoca_. Esto permite que la siguiente ejecución de la
función no incurra en ningún coste adicional.

Sin esta habilidad cada programa en el sistema, por ejemplo, debe tener
una copia de la librería del sistema incluida en la imagen binaria del
mismo, lo que significa un desperdicio de espacio libre en disco y
memoria principal. Además este esquema facilita la actualización de las
librería, puesto que los programas pueden utilizar directamente las
versiones actualizadas sin necesidad de volver a ser enlazados.

Puesto que durante la compilación de una librería no se conoce la región
que va a ocupar dentro de los espacios de direcciones virtuales de los
distintos procesos que la van a utilizar:

* Para las librerías _el compilador debe generar código PIC
(Position-Independent Code) o independiente de la posición_. Este tipo
de código se puede ejecutar adecuadamente y sin modificaciones
independientemente del lugar de la memoria donde esté ubicado. Esto
permite reducir el consumo de memoria principal compartiendo las
regiones de memoria física asignadas al código de una misma librería en
los distintos procesos que la utilizan.

* En los sistemas operativos donde no se usa código PIC el compilador
debe generar código reubicable _para que la reubicación de las
direcciones virtuales de las librerías dinámicas se haga en tiempo de
carga_. Esto aumenta el tiempo de carga de las librerías y sólo permite
que compartan memoria física el código de las instancias de una misma
librería que ha sido cargado en la misma región del espacio de
direcciones virtual en los distintos procesos que la utilizan.

_Habitualmente las librerías incluyen información acerca de la versión
que puede ser utilizada para evitar que los programas se ejecuten con
versiones incompatibles de las mismas, o para permitir que haya más de
una versión de cada librería en memoria_. Así los viejos programas se
pueden ejecutar con las viejas versiones de las mismas, o con versiones
actualizadas pero compatibles, mientras los nuevos programas se ejecuten
con las versiones más recientes e incompatibles con los viejos
programas. A este sistema se lo conoce como *librerías compartidas*.

=== Paginación

El mapeo entre direcciones virtuales y físicas puede realizarse de
diversas maneras. La forma más extendida es _la **paginación**, que no
es sino un esquema de gestión de la memoria que permite que el espacio
de direcciones físico de un proceso no sea continuo_.

==== Método básico

En la paginación _la memoria física se divide en bloques de tamaño fijo
denominados **marcos**, mientras que el espacio de direcciones virtual
se divide en bloques del mismo tamaño que los marcos, denominados
**páginas**_. Cuando un proceso va a ser ejecutado sus páginas son
cargadas desde el almacenamiento secundario en marcos libres de la
memoria física.

La paginación es una forma de _reubicación de las direcciones en tiempo
de ejecución_ donde la transformación de las direcciones virtuales en
direcciones físicas se realiza de la siguiente manera (véase la ):

. _Cada dirección virtual generada por la CPU es divida en dos partes_: un *número de página* stem:[p] y un *desplazamiento* stem:[d].

. _El número de página es utilizado por la MMU para indexar la **tabla de páginas**_, que contiene el *número de marco* stem:[f] de cada página en la memoria física.

. _El número de marco stem:[f] es combinado con el *desplazamiento* stem:[d]_ para generar la dirección física que va a ser enviada por el bus de direcciones hacia la memoria.

// TODO: Incluir la formula.

El tamaño de las páginas —y el de los marcos— viene definido por el
hardware y normalmente es un número entero potencia de 2 que puede
variar entre 512 bytes y 16 MB, dependiendo de la arquitectura. Es
decir, si el espacio de direcciones es de stem:[2^m] y el tamaño de
página es de stem:[2^n], los stem:[m - n] bits de mayor orden del espacio
de direcciones indican el _número de página_, mientras que los stem:[n]
bits de menor orden indican el _desplazamiento_ (véase la )

===== Desde el punto de vista de los procesos

Cada página de un proceso requiere un marco. Por tanto, cuando un
proceso llega al sistema:

. _Si el proceso requiere n páginas, el sistema operativo debe
escoger n marcos_. Estos marcos son tomados de la _lista de marcos
libres_ que debe mantener el sistema. Puesto que son escogidos de allí
donde los haya libres, _el espacio de direcciones físico puede no ser
contiguo aunque los procesos vean un espacio de direcciones virtual
contiguo_.

. Los marcos seleccionados son asignados al proceso y _cada página del
proceso es cargada en uno_ de dichos marcos.

. _La tabla de páginas es actualizada_ de manera que en la entrada de
cada página del proceso se pone el número de marco correspondiente.

Un aspecto importante de la paginación es la diferencia entre como ven
los proceso la memoria y como es realmente la memoria física. Cada
proceso ve la memoria como un espacio único que lo contiene sólo a él.
Sin embargo la realidad es que el programa está disperso por la memoria
física, que además puede almacenar a otros programas. _Esto es posible
porque en cada momento la tabla de páginas sólo contiene las páginas del
proceso actual_.

===== Desde el punto de vista del sistema operativo

Puesto que el sistema operativo es quién gestiona la memoria física,
_éste debe mantenerse al tanto de las particularidades de su uso_:

* _Que marcos están asignados y a que página de que proceso o procesos_.

* _Que marcos están disponibles_.

Toda esta información generalmente se guarda en una estructura
denominada la _**tabla de marcos**, que tiene una entrada por cada
marco de la memoria física_.

Además _el sistema operativo debe mantener una copia de la tabla de
páginas para cada proceso en el PCB_, igual que mantiene una copia del
contador de programa y del contenido de los registros de la CPU. Esta
copia es utilizada:

* Por _el asignador para sustituir la tabla de páginas hardware cuando
realiza un cambio de contexto_. Por lo tanto el uso de la paginación
incrementa el tiempo del cambio de contexto.

* _Para el mapeo manual de direcciones virtuales en físicas_. Por
ejemplo, cuando un proceso realiza una llamada al sistema para realizar
una operación de E/S y proporciona una dirección como parámetro, dicha
dirección debe ser mapeada manualmente para producir la dirección física
correspondiente que será utilizada por el hardware para realizar la
operación.

===== Tamaño de las páginas

Una decisión de diseño importante es escoger el tamaño de las páginas
adecuado:

* _Con páginas más pequeñas esperamos tener menos fragmentación
interna_. Los marcos son asignados como unidades indivisibles, por lo
que si los requerimientos de memoria de un procesos no coinciden con un
límite de páginas el último marco asignado no sería utilizado
completamente (en ocasiones incluso se podría desperdiciar un marco
completo). A ese fenómeno se lo conoce como _fragmentación interna_

* _Con páginas más grande se pierde menos espacio en la tabla de
páginas_. No olvidemos que cuanto más pequeñas son las páginas más
páginas son necesarias y, por tanto, más entradas en la tabla de páginas
se necesitan. Además _la E/S es más eficiente cuanto más datos son
transferidos en cada operación_.

Los tamaños de páginas típicos son 4 y 8 KB. Por ejemplo, normalmente
cada entrada en la tabla de paginas es de 4 bytes —aunque esto también
puede variar—. Eso significa que cada entrada puede direccionar a uno de
los 2^32^ marcos de la memoria física. Si suponemos que el tamaño de
cada marco es de 4 KB, podemos determinar que el sistema es capaz de
direccionar 2^44^ bytes —o 16 TB— de memoria física, para lo que es
necesario disponer de una tabla de páginas de 4 MB.

==== Soporte hardware de la tabla de páginas

La implementación en hardware de la tabla de páginas puede realizarse de
diversas maneras:

* _Como un conjunto de registros dedicados de la CPU_. Es decir, la
tabla de páginas del proceso actual es alojada dentro de la propia CPU,
en unos registros destinados a tal fin.

* _Almacenada en la memoria_. Es decir, la tabla de páginas del
proceso actual es alojada en la memoria, normalmente en un formato
definido por la CPU.

Debido a la velocidad de los registros de la CPU _la implementación como
conjunto de registros es la más eficiente._ Sin embargo _sólo puede ser
utilizado para tablas de páginas razonablemente pequeñas_. El DEC
PDP-11 —para el que se diseño el primer UNIX— es un ejemplo de sistema
con esta implementación. En el mismo se utilizaba un espacio de
direcciones de 16 bits y un tamaño de páginas de 8 KB, por lo que sólo
necesitaba 8 registros dedicados para alojar toda tabla de páginas.

En los sistemas modernos se utilizan tablas de páginas muchos más
grandes —de un millón de entradas o más— que difícilmente pueden
alojarse en registros dentro de la CPU, ya que alojar tablas de páginas
de más de 256 entradas es muy costoso. Por eso los sistemas actuales
almacenan la tabla de páginas del proceso en ejecución en la memoria._
Eso permite disponer tablas de páginas de gran tamaño_ aunque a costa
de _necesitar dos acceso a la memoria física por cada acceso a una
palabra de la memoria virtualfootnote:[La solución a este problema pasa
porque la CPU disponga de una eficiente y pequeña —de entre 64 y 1024
entradas— memoria caché en la que almacenar las entradas de la tabla de
página previamente utilizadas en la traducción de las direcciones. A
dicha caché se la denomina TLB (Translation Look-aside Buffer).
Obviamente es necesario que el asignador borre la TLB durante los
cambios de contexto.]_.

Para que la MMU pueda conocer la ubicación de la tabla de páginas
durante la traducción de las direcciones, _la CPU debe disponer de un
registro —el *PTBR* (Page-Table Base Register)— donde se guarda la
dirección de la tabla de páginas actual_. Además esto permite que el
cambio de contexto sea más rápido —respecto al uso de registros para
almacenar la tabla de páginas— puesto que sólo es necesario carga un
único registro más —el PTBR— durante el mismo.

==== Protección

La protección de las páginas _se consigue mediante unos *bits de
protección* asociados a cada entrada de la tabla de páginas_ y
normalmente almacenados en la misma. Estos bits pueden ser:

* *Solo lectura*.

* *Lectura — Escritura*. En algunos sistemas hay un bit específico
para este permiso, mientras que en otros se utilizan bit separados como:
_lectura_, _escritura_ y _ejecución_.

* *Sólo ejecución*. Que no existen en todas las plataformas. Por
ejemplo, la familia Intel x86 careció de esta característica hasta que
AMD la incluyó en su arquitectura AMD64, lo que obligó a Intel a
incluirla en las versiones más modernas de su Pentium IV. El bit —que
para ser exacto indica _no ejecución_— fue introducido para evitar
cierto tipo de ataques de seguridad.

Durante la traducción de las direcciones la MMU comprueba que el tipo de
acceso sea válido. Si esto no es así, se genera una excepción de
violación de protección de memoria, dado que el acceso en un modo o
autorizado se considera una instrucciones privilegiada. Normalmente el
sistema operativo responde a dicha excepción terminando el proceso que
la generó.

_Además de los bits comentados se suele añadir a cada entrada un
**bit de válido**_:

* _Cuando una página es válida_, la pagina asociada está en el espacio
de direcciones virtual del proceso. Es decir, _es legal_.

* _Cuando la página no es inválida_, la página no está asociada al
espacio de direcciones virtual del proceso. Es decir, _es ilegal_.

_El sistema operativo puede utilizar este bit para permitir o denegar
el acceso a una página_, por ejemplo porque no le ha asignado un marco
ya que no está siendo utilizada por el proceso. Al igual que con los
bits de permisos, los intentos de acceso a una página ilegal generan una
excepción.

Por ejemplo, en la vemos el espacio de direcciones virtual y la tabla de
páginas de un proceso de 5096 bytes en un sistema con páginas de 1024
bytes. Puesto que el proceso no ocupa todo el espacio de direcciones,
sólo las direcciones de la 0 a la 5119 son válidas. En dicho ejemplo
podemos apreciar varios fenómenos:

* Debido a la fragmentación interna las direcciones de la 5097 a la 5119
son válidas, aunque el proceso solo ocupe hasta la 5096. Es decir, se
está asignando al proceso una porción de memoria que no necesita.

* Las páginas ocupadas por el proceso son válidas. Pero todas las
paginas en direcciones por encima de la 5119 están marcadas como
ilegales. _Así el sistema operativo no tiene que asignar marcos a
páginas no utilizadas por el proceso_.

En general los procesos sólo necesitan una porción muy pequeña de su
espacio de direcciones virtual. En esos casos es un desperdicio de
memoria crear y almacenar un tabla de página completa con una entrada
para cada página del espacio de direcciones. Para evitarlo _en algunas
CPU existe el *PTLR* (Page-Table Length Register) que se utiliza para
indicar el tamaño actual de la tabla de página_. Este valor es
comparado por la MMU durante la traducción con el número de página de
cada dirección virtual, de manera que las páginas con entradas más allá
de la última almacenada en la tabla son consideradas ilegales.

En realidad, tal y como vimos en el <<_el_proceso>>,
_lo más común es que los procesos tengan un espacio de direcciones
virtual disperso como el de la _. En la misma podemos observar como el
sistema operativo ubica los diferentes componentes del proceso de una
forma particular dentro del espacio de direcciones virtual. Este esquema
permite que tanto el _montón_ —a través del mecanismo de asignación
dinámica de memoria de `malloc()`— como la pila puedan extenderse, en
base a las necesidades de memoria que tenga el proceso, sobre la región
de memoria no ocupada. Esa región también puede ser parcialmente ocupada
por librerías de enlace dinámico o por otros objetos compartidos que
sean necesitados durante la ejecución del proceso. _En cualquier caso
las páginas de dicha región forman parte del espacio de direcciones
virtual pero no tienen marcos de memoria física asignados, en tanto en
cuanto el proceso no las vaya a utilizar. La falta de marco es indicada
por el sistema operativo utilizando el bit de válido para denegar el
acceso_.

==== Páginas compartidas

Una de las ventajas importantes de la paginación es la posibilidad de
compartir páginas entre procesos. Para conseguir esto basta con que
_las *páginas compartidas* de los distintos procesos tengan asignadas
un mismo marco_. Esto permite, por ejemplo, que los procesos de un
mismo programa puedan compartir las páginas de código o los datos de
sólo lectura con el fin de ahorrar memoria. También permite compartir
las páginas de código de una librería compartida enlazada a diferentes
procesos.

Compartir páginas no sólo permite ahorrar memoria pues en _los sistemas
operativos modernos la memoria compartida (véase el apartado
<<_memoria_compartida>>) se implementa mediante páginas compartidas_.

=== Paginación bajo demanda

La _paginación bajo demanda_ es la técnica con la que frecuentemente se
implementa la _memoria virtual_ en los sistemas con paginación. El
concepto de _memoria virtual_ no debe confundirse con el de _espacio de
direcciones virtual_, aunque están relacionados puesto que _el que
exista separación entre la memoria física y la manera en la que los
procesos perciben la memoria es un requisito para poder implementar la
memoria virtual_.

==== Memoria virtual

_La *memoria virtual* es una técnica que permite la ejecución de
procesos sin que éstos tengan que ser cargados completamente en la
memoria_.

Los programas suelen tener partes de código que rara vez son ejecutadas,
por ejemplo las funciones para manejar condiciones de error que, aunque
útiles, generalmente nunca son invocadas. También es frecuente que se
reserve más memoria para datos de lo que realmente es necesario. Por
ejemplo muchos programadores tiene la costumbres de hacer cosas tales
como declarar un _array_ de 1000 por 1000 elementos cuando realmente
sólo necesitan 100 por 100. Teniendo todo esto en cuenta y con el fin de
mejorar el aprovechamiento de la memoria, parece que sería interesante
no tener que cargar todas las porciones de los procesos pero de manera
que éstos aun así puedan seguir siendo ejecutados. Eso es exactamente lo
que proporciona la memoria virtual, en general, y la paginación bajo
demanda, en particular, para los sistemas que soportan paginación.

La habilidad de ejecutar un proceso cargado parcialmente en memoria
proporciona algunos beneficios importantes:

* _Un programa no estará nunca más limitado por la cantidad de memoria
disponible_. Es decir, los desarrolladores pueden escribir programas
considerando que disponen de un espacio de direcciones virtual
extremadamente grande y sin considerar la cantidad de memoria realmente
disponible. Es importante no olvidar que sin memoria virtual para que un
proceso pueda ser ejecutado debe estar completamente cargado en la
memoria.

* Puesto que cada programa ocupa menos memoria _más programas se pueden
ejecutar al mismo tiempo, con el correspondiente incremento en el uso de
la CPU y en el rendimiento del sistema_ y sin efectos negativos en el
tiempo de respuesta y en el de ejecución.

==== Método básico

_En la paginación bajo demanda las páginas individuales, en las que se
dividen los espacios de direcciones virtuales de los diferentes
procesos, pueden ser sacadas de la memoria de manera temporal y copiadas
a un almacenamiento de respaldo, para posteriormente volver a ser
traídas a la memoria cuando son necesitadas por su proceso_. A este
proceso de guardado y recuperación de las páginas sobre el
almacenamiento de respaldo se lo denomina *intercambio* o _swapping_ y
es llevado a cabo por un componente del sistema operativo denominado el
_paginador_.

Para que se puedan cargar las páginas cuando son necesitadas por su
proceso hace falta que el paginador sepa cuando lo son. Eso requiere que
el hardware proporcione algún tipo de soporte, por ejemplo incorporando
un *bit de válido* a la entrada de cada página en la tabla de páginas:

* _Cuando el bit de válido está a 1 la página es legal y está en la
memoria_. Es decir, la página existe en el espacio de direcciones
virtual del proceso y tiene asignado un marco de memoria física.

* _Cuando el bit de válido está a 0_ pueden ocurrir varias cosas:

    ** _La página es legal pero esta almacenada en disco_ y no en la memoria.

    ** _La página no es legal_.
    Es decir, no existe en el espacio de direcciones virtual del proceso.
    Esto puede ser debido a que la página esté en un hueco del espacio de direcciones —en una región que no está siendo utilizada— por lo que el sistema operativo no le ha asignado espacio de almacenamiento ni en disco ni en la memoria.

Si un proceso accede a una página _residente en memoria_ —marcada como
válida— no ocurre nada y la instrucción se ejecuta con normalidad. Pero
si accede a una página marcada como inválida:

. Al intentar acceder a la página la MMU comprueba el bit de válido y
_genera una excepción de fallo página al estar marcada como inválida_.
Dicha excepción es capturada por el sistema operativo.

. _El sistema operativo comprueba en una tabla interna si la página
es legal o no_. Es decir, si la página realmente no pertenece al
espacio de direcciones virtual del proceso o si pertenece pero está
almacenada en el disco. Esta tabla interna suele almacenarse en el PCB
del proceso como parte de la información de gestión de la memoria.

. _Si la página es ilegal, el proceso ha cometido un error y debe ser
terminado_. En UNIX, por ejemplo, el sistema envía al proceso una señal
de _violación de segmento_ que lo obliga a terminar.

. _Si la página es legal debe ser cargada desde el disco_:

.. _El núcleo debe buscar un marco de memoria libre_ que, por
ejemplo, se puede escoger de la lista de marcos libres del sistema.

.. _Se solicita una operación de disco para leer la página deseada en
el marco asignado_. Puesto que no resulta eficiente mantener la CPU
ocupada mientras la página es recuperada desde el disco, el sistema debe
solicitar la lectura de la página y poner al proceso en estado de
espera.

.. _Cuando la lectura del disco haya terminado se debe modificar la
tabla interna, antes mencionada, y la tabla de páginas para indicar que
la página está en la memoria_.

.. _Reiniciar la instrucción que fue interrumpida por la excepción_.
Generalmente esto se hace colocando el proceso nuevamente en la cola de
preparados y dejando que el asignador lo reinicie cuando sea escogido
por el planificador de la CPU.

Un caso extremo de la paginación bajo demanda es la *paginación bajo
demanda pura*. En ella _la ejecución de un proceso se inicia sin
cargar ninguna página en la memoria_. Cuando el sistema operativo sitúa
al contador de programas en la primera instrucción del proceso —que es
una página no residente en memoria— se genera inmediatamente un fallo de
página. La página es cargada en la memoria —tal y como hemos descrito
anteriormente— y el proceso continua ejecutándose, fallando cuando sea
necesario con cada página que necesite y no esté cargada. Las
principales ventajas de la _paginación bajo demanda pura_ son:

* Nunca se traerá desde el disco una página que no sea necesaria.

* El inicio de la ejecución de un proceso es mucho más rápido que si se
cargara todo el proceso desde el principio.

==== Requerimientos de la paginación bajo demanda

Los requerimientos hardware para que un sistema operativo pueda soportar
la paginación bajo demanda son:

* _Tabla de páginas con habilidad para marcar entradas inválidas_, ya
sea utilizando un bit específico o con valores especiales en los bits de
protección.

* _Disponibilidad de una memoria secundaria_. En esta memoria se
guardan las páginas que no están presentes en la memoria principal.
Normalmente se trata de un disco conocido como *dispositivo de
intercambio*, mientras que la sección de disco utilizada concretamente
para dicho propósito se conoce como *espacio de intercambio* o _swap_.

* _Posibilidad de reiniciar cualquier instrucción_ después de un fallo
de página. En la mayor parte de los casos esta funcionalidad es sencilla
de conseguir. Sin embargo, la mayor dificultad proviene de las
instrucciones que pueden modificar diferentes posiciones de la memoria,
como aquellas pensadas para mover bloques de bytes o palabras. En el
caso de que el bloque de origen o de destino atraviese un borde de
página, la instrucción sería interrumpida cuando la operación solo haya
sido realizada parcialmente. Si además ambos bloques se superpusieran,
no se podría reiniciar la instrucción completa. Las posibles soluciones
a este problema deben ser implementadas en el hardware.

==== Rendimiento de la paginación bajo demanda

Indudablemente el rendimiento de un sistema con paginación bajo demanda
se ve afectado por el número de fallos de páginas. En el peor de los
casos, en cada instrucción un proceso puede intentar acceder a una
página distinta empeorando notablemente el rendimiento. Sin embargo esto
no ocurre puesto que los programas tienden a tener localidad de
referencia (véase el <<_hiperpaginación>>).

===== Tiempo de acceso efectivo

El rendimiento de un sistema con paginación bajo demanda está
relacionado con el concepto de *tiempo de acceso efectivo* a la memoria.
_Éste intenta estimar el tiempo que realmente se tarda en acceder a la
memoria teniendo en cuenta mecanismos del sistema operativo como la
paginación bajo demanda_.

En muchos sistemas informáticos el *tiempo de acceso* —a la memoria
física— stem:[T_m] es de unos pocos nanosegundos. Por lo tanto, si no hay
fallos de página, el _tiempo de acceso efectivo_ es igual al tiempo de
acceso a la memoria. Pero si hay fallos de página, primero es necesario
leer la página del disco, por lo que el _tiempo de acceso efectivo_ a la
memoria es mayor.

Supongamos que conocemos la probabilidad stem:[p] de que ocurra un fallo de
página. _El tiempo de acceso efectivo se podría calcular como una media
ponderada por la probabilidad p del tiempo de acceso a la memoria stem:[T_m]
mas el tiempo necesario para gestionar cada fallo de página —o *tiempo
de fallo de página*— stem:[T_(fp)]:

[stem]
++++
T_(em)=(1-p)*T_m+p T_(fp)
++++

Por tanto, para calcular el _tiempo de acceso efectivo_ stem:[T_(em)]
necesitamos estimar el _tiempo de fallo de página_ stem:[T_(fp)], que se
consume fundamentalmente en:

* _Servir la excepción de fallo de página_. Esto incluye capturar la
interrupción, salvar los registros y el estado del proceso, determinar
que la interrupción es debida a una excepción de fallo de página,
comprobar si la página es legal y determinar la localización de la misma
en el disco. Aproximadamente, en realizar esta tarea el sistema puede
tardar de 1 a 100μs.

* _Leer la página en un marco libre_. En esta tarea se puede tardar
alrededor de 8ms, pero este tiempo puede ser mucho mayor si el
dispositivo está ocupado y se debe esperar a que se realicen otras
operaciones.

* _Reiniciar el proceso_. Si incluimos el tiempo de espera en la cola
de preparados, se puede tardar entre 1 y 100μs.

Como se puede apreciar _la mayor parte del tiempo de fallo de página es
debido al tiempo requerido para acceder al dispositivo de intercambio_.

Para ilustrar el cálculo del _tiempo de acceso efectivo_ a la memoria:
sólo vamos a considerar el tiempo requerido para acceder al dispositivo
de intercambio —ignorando las otras tareas a realizar durante el fallo
de página— vamos suponer que el _tiempo de acceso_ a la memoria stem:[T_m] es de 200 ns y que la probabilidad stem:[p] es muy pequeña (es decir, stem:[p ≪ 1]):

[stem]
++++
{:(T_(em),=,(1-p)*200ns+p * 8ms),
(      ,=,(1-p)*200ns+p * 8000000ns),
( ,approx, 200ns+7999800ns * p ):}
++++

Como se puede apreciar el _tiempo de acceso efectivo_ es proporcional a
la *tasa de fallos de página* .


[stem]
++++
T_(em) approx T_(m)+r_(fp)
++++

Por ejemplo, si un proceso causa un fallo de página en uno de cada 1000
accesos (stem:[p = 0,001]), el _tiempo de acceso efectivo_ es de 8,2 ms. Es
decir, el rendimiento del sistema es 40 veces inferior debido a la
paginación bajo demanda. Por tanto _es necesario mantener la tasa de
fallos de página lo más baja posible para mantener un rendimiento
adecuado_.

===== Manejo y uso del espacio de intercambio

_Otro aspecto fundamental que afecta al rendimiento de la paginación
bajo demanda es el uso del espacio de intercambio_. Cuando un proceso
genera un fallo de página el sistema operativo debe recuperar la página
de allí donde esté almacenada. Si esto ocurre al principio de la
ejecución, ese lugar seguramente será el archivo que contiene la imagen
binara del programa, pues es donde se encuentran las páginas en su
estado inicial. Sin embargo el acceso al espacio de intercambio es mucho
más eficiente que el acceso a un sistema de archivos, incluso aunque el
primero esté almacenado dentro de un archivo de gran tamaño. Esto es
debido a que los datos se organizan en bloques contiguos de gran tamaño,
se evitan las búsquedas de archivos y las indirecciones en la asignación
de espacio. Por ello debemos plantearnos que hacer con las imágenes de
los programas que van a ser ejecutados.

* Se puede mejorar el rendimiento _copiando en el espacio de
intercambio la imagen completa de los programas durante el inicio del
proceso, para después realizar la paginación bajo demanda sobre dicha
copia_.

* Otra alternativa es _cargar las páginas desde el archivo que contiene
la imagen cuando son usadas por primera vez pero siendo escritas en el
espacio de intercambio cuando dichas páginas tiene que ser
reemplazadas_. Esta aproximación garantiza que sólo las páginas
necesarias son leídas desde el sistema de archivos reduciendo el uso de
espacio de intercambio, mientras que las siguientes operaciones de
intercambio se hacen sobre dicho espacio.

* También se puede suponer que el código de los procesos no puede
cambiar. Esto permite _utilizar el archivo de la imagen binaria para
recargar las páginas de código, lo que también evita escribirlas cuando
son sustituidas. Sin embargo el espacio de intercambio se sigue
utilizando para las páginas que no están directamente asociadas a un
archivo, como la pila o el montón de los procesos_. Este método parece
conseguir un buen compromiso entre el tamaño del espacio de intercambio
y el rendimiento. Por eso se utiliza en la mayor parte de los sistemas
operativos modernos.

==== Copy-on-write

_El *copy-on-write* o copia durante la escritura permite la creación
rápida de nuevos procesos, minimizando la cantidad de páginas que deben
ser asignadas a estos_. Para entenderlo es importante recordar que la
llamada al sistema `fork()` crear un proceso hijo cuyo espacio de
direcciones es un duplicado del espacio de direcciones del padre.
Indudablemente esto significa que durante la llamada es necesario
asignar suficientes marcos de memoria física como para alojar las
páginas del nuevo proceso hijo. El _copy-on-write_ minimiza de la
siguiente manera el número de marcos que deben ser asignadas al nuevo
proceso:

. _Cuando la llamada al sistema `fork()` crea el nuevo proceso lo hace
de forma que éste comparta todas sus páginas con las del padre_ (véase
la ). Sin el _copy-on-write_ el `fork()` tendría que asignar marcos de
memoria física a el hijo, para a continuación copiar las páginas del
padre en ellos. Sin embargo con el _copy-on-write_ padre e hijo mapean
sus páginas en los mismos marcos, evitando tener que asignar memoria
libre.

. _Las páginas compartidas se marcan como copy-on-write_. Para ello
se puede marcar todas las páginas como de _solo lectura_ en la tabla de
páginas de ambos procesos y utilizar una tabla interna alojada en el PCB
para indicar cuales son realmente de _sólo lectura_ y cuales están en
_copy-on-write_. _Es importante destacar que realmente sólo las
páginas que pueden ser modificadas se marcan como copy-on-write._ Las
páginas que no puede ser modificadas —por ejemplo las que contienen el
código ejecutable del programa— simplemente pueden ser compartidas como
de sólo lectura por los procesos, como hemos comentado anteriormente.

. _Si algún proceso intenta escribir en una página copy-on-write, la
MMU genera una excepción para notificar el suceso al sistema
operativo_. Siguiendo lo indicado en el punto anterior, la excepción se
originaría porque la página está marcada como de _solo lectura,_ por lo
que el sistema operativo debería comprobar si se trata de un acceso a
una página _copy-on-write_ o a un intento real de escribir en una página
de _sólo lectura_. Para ello el sistema sólo tendría que mirar la
tabla interna almacenada en el PCB. Si se ha intentado escribir en una
página de _solo lectura_, el proceso ha cometido un error y
generalmente debe ser terminado.

.  _Si el sistema detecta una escritura a una página de copy-on-write
sólo tiene que copiarla en un marco libre y mapearlo en el espacio de
direcciones del proceso_ (véase la ). Para esto se sustituye la página
compartida por otra que contiene una copia pero que ya no está
compartida. Indudablemente la nueva página debe ser marcada como de
escritura para que en el futuro pueda ser modificada por el proceso.

.  _La página original marcada como copy-on-write puede ser marcada
como de escritura y no como copy-on-write, pero sólo si ya no va a
seguir siendo compartida_. Esto es así porque una página marcada como
_copy-on-write_ puede ser compartida por varios procesos.

.  _El sistema operativo puede reiniciar el proceso_. A partir de
ahora éste puede escribir en la página sin afectar al resto de los
procesos. Sin embargo puede seguir compartiendo otras páginas en
_copy-on-write_.

El _copy-on-write_ permite ahorrar memoria y tiempo en la creación de
los procesos puesto que sólo se copian las páginas que son modificadas
por éstos, por lo que se trata de una técnica común en múltiples
sistemas operativos, como por ejemplo Microsoft Windows, Linux y
Solaris.

El _copy-on-write_ _es especialmente interesante si a continuación se
va a utilizar la llamada al sistema exec() puesto que si es así copiar
el espacio de direcciones completo es una pérdida de tiempo_.

==== Archivos mapeados en memoria

_Los *archivos mapeados en memoria* permiten acceder a un archivo como
parte del espacio de direcciones virtuales de un proceso_. Algunas de
las características de esta técnica son:

* Cuando una región del espacio de direcciones queda marcada para ser
mapeada sobre una región de un archivo _se utiliza una estrategia
similar a la comentada para el método básico de la paginación bajo
demanda. La diferencia es que las páginas son cargadas desde dicho
archivo y no desde el espacio de intercambio_. Es decir, en un primer
acceso a una página mapeada se produce un fallo de página que es
resuelto por el sistema operativo leyendo una porción del archivo en el
marco asignado a la página.

* Esto significa que _la lectura y escritura del archivo se realiza a
través de lecturas y escrituras en la memoria_, lo que simplifica el
acceso y elimina el costo adicional de las llamadas al sistema:
`read()`, `write()`, etc.

* _Las escrituras en disco se suelen realizar de forma asíncrona_.
Para ello el sistema operativo comprueba periódicamente las páginas
modificadas y las escribe en disco.

* _Los marcos utilizados en el mapeo pueden ser compartidos, lo que
permite compartir los datos de los archivo_. Además se puede incluir
soporte de _copy-on-write_, lo que permite a los procesos compartir un
archivo en modo de sólo lectura pero disponiendo de sus propias copias
de aquellas páginas que modifiquen. Indudablemente para que los procesos
puedan compartir datos es necesario que exista algún tipo de
coordinación (véase el <<_sincronización>>).

Algunos sistemas operativos ofrecen el servicio de mapeo de archivos en
la memoria sólo a través de una llamada al sistema concreta, permitiendo
utilizar las llamadas estándar —`read()`, `write()`, etc.— para
hacer uso de la E/S tradicional. Sin embargo _muchos sistemas modernos
utilizan el mapeo en la memoria independientemente de que se pidan o
no_. Por ejemplo, en Linux si un proceso utiliza llamada al sistema
`mmap()` es porque explícitamente pide que el archivo sea mapeado en
memoria. Por tanto, el núcleo mapea el archivo en el espacio de
direcciones del proceso. Sin embargo, si un archivo es abierto con
llamadas al sistemas estándar —como `open()`— Linux mapea el archivo
en el espacio de direcciones del núcleo y traduce las llamadas `read()`
y `write()` en accesos a la memoria en dicha región. No importa como sea
abierto el archivo, Linux trata toda la E/S a archivos como mapeada en
memoria, permitiendo que el acceso a los mismos tenga lugar a través del
eficiente componente de gestión de la memoria.

==== Reemplazo de página

Hasta el momento hemos considerado que disponemos de memoria física
suficiente para atender cualquier fallo de página pero ¿qué pasa cuando
no quedan marcos libres?. En ese caso el código que da servicio a la
excepción de fallo de página debe escoger alguna página, intercambiarla
con el disco y utilizar el marco de la misma para cargar la nueva
página. Es decir, debemos modificar la función que ejecuta los pasos
descritos en el <<_protección_y_seguridad>> de la siguiente manera:

["arabic", start=4]
. _Si la página es legal, debe ser cargada desde el disco_.

    .. _Buscar la localización de la página en disco_.

    .. _El núcleo debe buscar un marco de memoria libre_ que, por ejemplo, se puede escoger de la lista de marcos libres del sistema.

        ... _Si hay uno, usarlo_.

        ... _Si no hay, usar un algoritmo de reemplazo de página para seleccionar una víctima_.

        ... _Escribir la víctima en el disco_ y cambiar las tablas de paginas y de marcos libres de acuerdo a la nueva situación. Para evitar mantener la CPU ocupada, el sistema debe solicitar la escritura de la página y poner al proceso en estado de espera.

    .. _Se solicita una operación de disco para leer la página deseada en el marco asignado_. Para evitar mantener la CPU ocupada, el sistema debe solicitar la escritura de la página y poner al proceso en estado de espera.

    .. _Cuando la lectura del disco haya terminado se debe modificar la tabla interna de páginas válidas y la tabla de páginas para indicar que la página está en la memoria._

    .. _Reiniciar el proceso interrumpido_.

Es importante destacar que _en caso de reemplazo se necesita realizar
dos accesos al disco. Esto se puede evitar utilizando un *bit de
modificado* asociado a cada página en la tabla de páginas_.

* _Este bit es puesto a 1 por el hardware cuando se modifica la
página_.

* _Se puede evitar escribir en disco aquellas páginas que tienen este
bit a 0 cuando son seleccionada para reemplazo_, siempre que el
contenido de la página no haya sido sobrescrito por otra en el espacio
de intercambio

En general para implementar la paginación bajo demanda necesitamos:

* _Un algoritmo de asignación de marcos_ que se encarga de asignar los
marcos a los procesos.

* _Un algoritmo de reemplazo de página_ para seleccionar que página
reemplazamos cuando no hay marcos suficientes.

Obviamente estos algoritmos deben ser escogidos de forma que mantengan
la tasa de fallos de página lo más baja posible.

===== Algoritmos de reemplazo de páginas

Existen diversos criterios para escoger la página que reemplazamos
cuando no hay suficientes marcos disponibles. En cualquier caso _el
algoritmo óptimo —el que garantiza la tasa de fallos de página más baja—
consiste en seleccionar siempre la página que más se va a tardar en
necesitar_. Desafortunadamente _este algoritmo es difícil de
implementar puesto que necesita tener información acerca de cuáles van a
ser las páginas referencias en el futuro_. Por eso sólo se puede
utilizar en estudios comparativos con el fin de saber cuanto se aproxima
al óptimo un algoritmo de reemplazo concreto.

Otros algoritmos de reemplazo pueden utilizar uno o varios de los
siguientes criterios:

* _Reemplazar la página que hace más tiempo que fue cargada_. Este
criterio da lugar al algoritmo FIFO de reemplazo que no siempre tiene un
buen rendimiento puesto que la página más antigua no necesariamente es
la que se va a tardar más tiempo en necesitar —que sería la elección
óptima—.

* _Reemplazar la página que hace más tiempo que fue
utilizada_ bajo la hipótesis de que si una página no ha sido usada
durante un gran periodo de tiempo, entonces es poco probable que vaya a
serlo en el futuro. Este criterio da lugar a la familia de algoritmos
*LRU* (Least Recently Used):

    ** _Estos algoritmos requieren de soporte por parte del hardware_
puesto que al sistema operativo no se le notifican los acceso legales a
las páginas, por lo que no tiene forma de saber cuando una página fue
usada por última vez.

    ** _Normalmente el soporte por parte del hardware es a través de un
bit en la tabla de páginas llamado **bit de referencia**_. Este
bit se pone a 1 cada vez que una instrucción ejecutada en la CPU
referencia a una página, lo que permite al sistema operativo hacerse una
idea aproximadafootnote:[Se trata de una aproximación puesto que usando
el _bit de referencia_ el sistema operativo no puede conocer con
exactitud la última vez que una página fue utilizada. Sin embargo,
aunque existen soluciones exactas que hacen uso de un contador o de una
pila que se actualiza en cada acceso a las páginas, se trata de
soluciones muy costosas como para ser implementarlas en hardware.] de
las páginas que han sido usadas recientemente. A los algoritmos que
siguen esta aproximación se los denomina _*NRU*_ (Not Recently Used).

    ** _Dentro de los algoritmos NRU también están aquellos que son
mejorados incluyendo el valor del bit de modificado en el criterio de
elección de la página_. Estos algoritmos escogen las páginas no
referencias antes que las referencias —para lo que utilizan el valor del
bit de referencia— y dentro de cada clase las no modificadas antes que
las modificadas —para lo que utilizan el valor del bit de modificado—
para evitar en lo posible reemplazar páginas cuyo contenido tiene que
ser escrito en disco.

* _Reemplazar la página que ha sido usada
con mayor o menos frecuencia_ utilizando contadores de referencias para
cada página —almacenados en la tabla de páginas— que sos actualizados
por el hardware en cada referencia. Este criterio da lugar a los
algoritmos *LFU* (Least Frequently Used) —cuando se escogen las páginas
utilizadas con menos frecuencia— y *MFU* (Most Frequently Used) —cuando
se escogen las páginas utilizadas con más frecuencia—.

===== Algoritmos de buffering de páginas

Existen otros procedimientos que pueden ser utilizados, junto con alguno
de los algoritmos de reemplazo comentados, con el objetivo de mejorar su
eficiencia. Estos procedimientos se agrupan dentro de lo que se denomina
algoritmos de *buffering de páginas*.

* _Se puede mantener una lista de marcos libres_. Cuando se produce un
fallo de paginas se escoge un marco de la lista y se carga la página, al
tiempo que se selecciona una página como víctima y se copia al disco.
Esto permite que el proceso se reinicie lo antes posible, sin esperar a
que la página reemplazada sea escrito en el disco. Posteriormente,
cuando la escritura finalice, el marco es incluido en la lista de marcos
libres.

* Una mejora de lo anterior sería _recordar que página estuvo en cada
marco antes de que éste pasara a la lista de marcos libres_. De esta
forma las páginas podrían ser recuperadas directamente desde la lista si
fallara alguna antes de que su marco fuera utilizado por otra página.
Esto permite reducir los efectos de que el algoritmo de reemplazo escoja
una víctima equivocada.

* _Se puede mantener una lista de
páginas modificadas e ir escribiéndolas cuando el dispositivo del
espacio de intercambio no esté ocupado_. Este esquema aumenta la
probabilidad de que una página esté limpia cuando sea seleccionada por
el algoritmo de reemplazo, evitando la escritura en disco.

===== Reemplazo local frente a global

Cuando un proceso necesita un marco el algoritmo de reemplazo puede
tanto extraerlo de cualquier proceso como ser obligado a considerar sólo
aquellas páginas que pertenecen al proceso que generó el fallo. Eso
permite clasificar los algoritmos de reemplazo en dos categorías:

* En _el *reemplazo local* sólo se pueden escoger marcos de entre los
asignados al proceso_.

    ** El número de marcos asignados a un proceso no cambia por que ocurran fallos de páginas.

    ** El mayor inconveniente es que _un proceso no puede hacer disponible a otros procesos los marcos de memoria que menos usa_.

* En _el *reemplazo global* se pueden escoger marcos de entre todos los
del sistema_, independientemente de que estén asignados a otro proceso
o no.

    ** El número de marcos asignados a un proceso puede aumentar si durante los fallos de página se seleccionan marcos de otros procesos.

    ** El mayor inconveniente _es que los procesos no pueden controlar su tasa de fallos de página_, puesto que esta depende del comportamiento de los otros procesos, afectando al tiempo de ejecución de forma significativa.

Generalmente _el reemplazo global proporciona mayor rendimiento por lo
que es el método más utilizado_.

==== Asignación de marcos de página

La cuestión que queda por resolver es cómo repartir los marcos de
memoria física libre entre los diferentes procesos con el fin de cubrir
las necesidades de reemplazo de cada uno de ellos. Posibles soluciones a
esto serían: repartir la memoria por igual entre todos los procesos o
hacerlo en proporción a la cantidad de memoria virtual que utilizan. Sin
embargo parece que puede ser interesante determinar el mínimo número de
marcos que realmente necesita cada proceso, pues así el sistema podría
disponer de memoria libre para aumentar el número de procesos
—aumentando el uso de la CPU— o para dedicarla a otras funciones —como
es el caso de los _búferes_ y las cachés de E/S —.

El mínimo número de marcos viene establecido por diversos factores:

* Cuando ocurre un fallo de página la instrucción que la ha provocado
debe ser reiniciada después de cargar la página en un marco libre. Por
lo tanto _un proceso debe disponer de suficientes marcos como para
guardar todas las páginas a las que una simple instrucción pueda
acceder_, pues de lo contrario el proceso nunca podría ser reiniciado
al fallar permanentemente en alguno de los acceso a memoria de la
instrucción. Obviamente este límite viene establecido por la
arquitectura de la máquina.

* Todo proceso tiene una cierta cantidad de páginas que en cada instante
son utilizadas frecuentemente. _Si el proceso no dispone de suficientes
marcos como para alojar dichas páginas, generará fallos de página con
demasiada frecuencia_. Esto afecta negativamente al rendimiento del
sistema, por lo que es conveniente que el sistema asigne al número de
marcos necesario para que eso no ocurra.

En general, si se va reduciendo el número de marcos asignados a un
proceso, mucho antes de haber alcanzado el mínimo establecido por la
arquitectura, el proceso dejará de ser útil debido a la elevada tasa de
fallos de página, que será mayor cuantos menos marcos tenga asignados.
Cuando eso ocurre se dice que el proceso está _hiperpaginando_.

==== Hiperpaginación

Se dice que _un proceso sufre de *hiperpaginación* cuando gasta más
tiempo paginando que ejecutándos_e.

===== Causas de la hiperpaginación

En los primeros sistemas multiprogramados que implementaron la
paginación bajo demanda era posible que se diera el siguiente caso:

. _El sistema operativo monitorizaba el uso de la CPU_. Si el uso de
la misma era bajo, se cargaban nuevos procesos desde la cola de entrada
para aumentar el grado de multiprogramación.

. _Si un proceso necesitaba demasiada memoria, le podía quitar los
marcos a otro_ puesto que se utilizaba un algoritmo de reemplazo global.
Esto podía ocasionar que aumentara la tasa de fallos de página del
proceso que perdía los marcos.

. _Al aumentar los fallos de pagina el uso de la CPU decrecía_, por
lo que el sistema operativo cargaba más procesos para aumentar el grado
de multiprogramación y con ello el uso de la CPU.

. _Esto reducía la cantidad de memoria disponible para cada
proceso_, lo que aumentaba la tasa de fallos de páginas que nuevamente
reducía el uso de la CPU

. Este mecanismo iteraba hasta reducir considerablemente el
rendimiento del sistema.

El fenómeno comentado se ilustra en la donde el uso de la CPU es trazado
frente al número de procesos cargados en el sistema. Cuando esto último
aumenta el uso de la CPU aumenta hasta alcanzar un máximo. Si el grado
de multiprogramación supera dicho punto, el sistema comienza a
hiperpaginar, por lo que el uso de la CPU disminuye bruscamente. Por lo
tanto, si el sistema está hiperpaginando, es necesario reducir el grado
de multiprogramación con el objetivo de liberar memoria.

En los sistemas de tiempo compartido modernos ocurre algo parecido a lo
descrito para los sistemas multiprogramados, aunque sin el efecto en
cadena ocasionado por el intento del planificador de largo plazo de
maximizar el uso de la CPU, ya que estos sistemas carecen de dicho
planificador. Sea como fuere, _en ambos casos los procesos
hiperpaginarán si no se les asigna un número suficiente de marcos_.

===== Soluciones a la hiperpaginación

Para el problema de la hiperpaginación existen diversas soluciones:

* _Utiliza un algoritmo de reemplazo local_ pues de esta manera un
proceso que hiperpagina no puede afectar a otro. Sin embargo, el uso
intensivo del dispositivo de intercambio podría afectar al rendimiento
del sistema al aumentar el tiempo de acceso efectivo.

* _Proporcionar a un proceso tantos marcos como le hagan falta_. Como
ya hemos comentados en diversas ocasiones, para evitar la
hiperpaginación es necesario asignar al procesos al menos un número
mínimos de marcos, que a priori no es conocido. Una de las estrategias
que pretenden estimar dicho número es el *modelo de conjunto de
trabajo*.

===== Modelo del conjunto de trabajo

Para entender el modelo de conjunto de trabajo es necesario comenzar
definiendo el *modelo de localidad*. El modelo de localidad establece
que:

* _Una localidad es un conjunto de páginas que se utilizan juntas_.

* _Cuando un proceso se ejecuta se va moviendo de una localidad a otra_.

Por ejemplo, cuando se invoca una función se define una nueva localidad.
En esta localidad las referencias a la memoria se realizan al código de
la función, a las variables locales de la misma y a algunas variables
globales del programa.

Supongamos que proporcionamos a un proceso suficientes marcos como para
alojar toda su localidad en un momento dado. Entonces el proceso
generará fallos de página hasta que todas las páginas de su localidad
estén cargadas, pero después de eso no volverá a fallar hasta que no
cambie a una nueva localidad. Sin embargo _si damos al proceso menos
marcos de los que necesita su localidad, éste hiperpaginará_.

_El *modelo de conjunto de trabajo* es una estrategia que permite
obtener una aproximación de la localidad del programa_ y consiste en lo
siguiente:

* _Definir el parámetro stem:[Delta] como el tamaño de la ventana del conjunto
de trabajo_.

* _En un instante dado el conjunto de páginas presente en las stem:[Delta]
referencias más recientes a la memoria se consideran el **conjunto de
trabajo**_.

* Por lo tanto, _el *conjunto de trabajo* es una aproximación de
localidad del programa_.

Por ejemplo, dada la siguiente lista de referencias a páginas en la
memoria memoria:

// TODO: Arreglar esto.

si stem:[Delta = 10] referencias a la memoria, entonces el conjunto de trabajo en
stem:[t_1] es stem:[{1, 2, 5, 6, 7}]. Mientras que en stem:[t_2] el conjunto de
trabajo es stem:[{3, 4}].

Obviamente _la precisión del conjunto de trabajo como aproximación de
la localidad del programa depende del parámetro stem:[Delta]_. Por ejemplo:

* Si stem:[Delta] es muy pequeña, el conjunto de trabajo no cubría toda la
localidad.

* Si stem:[Delta] es muy grande, el conjunto de trabajo se superpondrían a varias
localidades.

===== Uso del conjunto del trabajo para evitar la hiperpaginación

El uso del conjunto de trabajo es bastante sencillo:

. _Se selecciona stem:[Delta]_.

. _El sistema operativo monitoriza el conjunto de trabajo de cada proceso y le asigna tantos marcos como páginas haya en el conjunto de trabajo_.

. _Si sobran suficientes marcos otro proceso puede ser iniciado —en el caso de los sistemas multiprogramados— o se puede destinar la memoria libre a otros usos_.

. _Si el tamaño del conjunto de trabajo D crece y excede el número de marcos disponibles, el sistema podría seleccionar un proceso para ser suspendido_. Éste podrá volver a ser reiniciado más tarde.

Donde el tamaño del conjunto de trabajo stem:[D] es la suma del tamaño de los
conjuntos de trabajo stem:[WSS^i] para cada proceso stem:[i]:

[stem]
++++
D=sum WSS_i
++++

y representa la demanda total de marcos. Por eso _si stem:[D] es mayor que el
número de marcos disponibles, habrá hiperpaginación_.

Este sencillo algoritmo anterior permite evitar la hiperpaginación. Sin
embargo, el problema está en como mover la ventana del conjunto de
trabajo en cada referencia, con el fin de volver a calcular el conjunto
de trabajo. Una posible aproximación sería utilizar un temporizador que
periódicamente invocase a una función encargada de examinar el bit de
referencia de las páginas en la ventana _stem:[Delta]_. Es de suponer que las
páginas con el bit de referencia a 1 forman parte de la localidad del
programa y por tanto serán el conjunto de trabajo a lo largo del
siguiente periodo.

==== Otras consideraciones

Ya hemos comentado que las principales decisiones que deben ser tomadas
en el diseño de un sistema con paginación bajo demanda son la elección
del algoritmo de reemplazo y la del de asignación de marcos de página.
Sin embargo hay otras consideraciones que deben ser tenidas en cuenta.

===== Prepaginado

_El *prepaginado* es una técnica que consiste en cargar múltiples
páginas junto con la página demandada en cada fallo de página_. Esas
otras páginas se escogen especulativamente bajo la hipótesis de que van
a ser necesitadas por el proceso en un corto espacio de tiempo, de
manera que si la predicción es acertada la tasa de fallos de página se
reduce significativamente. Esta técnica puede ser utiliza, por ejemplo,
en las siguiente situaciones:

* En la paginación bajo demanda pura, donde el sistema sabe de antemano
que cuando se inicia un proceso siempre fallan las primeras páginas de
código, por lo que son buenas candidatas para el prepaginado.

* En el acceso secuencial a archivos mapeados en memoria. El sistema
puede determinar que el acceso es de ese tipo tanto mediante el uso de
técnicas heurísticas como mediante las indicaciones dadas por el proceso
en la llamada al sistema con la que se abrió el archivo. En cualquier
caso, si el sistema determina que el acceso al archivo es secuencial, en
cada fallo de página puede cargar tanto la página demanda como las
siguientes en previsión de que vayan a ser utilizas por el proceso.

En general el único inconveniente del prepaginado es que debe ser
ajustarlo para que el coste del mismo sea inferior al de servir los
fallos de página.

===== Aplicaciones en modo RAW

_Algunas aplicaciones al acceder a sus datos a través de los mecanismos
de memoria virtual del sistema operativo ofrecen peor rendimiento del
que conseguirían si este mecanismo no existiera_. El ejemplo típico son
las bases de datos, que conocen sus necesidades de memoria y disco mejor
que cualquier sistema operativo de propósito general, por lo que salen
beneficiadas si implementan sus propios algoritmos de gestión de la
memoria y de buffering de E/S.

_Muchos sistemas operativos modernos permiten que los programas que lo
soliciten puedan acceder a los discos en modo raw_. En el _modo raw_ no
hay sistemas de archivos, ni paginación bajo demanda, ni bloqueo de
archivos, ni prepaginación, ni nada; por lo que dichas aplicaciones
deben implementar sus propios algoritmos de almacenamiento y gestión de
la memoria. Sin embargo, hay que tener en cuenta que la mayor parte de
las aplicaciones siempre funcionan mejor utilizando los servicios
convencionales ofrecidos por el sistema operativo.

===== Tamaño de las páginas

Como ya comentamos al estudiar el método básico de paginación (véase el
apartado <<_paginación>>), una decisión de diseño importante es
escoger el tamaño adecuado para las páginas:

* *Con páginas grandes*:

    ** _Se consiguen menos fallos de páginas_.
    Por ejemplo, en un caso extremo un proceso de 100 KB solo podría generar un fallo de página si cada página es de 100 KB, pero puede generar 102400 fallos si cada pagina es de 1 byte.

    ** _Se consiguen tablas de páginas más pequeñas_.

    ** _La E/S para acceder al contenido de cada página requiere menos tiempo_.
    En general el tiempo de transferencia es proporcional a la cantidad de información transferida, lo que debería beneficiar a los sistemas con páginas de pequeño tamaño.
    Sin embargo la latencia y el tiempo requerido para posicionar la cabeza lectora de los discos es muy superior al tiempo de transferencias de datos, por lo que es más eficiente tener menos transferencias de mayor tamaño —como cuando se usan páginas de grandes— que más transferencias de menor tamaño —como cuando se usan páginas pequeñas—.

* *Con páginas pequeñas*:

    ** _Se consigue tener menos fragmentación interna_ y por tanto un mejor aprovechamiento de la memoria.

    ** _Teóricamente se obtiene mejor resolución para asignar y transferir al disco sólo la memoria que realmente necesitamos_.
    Esto a la larga debería redundar en menos memoria asignada y menos operaciones de E/S.

En la actualidad el tamaño de página más común es de 4KB en sistemas de
32 bits y 8 KB en los de 64 bits, ya que son adecuados para la mayor
parte de las aplicaciones. Sin embargo, _muchos sistemas modernos
soportan el uso simultáneo de múltiples tamaños de página_. Esto
permite que la mayor parte de las aplicaciones utilicen el tamaño
estándar, mientras las que hacen un uso intensivo de la memoria puedan
utilizar páginas de mayor tamaño. Por ejemplo, en la familia Intel x86
el tamaño estándar es de 4 KB, pero muchas bases de datos —como por
ejemplo Oracle— y núcleos de sistema operativo —como por ejemplo Linux o
Solaris— utilizan páginas de 4 MBfootnote:[ Es común que los núcleos de
los sistemas operativos utilicen páginas de gran tamaño para alojar su
código y sus datos. De esta forma se minimiza el número de entradas de
la TLB que utilizan, con el fin de disponer de más entradas libres para
los procesos en ejecución.] cuando corren sobre dicha arquitectura.

===== Efecto de la estructura de los programas

_Los programas estructurados con un buena localidad de referencia
pueden mejorar su rendimiento en los sistemas con paginación bajo
demanda_.

Vamos a ilustrarlo con el siguiente ejemplo de un programa que
inicializa a 0 un _array_ de 128 por 128 elementos.

[source, cpp]
----
int data[][] = new int[128][128];

for (int j = 0; j < 128; j++)
  for (int i = 0; i < 128; i++)
    data[i][j] = 0;
----

Un _array_ como el indicado es almacenado en filas:

[source, cpp]
----
data[0][0], data[0][1], ..., data[0][127]
data[1][0], data[1][1], ..., data[127][127]
----

De manera que si suponemos que el tamaño de cada página es de 128
palabras, en el mejor de los casos cada fila estará almacenada en una
página. Por lo tanto:

* Si el sistema le asigna 128 marcos o más, el proceso solo generará 128
fallos de página.

* Si el sistema operativo le asigna un solo marco, el proceso tendrá
16,384 fallos aproximadamente.

Sin embargo, el ejemplo sería diferente si el bucle interno del programa
recorriera las columnas del _array_ y no las filas:

Pues se podrían a 0 primero todas las palabras de una misma página antes
de empezar con la siguiente, reduciendo el número de fallos de página a
128 aunque el sistema operativo sólo asigne un marco al proceso.

Por lo tanto se puede concluir que:

* _La selección cuidadosa de las estructuras de datos y de programación
pueden mejorar la localidad, reduciendo la tasa de fallos de páginas y
el tamaño del conjunto de trabajo_. Por ejemplo, las pilas tienen buena
localidad puesto que el acceso siempre se realiza en lo alto de las
mismas. Sin embargo las tablas de dispersión, obviamente, están
diseñadas para dispersar las referencias, lo que produce una mala
localidad.

* _La elección del lenguaje de programación también puede tener
efecto_. En los lenguajes como C y {cpp} se utilizan punteros con
frecuencia, lo que aleatoriza el acceso a la memoria empeorando la
localidad de referencia. Además algunos estudios indican que los
lenguajes orientados a objetos tienden a tener peor localidad de
referencia que los que no lo son.

* _El compilador y el cargador también pueden tener un efecto
importante_:

    ** _Separando el código de los datos para permitir que las paginas de código pueda ser de sólo lectura_.
    Esto es interesante porque las paginas no modificadas no tienen que ser escritas antes de ser reemplazadas.

    ** _El compilador puede colocar las _funciones_ que se llaman entre sí en la misma página._

    ** _El cargador puede situar las _funciones_ _en la memoria _de _tal_ forma que _en lo posible no _crucen los bordes de las páginas_.

===== Interbloqueo de E/S

_Supongamos que un proceso solicita una operación de E/S sobre el
contenido de alguna de las páginas de su espacio de direcciones y que la
página es reemplazada después de que el proceso queda en espera pero
antes de que la operación es realizada_. En ese caso la operación de
E/S se podría acabar realizando sobre una página que pertenece a un
proceso diferente. Para evitarlo existen diversas soluciones:

* _Se puede utilizar la memoria del núcleo como búfer en las
operaciones de E/S_. En una escritura esto obliga a la llamada al
sistema a copiar los datos desde las páginas del proceso a la memoria
del núcleo antes de solicitar la operación de E/S. Mientras que en las
operaciones de lectura sería justo al contrario.

* _Cada página puede tener un bit de bloqueo_ que se utiliza para
indicar que páginas no pueden ser seleccionadas para reemplazo.

_Además los bits de bloqueo se pueden utilizar en otras muchas
situaciones_:

* _Bloquear las páginas del núcleo para evitar que sean reemplazadas_.

* _Bloquear las páginas que acaban de ser cargadas_. Esto evita que un
proceso de mayor prioridad pueda reclamar el marco antes de que el
proceso para el que se cargó la página sea reiniciado, desperdiciando el
trabajo de cargarla y provocando un nuevo fallo de página. Para
implementarlo se puede poner el bit de bloqueo a 1 cuando la página se
carga, volviéndolo a poner a 0 cuando el proceso es planificado por
primera vez después del fallo de página que provocó la carga de la
misma.

* _En los sistemas con tiempo real flexible se suele permitir que las
tareas de tiempo real informen de cuales son las páginas más importantes
con el fin de que sean bloqueadas para evitar que puedan ser
reemplazadas_. Para evitar riesgos, el sistema suele considerar estás
solicitudes como _consejos de bloqueo_, de manera que es libre de
descartar dichos consejos si el conjunto de marcos libres llega a ser
demasiado pequeño o si un proceso concreto pide bloquear demasiadas
páginas.

=== Interfaz de gestión de la memoria

Gracias a la abstracción de las técnicas de memoria virtual —como la
paginación bajo demanda— desde el punto de vista de los procesos en
cualquier sistema moderno _prácticamente sólo hace falta una llamada al
sistema para gestionar su espacio de direcciones virtual_. En los
sistemas POSIX —como GNU/Linux— esta llamada es `mmap()` —junto a su
opuesta `munmap()`— y sirve para:

* _Reservar una porción de espacio de direcciones virtual del
proceso_. Obviamente la llamada sólo hace la reserva para que dicha
región pueda ser usada por el proceso, siendo el componente de
paginación bajo demanda el responsable de asignar la memoria física que
la respalda.

* _Establecer permisos —lectura, escritura y ejecución—, opciones de
compartición entre procesos, bloqueo de páginas en la memoria física,
páginas de gran tamaño, etc._ en la región de memoria virtual a
reservar.

* _Mapear archivos en regiones del espacio de direcciones virtual_.

Sin embargo _en funciones como `mmap()` la página es la unidad mínima en
la gestión de la memoria_. Es decir, las regiones reservadas del
espacio de direcciones virtual siempre comienzan en un borde de página y
su tamaño es múltiplo del tamaño de página. La cuestión es como
compatibilizar eso con las necesidades reales de los programas, que
durante su ejecución necesitan reservar y liberar constantemente memoria
para pequeños elementos como: _arrays_, cadenas de texto, estructuras,
objetos, etc. Para esos casos _utilizar directamente `mmap()` no es una
solución puesto que la fragmentación interna con llevaría un importante
derroche de recursos_.

==== Uso del espacio de direcciones virtual del proceso

Los procesos pueden utilizar diversas ubicaciones dentro de su espacio
de direcciones virtual para almacenar los datos que necesitan para su
ejecución (véase la ):

* _La variables y constantes globales se almacenan en la sección de
datos_, que tiene tamaño fijo ya que las dimensiones de estas variables
se conocen de antemano en tiempo de compilación, al igual que ocurre con
el código del programa.

* _Las variables locales y los argumentos de las _funciones_ se
almacenan en la pila_ junto con la direcciones de retorno de las mismas.
Esta es la ubicación ideal puesto que al retornar de una función, la
pila se restablece al estado previo al que tenía cuando se invocó dicha
función, haciendo que las variables locales y argumentos desaparezcan
automáticamente.

* _Las variables asignadas dinámicamente —por ejemplo, usando
`malloc()`/`free()` en C o `new`/`delete` en {cpp} o Java— se almacenan en el
montón_, que no es más una región continua de memoria ubicada
inmediatamente después de la sección de datos del proceso.

Cada lenguaje de programación debe proporcionar —por ejemplo a través de
su _librería estándar_— un mecanismo en espacio de usuario adecuado
para la gestión en tiempo de ejecución de la memoria del _montón_ del
proceso. Para eso cada lenguaje puede utilizar su propia implementación
de dicho mecanismo o bien recurrir a la proporcionada por la _librería
del sistema_. Por ejemplo, en los sistemas POSIX la _librería del
sistema_ proporciona su propia implementación, accesible a través de las
funciones `malloc()` y `free()`, que es utilizada directamente por los
programas escritos en C. Otros lenguajes de programación tienen otras
interfaces para gestionar la memoria pero utilizan internamente las
funciones `malloc()` y `free()` de la _librería estándar_. Pero este
no es el caso ni de {cpp} ni de Java ni de otros muchos lenguajes; donde
los operadores `new` y `delete` utilizan sus propios algoritmos de
gestión de la memoria del montón más optimizados que `malloc()` y
`free()` para la creación y destrucción de objetos de manera eficiente.

==== Gestión de la memoria del montón

Para ilustrar cómo se gestiona la memoria del _montón_ utilizaremos como
ejemplo el mecanismo empleado por la _librería del sistema_ de los
sistemas POSIX —accesible a través de las funciones `malloc()` y
`free()`— aunque es importante tener en cuenta que esta tarea se
realiza de manera muy similar en las implementaciones de otros sistemas
operativos y lenguajes de programación.

El funcionamiento básico de `malloc()` sigue las siguiente reglas:

. _Cuando la memoria solicitada supera cierto umbral —128KB en
sistemas GNU/Linux— es reservada directamente mediante la llamada al
sistema `mmap()`_. Eso significa que las peticiones de gran tamaño
realmente no consumen espacio del _montón_.

. _En caso contrario la petición de memoria se atiende utilizando un
algoritmo de reserva de memoria continua sobre el espacio libre en el
montón._ Estos algoritmos los veremos posteriormente.

. _Si no hay suficiente memoria libre contigua como para atender la
petición se utiliza la llamada al sistema `brk()` para ampliar el tamaño
del montón_ sobre la región no asignada del espacio de direcciones
virtual del proceso.

Cuando un proceso hace una petición de memoria dinámica espera que el
espacio ofrecido sea continuo en el espacio de direcciones virtual, por
lo que es necesario utilizar algún algoritmo de *asignación de memoria
contigua*. Como las peticiones de los procesos son de tamaño variable,
la forma más eficiente de enfrentar este problema es utilizando lo que
se denomina un esquema de *particionado dinámico*:

. _La librería mantiene una lista indicando que regiones del montón
están libres y cuales no_. El montón se inicializa con un tamaño
determinado completamente libre, por lo que es considerado como un gran
hueco de memoria disponible.

. _Cuando un proceso realiza una petición —a través de `malloc()`— se
busca un hueco lo suficientemente grande para atenderla_. Si se
encuentra, sólo se le asigna el espacio necesario, que es marcado como
ocupado en la lista. El resto sigue siendo considerado como un hueco
libre, aunque de menor tamaño.

. _Si el proceso libera una porción de la memoria y se crean dos
huecos adyacentes, se funden en uno solo_.

En general, en un momento dado tenemos una petición de tamaño _n_ que
debemos satisfacer con una lista de huecos libres de tamaño variable.
Esto no es más que un caso particular del _problema _clásico_ de la
asignación dinámica de almacenamiento_ para el que hay diversas
soluciones:

* _En el *primer ajuste* se escoge el primer hueco lo suficientemente
grande como para satisfacer la petición_. La búsqueda puede ser desde
el principio de la lista o desde donde ha terminado la búsqueda
anterior.

* _En el *mejor ajuste* se escoge el hueco más pequeño que sea lo
suficientemente grande para satisfacer la petición_. Indudablemente
esto obliga a recorrer la lista de huecos completa o a tenerla ordenada
por tamaño.

* _En el *peor ajuste* se escoge el hueco más grande_. Igualmente
obliga a buscar en toda la lista de huecos o a tenerla ordenada por
tamaño.

En la actualidad _la estrategia más común es utilizar el mejor ajuste
junto con algún tipo de estructura de datos que mantenga los huecos
libres ordenados por tamaño_, de manera que puedan ser encontrados
eficientemente.

==== Fragmentación

Las estrategia comentada no sufre de _fragmentación interna_ porque se
asigna exactamente la cantidad de memoria solicitada. Sin embargo si
sufre de otro tipo de fragmentación denominada fragmentación externa.

_La *fragmentación externa* ocurre cuando hay suficiente espacio libre
para satisfacer una petición pero el espacio no es contiguo_. Es decir,
el espacio de almacenamiento está fraccionado en un gran número de
huecos de pequeño tamaño, obligando a la librería a invocar la llamada
al sistema `brk()` con el objetivo de incrementar el tamaño del montón.
Este problema:

* _Afecta tanto a la estrategia del primer como del mejor ajuste_.
Siendo el primero mejor en algunos sistemas y el segundo mejor en otros.

* Algunos análisis estadísticos realizados con el primer ajuste revelan
que _incluso con algunas optimizaciones, con n bloques asignados se
pierden 0.5n por fragmentación externa_. Es decir, un tercio de la
memoria no es utilizable. A esto se lo conoce como la _regla del 50%_.

Lamentablemente este problema no tiene una solución sencilla ya que
aunque se podría intentar mover los bloques de memoria para que toda la
memoria libre quedara en un único hueco, sería necesario modificar en
tiempo de ejecución las direcciones virtuales utilizadas por el proceso
en cada puntero o referencia a los bloques de memoria en el montón.

== Gestión del almacenamiento

=== Dispositivos de almacenamiento

Los ordenadores pueden almacenar información en diferentes soportes de
almacenamiento —por ejemplo en discos magnéticos, en DVD, en memorias de
estado sólido, etc.—. Cada uno tiene propiedades físicas diferentes que
pasamos a comentar brevemente a continuación.

==== Discos magnéticos

Los discos magnéticos son el tipo principal de almacenamiento
secundario, generalmente en la forma de lo que se denominan discos
duros. Tal y como se puede apreciar en la cada unidad está compuesta por
una serie de platos de forma circular recubiertos de material magnético.
La información se almacena grabándola magnéticamente sobre los platos,
para lo cual se utilizan unas cabezas de lectura que «flotan» tanto por
encima como por debajo de cada plato.

Desde el punto de vista lógico (véase la ) _la superficie de cada plato
está dividida en *pistas* circulares, cada una de las cuales se
subdivide en **sectores**_. _El conjunto de pistas formado por todas
aquellas que están situadas en la misma posición en los distintos platos
se denomina **cilindro**_.

En estos dispositivos consume mucho más tiempo mover la cabeza de
lectura hasta el sector de interés que la lectura y transferencia de los
datos almacenados a la memoria RAM. Por lo tanto el tiempo de acceso
aleatorio al disco es mucho mayor que el de acceso secuencial.

==== Discos ópticos

Los discos ópticos —CD, DVD, BluRay, etc.— consisten en un disco
circular en el cual la información se almacena haciendo uso de surcos
microscópicos que se leen haciendo incidir un láser sobre una de las
caras planas que lo componen.

En este tipo de discos la información se almacena siguiendo un recorrido
continuo en espiral que cubre la superficie entera del disco,
extendiéndose desde el interior hacia el exterior. Dado que el láser
siempre debe desplazarse sobre la espiral, el acceso aleatorio a los
datos es más lento que con otras tecnologías de disco.

==== Memorias de estado sólido

Una memoria de estado sólido —memoria USB, SSD, etc.— es un dispositivo
de almacenamiento que usa una memoria no volátil, como las _memorias
flash_, para almacenar datos, en lugar de utilizar discos ópticos o
magnéticos. En este tipo de memorias la información se almacena como en
un vector lineal de bytes, que se puede indexar aleatoriamente con la
misma eficiencia con la que se accede secuencialmente, como ocurre con
la memoria RAM, Aunque algunos dispositivos, de cara al resto del
sistema informático, emulan una interfaz y un modo de direccionamiento
similar al utilizado por los discos magnéticos —es decir, usando pistas,
sectores y cilindros.— por temas de compatibilidad.

=== Archivos y sistemas de archivos

Teniendo en cuenta la gran diversidad de dispositivos de almacenamiento
que existen, para que el sistema informático sea cómodo de utilizar el
sistema operativo proporciona una visión lógica uniforme de todos los
sistemas de almacenamiento. Es decir, abstrae las propiedades físicas de
los dispositivos de almacenamiento para definir una unidad de
almacenamiento lógico que sea útil para los usuarios. Esta unidad es el
_archivo_.

_Un *archivo* es una colección de información relacionada cuya
estructura y significado de sus datos los define su creador_.
Desde la perspectiva de los usuarios, un archivo es la unidad más
pequeña de almacenamiento. Es decir, no se pueden escribir datos en el
almacenamiento secundario a menos que estos se encuentren dentro de un
archivo.

El sistema operativo puede ofrecer esta abstracción gracia al *sistema
de archivos*. _Este proporciona los mecanismos para el almacenamiento
de lo datos y programas en archivos_, tanto del propio sistema
operativo como los de todos los usuarios del sistema informático.

_Los sistemas de archivos están compuestos de dos partes claramente
diferenciadas:_

* _Una colección de archivos_, cada uno de los cuales almacena una
serie de datos relacionados.

* _Una colección de estructuras de metadatos_, que contienen
información relativa a los archivos almacenados —nombre, ubicación en el
disco, permisos, etc.— y que se encarga de organizarlos, generalmente
haciendo uso de una estructura de directorios.

=== Volúmenes de datos

Los dispositivos de almacenamiento comentados anteriormente pueden ser
utilizados al 100% con un único sistema de archivos. Sin embargo, en
ocasiones es interesante hacer divisiones con el objeto de disponer de
múltiples sistemas de archivos en el mismo dispositivo. Cada una de esas
divisiones es un _volumen_.

En otros casos interesa combinar divisiones o dispositivos de
almacenamiento completos para crear espacios de mayor tamaño —también
denominadas volúmenes— cada una de las cuales puede albergar un único
sistema de archivos. Así que en general _utilizaremos el término
*volumen* para referirnos a un espacio de almacenamiento que alberga un
sistema de archivos_, tanto si ese espacio es una pequeña parte del
espacio completo del dispositivo como si se trata de una estructura de
mayor tamaño compuesta a partir de varios dispositivos.

A continuación comentaremos brevemente las tecnologías utilizadas con
mayor frecuencia para construir estos volúmenes.

==== RAID

La tecnología *RAID* (_Redundant Array of Inexpensive Disks_) permite
combinar varios discos duros para mejorar las prestaciones a través del
paralelismo en el acceso o para mejorar la fiabilidad a través del
almacenamiento de información redundante. En concreto se definen
diversos _niveles RAID_, de entre los cuales los más comunes son:

* _En un *conjunto RAID 0* se distribuyen los datos equitativamente en
bloques de tamaño fijo entrelazados entre dos o más discos_, sin
incluir ningún tipo de información redundante. Esto permite leer y
escribir más datos en el mismo tiempo ya que se pueden enviar en
paralelo peticiones a los distintos discos. Sin embargo la fiabilidad es
inversamente proporcional al número de discos, ya que para que el
conjunto falle basta con que lo haga cualquiera de ellos.

* _En un *conjunto RAID 1* se crea una copia exacta —en espejo— de los
datos en dos o más discos_. El resultado es que, incluso con dos
discos, se incrementa exponencialmente la fiabilidad respecto a tener
uno solo, ya que para que el conjunto falle es necesario que lo hagan
todos los discos. Adicionalmente el rendimiento en las operaciones de
lectura se incrementa linealmente con el número de copias, ya que los
datos están disponibles en todos los discos al mismo tiempo, por lo que
se pueden balancear la operaciones de lectura entre todos ellos.

* __En un *conjunto RAID 5* se distribuyen los datos equitativamente en
bloques de tamaño fijo entrelazados entre dos o más discos y se
utiliza uno adicional para almacenar la información de paridad de los
bloques de una misma división__footnote:[En RAID se denomina división o
stripe a la serie de bloques consecutivos escogido cada uno de uno de
los discos del conjunto.]. El disco utilizado para almacenar el bloque
de paridad cambia de forma escalonada de una división a la siguiente, de
ahí que se diga que _el bloque de paridad está distribuido_. Algunos
aspectos adicionales a tener en cuenta son que:

    ** _Cada vez que se escribe un bloque de datos se debe actualizar el bloque de paridad_. Por lo tanto las escrituras en un conjunto RAID 5 son costosas en términos de operaciones de disco y tráfico.

    ** _Los bloques de paridad no se leen durante las lecturas de datos_, ya que eso reduciría el rendimiento. Sólo se hace en caso de que la lectura de un sector falle, puesto que _el sector en la misma posición relativa dentro de cada uno de los otros bloques de datos de la división y en el bloque de paridad se pueden utilizar para reconstruir el sector erróneo._

 ** _En un conjunto RAID 5 el fallo de 2 discos provoca la pérdida completa de los datos_. Esto significa que aunque se pueden añadir discos de manera ilimitada, eso no suele ocurrir puesto que _a más discos en el conjunto más probabilidad de que fallen dos de ellos_.

* _En un *conjunto RAID 6* se utiliza la misma estrategia que en RAID 5 pero _en cada división hay dos _bloques de paridad —_en lugar de uno—_ _en dos discos diferentes_. Esto permite que fallen hasta dos discos sin perder los datos.

* _En un conjunto con niveles anidados se combinan varios niveles RAID
básicos como si fueran capas superpuestas_. Ejemplos típicos son:

    ** RAID 0+1, donde se hace un espejo de un conjunto RAID 0.

    ** RAID 1+0 o RAID 10, donde diversos conjuntos en espejo se combina en un RAID 0, aumentando la capacidad total.

    ** RAID 50, donde diversos conjuntos RAID 5 se combinan en un RAID 0, aumentando también la capacidad total.

La implementación de RAID es otra de las áreas donde existen diversas
variantes:

* _RAID puede implementarse en el hardware de la controladora de
disco_, de tal forma que sólo los discos conectados a esta pueden
formar parte de un conjunto RAID determinado. Esta solución es muy
eficiente, especialmente cuando se utilizan niveles que requieren
cálculo de la paridad, ya que se evita utilizar tiempo de CPU para ese
trabajo. Sin embargo estas controladoras son notablemente más caras que
las que carecen de soporte para RAID.

* _RAID puede implementarse dentro del sistema operativo en lo que se
denomina el software de **gestión de volúmenes**_. En este caso las
soluciones RAID con paridad son bastante lentas por lo que normalmente
sólo se soportan los niveles RAID 0, 1, 10 o 0+1. Algunas controladoras
de disco modernas que dicen venir con soporte RAID realmente implementan
esta tecnología en software, a nivel del controlador de dispositivo,
mientras que en el hardware sólo se implementan unas características de
apoyo mínimasfootnote:[En algunos entornos se denomina a este tipo de
implementaciones _fakeRAID_ o _hostRAID_.].

Cada conjunto RAID se comporta como una unidad de almacenamiento
independiente desde el punto de vista del resto del sistema, por lo que
se puede utilizar entero para albergar un único sistema de archivos. Sin
embargo, lo más común es dividirlo en regiones con el objeto de utilizar
múltiples sistemas de archivos o combinarlo en estructuras de mayor
tamaño, para lo cuál se pueden utilizar alguna de las técnicas que
veremos a continuación.

==== Particiones

_Un disco, un conjunto RAID o cualquier otro dispositivo de
almacenamiento se puede dividir en regiones para utilizar en cada una de
ellas un sistema de archivos diferente_. A esas regiones se las conoce
comúnmente como *particiones*, *franjas* o *minidiscos*.

Según la plataforma, existen diversas maneras de implementar el soporte
de particiones. Entre los sistemas de escritorio las tecnologías más
difundidas y utilizadas son la *MBR* (_Master Boot Record_) y la *GPT*
(_GUID Partition Table_). En ambas _se almacena, en los primeros
sectores del dispositivo de almacenamiento, una tabla con una entrada
por partición donde se guardan las direcciones del primer y último
sector de cada una de ellas en el dispositivo_, así como otra
información. Eso es todo lo que necesita el sistema operativo para
determinar los límites de la región ocupada por cada sistema de
archivos.

==== Volúmenes dinámicos

Según la tecnología que se utilice para particionar es posible
encontrarse con una serie de restricciones comunes:

* El limitado número de particiones diferentes que puede contener un
mismo dispositivo.

* Limitaciones o imposibilidad de redimencionar las particiones.
Especialmente si el sistema operativo está en ejecución.

* La imposibilidad de crear particiones que hagan uso de regiones libres
en diferentes dispositivos de almacenamiento.

Para resolverlo algunos sistemas operativos incluyen un _software de
gestión de volúmenes_ que hace uso de tecnología propia para superar
estas limitaciones. Estas herramientas generalmente permiten agrupar
dispositivos completos, conjuntos RAID, particiones, etc. y sobre ellos
construir los volúmenes que sean necesarios. Estos volúmenes pueden ser
redimensionados —en ocasiones sin tener que detener la ejecución del
sistema operativo— y en caso de que haga falta se pueden incluir
dinámicamente nuevos dispositivos para incrementar el espacio
disponible. Además, como ya hemos comentado, el software de gestión de
volúmenes puede incluir alguna funcionalidad propia de conjuntos RAID
con el objeto de mejorar las prestaciones, a través del paralelismo en
el acceso, o mejorar la fiabilidad a través del almacenamiento de
información redundante.

=== Sistemas de archivos

Cada volumen puede albergar un sistema de archivos. A continuación
estudiaremos los elementos más comunes a la mayor parte de los sistemas
de archivos.

==== Estructura de un sistema de archivos

Un sistema de archivos suele estar compuesto de varios niveles
diferentes. En la se muestra un ejemplo típico de la estructura de un
sistema de archivos diseñado en niveles. Cada nivel utiliza las
funciones de los niveles inferiores y proporciona nuevas funciones a los
niveles superiores:

. En el nivel más bajo, accediendo directamente a los dispositivos de
almacenamiento, se encuentra el *control de E/S*. _Éste está
compuesto por los controladores de dispositivo encargados de transferir
la información entre la memoria principal y el disco_. Estos
controladores, que generalmente son compartidos entre los distintos
sistemas de archivos, transfieren los datos en unidades de _bloques_
—en lugar de transferir un byte cada vez— para mejorar la eficiencia .
Cada _bloque_ está formado por uno o más sectoresfootnote:[Dependiendo
de la unidad de disco, los sectores pueden tener tamaños de entre 32
bytes y 4096 bytes. Lo más común es que su tamaño sea de 512 bytes.].

. _El *sistema básico de archivos* se encarga de enviar comandos
genéricos al controlador de dispositivo apropiado con el fin de leer y
escribir bloques físicos en el disco_. Cada bloque físico se identifica
mediante su dirección de disco numérica (por ejemplo: unidad 1, cilindro
73, cabeza 2, sector 10).

. _El *módulo de organización de archivos* tiene conocimiento de los
archivos y se encarga de traducir las direcciones lógicas de bloque
—posición del bloque dentro del archivo— en las direcciones
físicas de bloque —poe ejemplo, cilindro, cabeza y sector de
los bloques correspondientes en el dispositivo de almacenamiento—_
que serán enviadas al _sistema básico de archivos_ para que realice las
transferencias solicitadas. Los bloques lógicos de cada archivo son
numerados de 0 a _N_, pero los bloques físicos asignados a estos
bloques lógicos no tienen porqué coincidir en los números de bloque. Por
eso el _módulo de organización de archivos_ debe utilizar la ubicación
del contenido del archivo y la información sobre la asignación de
bloques, para traducir las direcciones lógicas en direcciones físicas.
Además, el módulo de organización incluye el gestor de espacio libre,
que controla los bloques no asignados y proporciona dichos bloques
cuando el _módulo de organización de archivos_ lo necesita.

. _El *sistema lógico de archivos* gestiona los metadatos_. Los
metadatos incluyen toda la estructura del sistema de archivos, excepto
los propios datos de los archivos. Entre dichos metadatos está la
estructura de directorios y los _bloques de control de archivo_. _Un
*bloque de control de archivo* o *FCB* (File Control Block) contiene
información acerca del archivo_, incluyendo su propietario, los
permisos y la ubicación del contenido del mismo. Además, el _sistema
lógico de archivos_ también es responsable de las tareas de protección y
seguridad.

Cada sistema operativo puede soportar uno o más sistemas de archivos
para dispositivos de disco. Por ejemplo, en los sistemas UNIX se utiliza
el _sistema de archivos UNIX_ o UFS (_UNIX File System_), que está
basado en el sistema FFS (_Fast File System_) de Berkeley. Microsoft
Windows soporta los sistemas de archivo FAT, FAT32 y NTFS (_NT File
System_). En Linux se soportan más de cuarenta sistemas de archivo,
entre los que podríamos destacar: el _sistema de archivos extendido_
—ext2, ext3 y ext4— XFS y BTRFS. Además, la mayoría de los sistemas
operativos modernos soportan otros sistemas de archivo, como los
utilizados en los soportes removibles. Por ejemplo el ISO-9660,
utilizado por la mayor parte de los CD-ROM, o el UFS (_Universal File
System_), utilizado por los DVD-ROM.

==== Estructuras de metadatos

Para implementar un sistema de archivos se utilizan diversas estructuras
de metadatos alojadas tanto en el disco como en la memoria. Estas
estructuras varían dependiendo del sistema operativo y del sistema de
archivos. Sin embargo, a continuación intentaremos describir brevemente
las estructuras en disco de uso más común:

* _Un *bloque de control de arranque* (*bloque de inicio* o *sector
de arranque*) que suele ocupar el primer bloque de cada volumen y que
contiene la información necesaria para iniciar un sistema operativo a
partir de dicho volumen_. Este bloque puede estar vacío, si el volumen
no contiene un sistema operativo.

* _Un *bloque de control de volumen* que contiene todos los detalles
acerca del volumen_, tales como: el número máximo de bloques, el tamaño
de los bloques, el número de bloques libres y punteros a los mismos, así
como un contador de bloques de información FCB y punteros a estos. En
los sistemas de archivos para UNIX y Linux, a esta estructura se la
denomina *superbloque*. Mientras que en NTFS esta información se
almacena en la *tabla maestra de archivos* o *MFT* (_Master File
Table_).

* _Un FCB por cada archivo_ donde se almacenan numerosos detalles sobre
el mismo, por ejemplo: los permisos, el propietario, el tamaño y la
ubicación de los bloques de datos. En términos generales todos los FCB
del sistema de archivos se almacenan en una tabla denominada directorio
de dispositivo o tabla de contenidos del volumen. En los sistemas de
archivos para UNIX y Linux cada FCB se denomina *inodo* y se almacenan a
continuación del superbloque. En NTFS esta información se almacena en la
MFT, ya que cada entrada de dicha tabla es un FCB.

* _Una estructura de directorios para organizar los archivos_. En los
sistemas de archivos para UNIX y Linux, cada directorio es como un
archivo especial que almacena los nombres de los archivos que contiene y
los números de FCB asociados a los mismos. En NTFS es similar, aunque la
estructura de directorios completa se almacena en la propia MFT.

La información almacenada en memoria se utiliza tanto para la gestión
del sistema de archivo como para mejorar el rendimiento del mismo
mediante mecanismos de caché. Los datos se cargan en el momento de
comenzar a utilizar el sistema de archivos —montaje— y se descartan
cuando se va a dejar de hacer uso del mismo —desmontaje—. Las
estructuras existentes en la memoria pueden incluir las que a
continuación se describen:

* _Una tabla de montaje en memoria que contiene información acerca de
cada volumen montado_.

* _Una caché en memoria de la estructura de directorios que almacena la
información relativa a los directorios a los que se han accedió
recientemente_. Los directorios que actúan como puntos de montaje puede
contener un puntero a la entrada, en la tabla de montaje, del volumen
montado en el directorio.

* _La tabla global de archivos abiertos que contiene una copia del FCB
de cada archivo abierto en el sistema_, además de otras informaciones.

* _La *tabla de archivos abiertos* de cada proceso_. El PCB de
cada proceso contiene una tabla donde se listan los archivos abiertos
por el proceso. La tabla contiene para cada archivo un puntero a la
entrada correspondiente del mismo archivo en la tabla
global de archivos abiertos, así como otras informaciones
adicionales que son particulares de cada proceso.

==== Montaje de sistemas de archivos

Un sistema de archivos debe _montarse_ para que sus archivos sean
accesibles a los procesos del sistema. El proceso de montaje incluye los
siguientes pasos:

. _Al sistema operativo se le debe proporcionar el nombre o
identificador del dispositivo y el punto de montaje. El *punto de
montaje* es la ubicación dentro de la estructura de directorios —el
directorio concreto— a la que queremos conectar el sistema de
archivos_. Después de que el proceso de montaje se haya completado, los
archivos y directorios del sistema de archivos montado serán accesibles
como descendientes del directorio del punto de montaje.

. A continuación _el sistema operativo verifica que el dispositivo
contiene un sistema de archivos válido_. Para ello lee el _bloque de
control de volumen_ y comprueba que tiene un formato válido.

. Finalmente _el sistema operativo registra en la tabla de montaje
_el tipo de sistema de archivos y el identificador del del dispositivo
montado, mientras en la copia en memoria del FCB del directorio que hace
de punto de montaje almacena un identificador de la entrada
correspondiente en la tabla de montaje_. Esto permite que pueda ser
recorrida la estructura de directorios de distintos sistemas de
archivos, pasando de uno a otro de forma transparente, según sea
necesario.

En muchos sistemas operativos modernos el montaje se ejecuta
automáticamente cuando los dispositivos son detectados durante el
arranque del sistema o cuando se conectan durante el funcionamiento del
mismo —por ejemplo, cuando se inserta un medio en la unidad CD-ROM o se
pincha una memoria flash en un puerto USB—. Además, en algunos se
permite que el administrador del equipo ejecute operaciones de montaje
manuales.

==== Archivos

Cada sistema de archivos almacena en disco una tabla donde cada entrada
guarda un bloque de control de archivo o FCB (File Control Block) por
archivo. Concretamente, en cada FCB se almacena diversa información
acerca del archivo al que representa.

===== Atributos de archivos

La colección de atributos asociada a un archivo varía de un sistema
operativo a otro, pero típicamente son los siguientes:

* *Nombre*. Nombre simbólico del archivo que se mantiene en un formato
legible para conveniencia de las personas.

* *Identificador*. Identifica de forma unívoca el archivo dentro del
sistema de archivos. Generalmente es el índice del FCB en la tabla de
contenidos del volumen, donde se almacenan los FCB.

* *Tipo*. Es un atributo necesario en los sistemas que soportan
diferentes tipos de archivos.

* *Ubicación*. Es un puntero a un dispositivo y a la ubicación del
archivo dentro del mismo.

* *Tamaño*. Indica el tamaño actual de archivo —en bytes, palabras o
bloques— y, posiblemente, el tamaño máximo permitido.

* *Protección*. Información de control de acceso que determina quién
puede leerlo, escribirlo, ejecutarlo, etc.

* *Fecha, hora e identificación del usuario*. Esta información puede
mantenerse para los sucesos de creación, de última modificación y último
uso del archivo. Esto puede resultar útil para la protección, seguridad
y monitorización del uso del archivo.

Los atributos de los archivos se almacenan en las estructuras de
metadatos. Normalmente el nombre se almacena en la estructura de
directorios, de tal manera que una entrada de directorio está compuesta
del nombre de un archivo y del identificador de su FCB. Dicho
identificador permite localizar el FCB en la tabla de contenidos del
volumen, que contiene el resto de los atributos del archivo.

===== Operaciones con los archivos

Un archivo es un tipo abstracto de datos sobre el que pueden realizarse
diversas operaciones. Concretamente _el sistema operativo proporciona
llamadas al sistema para: crear, escribir, leer,
reposicionarfootnote:[Generalmente el sistema mantiene un puntero de
lectura/escritura que hace referencia a la ubicación dentro del archivo
en la que debe tener lugar la siguiente operación. Este puntero se
actualiza avanzando cada vez que se realiza un nueva lectura/escritura.
Para desplazarse aleatoriamente por el archivo, el sistema operativo
debe ofrecer una llamada al sistema que permita reposicionar el puntero
allí donde interese.], borrar y truncar archivos_. Además en muchos
sistemas se suelen incluir llamadas para otras operaciones comunes, como
añadir datos al final de un archivo o el renombrado de un archivo
existente. Estas operaciones primitivas puede combinarse a su vez para
realizar otras operaciones más complejas —por ejemplo, crear una copia
de un archivo o moverlo a otro lugar de la estructura de directorios—.
Además, muchos sistemas también disponen de operaciones para consultar y
modificar diversos atributos de un archivo, como la longitud o el
propietario del mismo.

La mayor parte de estas operaciones implican realizar una búsqueda en el
directorio para encontrar la entrada asociada con el archivo cuyo nombre
se ha indicado. Para evitarlo _muchos sistemas requierenfootnote:[En
unos pocos sistemas los archivos se abren automáticamente cuando un
proceso solicita su primera operación sobre los mismos y se cierran
cuando el proceso termina. Sin embargo lo más común es que los procesos
tengan que abrir los archivos explícitamente.] que el proceso haga una
llamada al sistema open(), antes de realizar cualquiera de estas
operaciones por primera vez sobre un archivo_. En concreto esta llamada
al sistema:

. Busca en el directorio el nombre del archivo hasta encontrar la
entrada asociada y recupera el identificador del mismo.

. Utiliza el identificador del archivo para recuperar el FCB
correspondiente.

. Crea una entrada para el archivo en la tabla de archivos abiertos
donde se almacena la información del FCB.

. Retorna devolviendo un identificador —en forma de puntero o de
índice— a la nueva entrada en la tabla de archivos abiertos.

El nombre con el que se designa a esas entradas en la tabla de archivos
abiertos varía de unos sistemas a otros. En los sistemas UNIX se utiliza
el término *descriptor de archivo* —o _file descriptor_— mientras que
en los sistemas Microsoft Windows se prefiere el término *manejador de
archivo* —o _file handler_—.

Después de utilizar la llamada al sistema `open()`, cuando se desee
solicitar una operación sobre un archivo, sólo es necesario proporcionar
el identificador devuelto, evitando así que haga falta realizar
exploración alguna del directorio. Cuando el archivo deja de ser
utilizado activamente por el proceso, puede ser cerrado utilizado la
llamada al sistema `close()`.

_En los sistemas operativos donde varios procesos pueden abrir un mismo
archivo se suelen utilizar dos niveles de tablas de archivos abiertos_:

. _Una tabla para cada proceso —almacenada en el PCB— donde se
indican todos los archivos que éste ha abierto_. En dicha tabla se
almacena toda la información referente al uso de cada archivo por parte
de un proceso. Por ejemplo, se puede almacenar la posición actual
utilizada por las operaciones de lectura y escritura o los derechos de
acceso.

. _Una tabla global para todo el sistema donde se almacena toda la
información independiente de los procesos_, como la ubicación del
archivo en el disco, las fechas de acceso y el tamaño del archivo.

Cuando un proceso invoca la llamada `open()` se añade una entrada en la
tabla de archivos abiertos del proceso, que a su vez apunta a la entrada
correspondiente dentro de la tabla global del sistema. Si el archivo no
existe en esta última, también hay que crear una entrada en la tabla
global del sistema haciendo uso de la información contenida en disco en
el FCB correspondiente. Es muy común que la tabla global almacene un
_contador de aperturas_ para cada archivo con el objetivo de indicar
cuantos procesos lo mantienen abierto. Dicho contador se decrementa con
cada llamada al sistema `close()`, de forma que cuando alcance cero
querrá decir que la entrada puede ser eliminada de la tabla global de
archivos abiertos.

===== Tipos de archivo

Cuando se diseña un sistema operativo es necesario considerar si debe
reconocer y soportar el concepto de tipo de archivo. Si el sistema
operativo reconoce el tipo de un archivo puede operar con el mismo de
formas razonables. Por ejemplo, el sistema puede impedir que un usuario
intente imprimir los archivos que contienen programas en formato
binario, pues el documento impreso sería ininteligible.

En los sistemas operativos más comunes las técnicas utilizadas para
implementar los tipos de archivo son las siguientes:

* _En MSDOS y Microsoft Windows el tipo de archivo se incluye como
parte del nombre del archivo_. Es decir, el nombre se divide en dos
partes: un nombre y una extensión; normalmente separadas por un carácter
de punto. El sistema puede utilizar la extensión para conocer el tipo de
archivo y el tipo de operaciones que se pueden realizar con el mismo.

* _En macOS cada archivo tiene un atributo que almacena el tipo_
—por ejemplo, `TEXT` para los archivos de texto o `APPL` para las
aplicaciones— y otro que contiene el nombre del programa que lo
creó. Cuando el usuario hace clic con el ratón sobre el icono de un
archivo, el programa que lo creó se ejecuta automáticamente y el archivo
se carga en la memoria.

* _En los sistemas UNIX se utiliza un *número mágico* almacenado al
principio de algunos archivos_ para indicar el tipo del mismo. No todos
los archivos tienen números mágicos, por lo que se permite hacer
sugerencias en forma de extensiones del nombre del archivo. Sin embargo
estas extensiones ni son obligatorias ni el sistema depende de ellas.
Fundamentalmente su objetivo es ayudar a los usuarios a determinar el
tipo de contenido de un archivo, por lo que pueden ser utilizadas o
ignoradas por cada aplicación concreta, en función de las preferencias
de sus desarrolladores.

==== Estructura de directorios

Algunos sistemas de archivos pueden almacenar millones de archivos en
terabytes de disco. Para gestionar todos esos datos necesitamos
organizarlos de alguna manera, lo que generalmente implica el uso de
directorios. _Un *directorio* puede considerarse una tabla de
símbolos que traduce los nombre de los archivos en los identificadores
que permiten recuperar sus correspondientes entradas en la tabla de
contenidos del volumen_, donde se almacenan los FCB. A continuación
vamos a estudiar los diversos esquemas para definir la estructura lógica
del sistema de directorios.

===== Directorios de un nivel

_En la estructura de directorios de un nivel todos los archivos están
contenidos en un único directorio_. Esto presenta algunas limitaciones:

* Cuando el número de usuarios del sistema aumenta se hace más difícil
que cada uno escoja nombres diferentes para sus archivos, lo cual es
necesario puesto que todos los archivos se encuentran en el mismo
directorio.

* Incluso en los sistemas operativos monousuario puede ser difícil para
un usuario mantener organizados sus datos a media que se incrementa el
número de archivos.

Este esquema fue utilizado por la primera versión del sistema operativo
MSDOS.

===== Directorio de dos niveles

_En la estructura de directorios de dos niveles cada usuario tiene su
propio *directorio de archivos de usuario* o *UFD* (User File
Directory) que cuelga del *directorio maestro de archivos* o *MFD*
(Master File Directory)_. Cuando un usuario se conecta al sistema o
inicia un trabajo se explora el MFD, que es una tabla indexada por el
nombre de los usuarios o por los números de cuenta, donde cada una de
sus entradas apunta al UFD de dicho usuario. Puesto que cada UFD incluye
sólo los archivos del usuario al que pertenece, el sistema operativo
puede confinar todas las operaciones que puede realizar un usuarios
sobre los archivos a su UFD. Sin embargo, aunque esto resuelve el
problema de la colisión de nombres entre diferentes usuarios, también
presenta algunas desventajas:

* La estructura descrita aísla a los usuarios, lo cual puede ser un
problema cuando éstos quieren compartir datos para cooperar en alguna
tarea. La solución pasa por utilizar *nombres de ruta* para designar a
un archivo de forma unívoca. Por ejemplo, si el usuario `usera` quiere
acceder a su archivo `test`, simplemente debe referirse a el como
`test`. Mientras que si quiere acceder al archivo `test` del usuario
`userb`, debe utilizar un _nombre de ruta_ como `/userb/test`, donde
se indica el nombre del usuario y el nombre del archivo. En general,
cada sistema operativo utiliza su propia sintaxis par nombrar los
archivos contenidos en los directorios de otros usuarios.

* Incluso en este caso puede ser difícil para un usuario mantener
organizados sus datos a media que se incrementa el número de archivos
personales.

===== Directorios con estructura de árbol

_La estructura de directorio de dos niveles puede generalizarse en la
estructura de directorios en árbol de altura arbitraria_. Esto permite
que los usuarios puedan crear sus propios subdirectorios para organizar
sus archivo de la forma más conveniente.

Cada sistema de archivos tiene un *directorio raíz* que puede contener
tanto archivos como otros directorios. A su vez cada directorio puede
contener un conjunto de archivos y subdirectorios. Normalmente cada
entrada de directorio incluye un bit donde se indica si dicha entrada
apunta a un archivo o a un subdirectorio. Esto se hace así porque los
directorios no son más que archivos con un formato interno especial, por
lo que el sistema debe saber si la entrada apunta a un directorio para
interpretar correctamente los datos del directorio.

Generalmente en el PCB de cada proceso se guarda cual es su
*directorio de trabajo actual*, de forma que cuando se hace
referencia a un archivo usando solo su nombre, se le busca en ese
directorio. Si se necesita un archivo que no se encuentra en el
directorio de trabajo actual, entonces el usuario debe especificar un
nombre de ruta desde el directorio de trabajo actual. O cambiar con una
llamada al sistema el directorio de trabajo del proceso al directorio
donde está almacenado el archivo. Los nombres de ruta pueden ser de dos
tipos:

* _Un *nombre de ruta absoluto* comienza en la raíz y va
indicando los directorios que componen la ruta de forma descendente
hasta llegar al archivo especificado_.

* _Un *nombre de ruta relativo* define una ruta a partir del
directorio actual_.

Con una estructura de directorios en árbol se puede permitir que unos
usuarios accedan a los archivos de otros. Para eso sólo es necesario que
se utilicen nombres de ruta para designar los archivos o que se cambie
el directorio de trabajo actual.

Este tipo de estructura de directorios es la utilizada por MSDOS y por
las distintas versiones de Microsoft Windows.

===== Directorios en grafo acíclico

La estructura de directorio en grafo acíclico es una generalización
natural del esquema con estructura en árbol. A diferencia de éste
último, _la estructura en grafo acíclico permite que los mismo archivos
y subdirectorios existan simultáneamente en distintos lugares de la
estructura de directorios._ Esto, por ejemplo, hace que los usuarios
puedan compartir archivos de forma que se puedan acceder a los mismo
directamente desde el directorio propiedad de los distintos usuarios.
Indudablemente eso significa que para acceder a un archivo o directorio
pueden existir diversas rutas.

Los archivos y subdirectorios compartidos pueden implementarse de
diversas formas:

* _Se pueden crear una entrada de directorio denominada **enlace**_. Un
enlace es, generalmente, un archivo que contiene la ruta relativa o
absoluta de otro archivo o subdirectorio. En los sistemas UNIX a estos
se los conoce como *enlaces simbólicos*.

* _También se pueden duplicar toda la información de la entrada de
directorio del archivo compartido en todos los directorios que también
contienen dicho archivo_. Así, mientras que los _enlaces_ son
claramente diferentes de la entrada original de directorio, las entradas
de directorio duplicadas hacen que la entrada original y la copia sean
indistinguibles. En los sistemas UNIX a las entradas duplicadas se las
conoce como *enlaces duros*.

Una estructura en grafo acíclico es más flexible que una estructura en
árbol, pero no por eso está exenta de inconvenientes:

* _Si estamos intentando recorrer el sistema de archivos completo_
—por ejemplo, para buscar un archivo o para copiarlos en un
dispositivo de copias de seguridad— _debemos evitar acceder más de
una vez a los archivos y subdirectorios compartidos_. No olvidemos que
en los sistemas con estructura en grafo acíclico cada archivo puede
tener múltiples nombres de ruta absoluta. Esto es más sencillo de
resolver en el caso de los enlaces, puesto que podemos evitar
recorrerlos al ser claramente distinguibles del archivo original.

* ¿Cuándo puede liberarse el espacio asignado a un archivo compartido?
Si lo hacemos cuando un usuario lo borra podríamos dejar punteros que
referencian a archivos que no existen.

* El caso más sencillo de resolver es el de los _enlaces_ ya que
pueden ser borrados sin que el archivo original se vea afectado, puesto
que lo que se elimina es el enlace y no el archivo original.

    ** Si lo que se pretende borrar es la entrada de un archivo original que es apuntado desde un _enlace_, entonces no hay problema en hacerlo y liberar el espacio asignado al mismo, dejando que el enlace apunte a un archivo que no existe. Ciertamente podríamos plantearnos la posibilidad de buscar esos enlaces y eliminarlos pero, a menos que el FCB de cada archivo guarde las rutas a los enlaces que le señalan, esta búsqueda puede ser muy costosa. Por eso lo más común es conservar los enlaces hasta que se produzca un intento de utilizarlos, en cuyo caso determinaremos que el archivo referenciado fue borrado y trataremos el acceso al enlace de forma similar a cualquier otro acceso ilegal a un archivo que no existe.

    ** Otra opción es almacenar en la entrada del archivo original un contador con el número de referencias al archivo. Así, cuando el contador sea 0, sabremos que a llegado el momento de liberar el espacio asignado. En los sistemas UNIX se utiliza esta técnica para los _enlaces duros_.

Por último _no debemos olvidar que la estructura de directorios en
grafo se conserva acíclica si se prohíbe que hayan múltiples referencias
a un mismo directorio_. Ese es el motivo por el que en los sistemas
UNIX no se permite que los _enlaces duros_ hagan referencia a
directorios. Sin embargo si se pueden utilizar _enlaces simbólicos_
para este fin, puesto que al ser distinguibles del directorio original
podemos evitar los ciclos si mientras se explora se ignorar dichos
enlaces.

===== Directorios en forma de grafo general

Uno de los principales problemas de la estructura de directorios en
grafo acíclico es garantizar que no exista ningún ciclo. Esto es
interesante puesto que mientras sea así los algoritmos diseñados para
recorrer el grafo y para determinar cuando no existen más referencias a
un archivo son relativamente simples. No olvidemos que:

* _Es importante evitar encontrar cualquier archivo dos o más veces_,
tanto por razones de corrección como de rendimiento.

* _En una estructura de directorios en forma de grafo general donde
existan ciclos puede que el contador de referencias no sea 0, aunque no
hayan más referencias al archivo_. Esto significa que generalmente se
necesita algún mecanismo de recolección de basurafootnote:[La
recolección de basura implica recorrer todo el sistema de archivos y
marcar todos aquellos elementos que sean accesibles. Después, en una
segunda pasada, se elimina todo lo que no esté marcado.] para determinar
con seguridad cuando se ha borrado la última referencia. Sin embargo la
recolección de basura para un sistema de archivos basado en disco
consume mucho tiempo, por lo que en pocas ocasiones se utiliza.

Por tanto, es mucho más sencillo trabajar con estructuras de directorio
en grafo acíclico. Para evitar que en un grafo aparezca un ciclo al
añadir un nuevo enlace, se pueden utilizar diversos algoritmos. Sin
embargo, puesto que suelen ser muy costosos, lo más simple es ignorar
todos los enlaces en los casos en los que se recorre el árbol de
directorios para realizar una tarea en la que es importante no entrar en
un bucle —por ejemplo, al hacer una búsqueda— En el caso de la
duplicación de entradas de directorio —donde las entradas duplicadas
no se pueden distinguir de la original y, por tanto, no se pueden
ignorar— lo más sencillo es que el sistema operativo no permita
crear múltiples referencias a un mismo directorio.

=== Compartición de archivos

Como ya hemos comentado, el que los usuarios puedan compartir archivos
es algo muy deseable pues permite que éstos puedan colaborar en la
realización de una tarea determinada. Sin embargo al añadir esta
característica hay que tener en cuenta algunos aspectos que deben ser
resueltos en el diseño del sistema operativo.

==== Múltiples usuarios y protección

Cuando un sistema operativo admite múltiples usuarios y utiliza una
estructura de directorio que permite que éstos compartan archivos, cobra
gran importancia la protección de los datos. En este sentido el sistema
operativo debe adoptar un papel de mediador en lo que respecta a la
compartición de los archivos.

Para implementar la compartición y los mecanismos de protección el
sistema debe soportar más atributos para cada archivo y directorio que
los que necesita en un sistema monousuario. Aunque a lo largo de la
historia se han adoptado diversos enfoques, la mayoría han evolucionado
hasta utilizar los conceptos de _propietario_ (o _usuario_) y _grupo_
de un archivo:

* _El propietario de un archivo es el usuario que puede cambiar los
atributos y conceder el acceso_. Se trata del usuario que dispone del
mayor grado de control sobre el archivo.

* _El grupo es un conjunto de usuarios que pueden compartir el acceso
al archivo_. El propietario del archivo es quien define que operaciones
pueden ser ejecutadas por los miembros del grupo.

Los identificadores del propietario y el grupo de un archivo se
almacenan junto con los otros atributos en el FCB. Cuando un usuarios
solicita realiza una operación sobre un archivo, se compara el
identificador del usuario con el atributo del propietario para
determinar si el solicitante es el propietario. Exactamente de la misma
manera se puede proceder con los identificadores de grupo. El resultado
de la comparación indicará que permisos son aplicables. A continuación
el sistema aplicará dichos permisos a la operación solicitada y la
autorizará o denegará según sea el caso.

Existen diversas implementaciones del esquema utilizado para determinar
los permisos aplicables aun usuario que pretende operar sobre un archivo
concreto:

* El esquema más general consiste en _asociar a cada archivo o
directorio una *lista de control de acceso* o *ACL* (Access-control
list) que especifique los nombres de usuario o grupos y los tipos de
acceso para cada uno_. Cuando un usuario solicita acceder a un archivo
concreto, el sistema operativo comprueba la ACL asociada a dicho
archivo. Si dicho usuario, o alguno de sus grupos, está incluido en la
lista para el tipo de acceso solicitado, se permite el acceso. Esta
técnica presenta diversas ventajas e inconvenientes:

    ** _Se trata de la técnica más general_, permitiendo la implementación de políticas de acceso muy complejas.

    ** Sin embargo, _construir la lista puede ser una tarea tediosa_. Por ejemplo, si queremos que varios usuarios puedan leer unos archivos determinados, es necesario enumerar todos los usuarios que disponen de ese acceso en las ACL de dichos archivos.

    ** _El FCB, que hasta el momento tenía un tamaño fijo, ahora tendrá que ser de tamaño variable para almacenar la ACL_, lo que requiere mecanismos más complejos de gestión del espacio.

* Para solucionar algunos de los problemas de las ACL _muchos sistemas
utilizan listas de control de acceso condensadas_. Para condensar la
longitud de la lista de control de acceso, muchos sistemas clasifican a
los usuarios en tres grupos: _propietario_, _grupo_ y _otros_. Así
sólo es necesario un campo para cada clase de usuario, siendo cada campo
una colección de bits, donde cada uno permite o deniega el tipo de
acceso asociado al mismo. Por ejemplo, en los sistemas UNIX se definen 3
campos (_propietario_, _grupo_ y _otros_) de 3 bits cada uno:
`rwx`, donde `r` controla el acceso de lectura, `w` controla el acceso
de escritura y `x` controla la ejecución. Las ACL condensadas son más
sencillas de construir, al mismo tiempo que por tener una longitud fija
es mucho más simple gestionar el espacio para el FCB donde se almacena.

* _La técnica más común en los sistemas operativos modernos consiste en
combinar ambos tipos de listas de control de acceso_. Sin embargo esta
solución no está exenta de dificultades:

    ** _Uno de los problemas es que los usuarios deben poder determinar cuando están activados los permisos ACL más generales_. En Linux, por ejemplo, se utiliza el símbolo `+` a listar los permisos de la ACL condensada para indicar dicha circunstancia. Esos permisos pueden ser gestionados utilizando los comandos `setfacl` y `getfacl`.

    ** _Otra dificultad es la relativa a la asignación de precedencias cuando ambas ACL entran en conflicto_. En general se suele asignar a la ACL más prioridad que a la ACL condensada, pues la primera tiene una granularidad más fina y no se crea de forma predeterminada.

La familia de sistemas operativos Microsoft Windows utiliza las ACL más
generales, mientras que en los sistemas operativos Linux y Solaris se
implementan ambos tipos de ACL.

_Otra técnica para resolver el problema de la protección consiste en
asociar una contraseña con cada archivo o directorio_. Sin embargo esto
tiene el inconveniente de que el número de contraseñas que un usuario
puede tener que recordar puede ser muy grande. No olvidemos que si se
utiliza la misma contraseña para todos los archivo, desde el momento en
que esa contraseña sea descubierta todos los archivos serán accesibles.

==== Semántica de coherencia

_La *semántica de coherencia* especifica cuando las modificaciones que
un usuario realice en los archivos serán observables por los otros
usuarios_. La semántica de coherencia está directamente relacionada con
los algoritmos de sincronización de procesos (véase tema
<<_sincronización>>). Sin embargo es normal que esos complejos
algoritmos no se implementen en el caso de la E/S de archivo, debido a
la alta latencia y las bajas velocidades de la transferencia de los
discos y de las redes.

A continuación vamos comentar algunos ejemplos de semántica de
coherencia:

===== Semántica de UNIX

Los sistemas de archivos de los sistemas operativos UNIX utilizan la
siguiente semántica de coherencia:

* _Las escrituras en un archivo abierto por parte de un proceso son
visibles inmediatamente para los procesos que tengan abierto el
mismo archivo_.

* _Existe un modo de compartición que permite a los procesos compartir
el puntero de ubicación actual dentro del archivo_. Así, el incremento
de ese puntero por parte de un proceso afecta a todos los procesos que
estén compartiendo el archivo.

En la semántica de UNIX cada archivo está asociado con una única imagen
física a la que se accede en forma de recurso de acceso exclusivo
—por ejemplo, un proceso que haga un `read()` sobre un
archivo podrá quedar en espera si al mismo tiempo otro proceso está
ejecutando un `write()`, hasta que este último
termine—. La contienda por acceder a esta imagen única provoca
retardos en los procesos debido a estos bloqueos.

===== Semántica de sesión

Suponiendo que _una *sesión de archivo* es el conjunto de operaciones
entre las llamadas `open()`y `close()`_, el sistema de archivos Andrew —o
AFS— utiliza la siguiente semántica de coherencia:

* _Las escrituras en un archivo abierto por parte de un proceso no son
visibles inmediatamente para los otros usuarios que hayan abierto ese
mismo archivo_.

* _Una vez que se cierra un archivo, los cambios realizados en él son
visibles únicamente en las sesiones que comiencen posteriormente_. Las
sesiones ya abiertas sobre el archivo no reflejarán dichos cambios.

Esto significa que un archivo puede permanecer temporalmente asociado a
varias imágenes físicas al mismo tiempo. Así se permite que múltiples
usuarios realicen accesos concurrentes, tanto de lectura como de
escritura, en sus propias imágenes del archivo, evitando los retardos.

===== Semántica de archivos compartidos inmutables

En esta semántica, _cuando un archivo es declarado como compartido por
su creador ya no puede ser ser modificado_. Estos archivos inmutables
cumplen dos propiedades clave: su nombre no puede reutilizarse y su
contenido no puede ser modificado. Así podemos estar seguros de que el
contenido de un archivo inmutable es fijo. La implementación de esta
semántica en un sistema distribuido es muy simple.

==== Bloqueos de archivo

Algunos sistemas operativos proporcionan funciones para bloquear un
archivo —o determinadas porciones de un archivo— abierto. Esto permite
que un proceso _impida que otros procesos puedan acceder al archivo
bloqueado_. Los bloqueos de archivo resultan útiles para encadenar
varias operaciones de E/S sobre un archivo teniendo la seguridad de que
otros procesos no podrán hacer modificaciones en el mismo mientras
tanto.

Los sistemas operativos pueden proporcionar diferentes tipos de bloqueos
de archivo:

* _Un *bloqueo compartido* es un tipo de bloqueo que puede ser adquirido
—bloquear el archivo— al mismo tiempo por varios procesos._

* _Un *bloqueo exclusivo* sólo puede ser adquirido por un proceso cada
vez_. Si otro proceso intenta adquirir bloqueo exclusivo sobre un
archivo ya bloqueado, se suspende a la espera.

Algunos sistemas operativos sólo proporcionan el _bloqueo exclusivo_.
Sin embargo en los que implementan ambos tipos de bloqueo, lo normal es
que los procesos que pretenden acceder a un archivo compartido para sólo
lectura utilicen el _bloqueo compartido_, mientras que los que acceden
para modificar el contenido utilicen el _bloqueo exclusivo_. Así
varios procesos puedan leer el archivo al mismo tiempo, pero si un
proceso accede para escribir ningún otro podrá acceder ni para leer ni
para escribir.

Además los sistemas operativos pueden proporcionar mecanismos de bloqueo
de archivos:

* *Obligatorios*. Si un bloqueo es obligatorio, después de que un
proceso adquiera un bloqueo exclusivo, _el sistema operativo impedirá a
todos los demás procesos que hagan cualquier operación sobre el
archivo bloqueado_. Esto ocurrirá incluso si los otros procesos no han
sido programados para intentar adquirir el bloqueo. Por tanto, el
sistema operativo es el encargado de garantizar que los bloqueos se
cumplen, haciendo las comprobaciones pertinentes en las llamadas al
sistema.

* *Sugeridos*. Si un bloqueo es sugerido, _el sistema operativo sólo
impedirá que accedan al archivo bloqueado aquellos procesos programados
para adquirir el bloqueo explícitamente_ —usando la llamada al sistema
correspondiente—. El sistema operativo no impedirá el acceso al archivo
a un proceso que lo abre, lee o escribe sin más. Son los desarrolladores
del software los encargados de intentar adquirir el bloqueo y de
liberarlo cuando ya no es necesario.

Como regla general los sistemas operativos Microsoft Windows implementan
un mecanismo de bloqueo obligatorio, mientras que los sistemas UNIX
emplean bloqueos sugeridos.

=== Coherencia

Como hemos comentado anteriormente, parte de los metadatos se almacena
en la memoria principal para acelerar el acceso. Dicha información
generalmente está más actualizada que la correspondiente en el disco,
puesto que la información almacenada en la memoria no tiene porque ser
escrita inmediatamente después de una actualización.

¿Qué ocurriría entonces si fallase el sistema? Pues que el contenido de
la caché y de los búferes se perdería y con ellos también los cambios
realizados en los directorios y archivos abiertos. Esto puede dejar el
sistema de archivos en un estado incoherente, pues el estado real de
algunos archivos no sería el que se describe en la estructura de
metadatos.

==== Comprobación de coherencia

_El *comprobador de coherencia* comprueba la estructura de metadatos y
tratar de corregir todas las incoherencias que detecte_.

Los algoritmos de asignación y de gestión del espacio de almacenamiento
dictan los tipos de problemas que el comprobador puede tratar de
detectar y también el grado de éxito que el comprobador puede tener en
esa tarea. Por ejemplo la pérdida de un FCB, cuando es este el que
almacena la lista de bloques que contienen los datos del archivo, es
desastrosa porque no hay forma de saber en todo el disco que datos le
pertenecen. Por esta razón UNIX almacena en caché las entradas de
directorio para acelerar las lecturas, pero todas las escrituras de
datos que provoquen algún cambio en la asignación de espacio o en algún
otro tipo de metadato se realizan síncronamente —antes de volver al
proceso desde la llamada al sistema—.

Por ejemplo, si se hace un escritura de datos que extiende el tamaño de
un archivo; el cambio del FCB correspondiente, con el nuevo tamaño de
archivo y la lista actualizada de las direcciones de los bloques que
contienen o van a contener los datos del archivo, se escribe en disco
antes de terminar la llamada al sistema y devolver el control al proceso
que la invocó. Sin embargo, no ocurre lo mismo con los datos que el
proceso quería escribir en el archivo. El sistema operativo puede
copiarlos a búferes internos en la memoria para escribirlos en disco más
adelante, evitando interrumpir el proceso durante más tiempo.

==== Soft Updates

Para mejorar la eficiencia del sistema de archivos, sin comprometer la
coherencia en caso de fallo, los distintos sabores de los sistemas UNIX
BSD utilizan una técnica denominada _soft updates_. _Cuando se monta
un sistema de archivos con la opción *soft updates* el sistema operativo
desactiva la escritura síncrona de los metadatos, permitiendo que estos
sean escritos cuando los algoritmos de gestión de la caché lo consideren
necesario, pero se impone cierto orden en el que dichas operaciones de
escritura deben ser realizadas_. Por ejemplo, cuando se van a escribir
en el disco las modificaciones debidas a la creación de un nuevo
archivo, el sistema se asegura de que primero se escribe el nuevo FCB
—un _inodo_, en los sistemas UNIX BSD— y posteriormente escribe el
directorio con la nueva entrada de archivo con el identificador a dicho
FCB. Es sencillo darse cuenta de que haciéndolo al revés, si el sistema
fallase antes de crear el FCB, acabaríamos con una entrada de directorio
que apuntaría a un FCB inválido. Mientras que de esta manera el sistema
de archivos permanecerá consistente aunque el sistema falle entre ambas
operaciones.

==== Sistemas de archivos basados en registro

Otra solución al problema de la coherencia consiste en aplicar técnicas
de recuperación basadas en registro durante las actualizaciones de los
metadatos del sistema de archivos.

Fundamentalmente __en los *sistemas de archivos basados en registro* —o
con *journaling*— todos los cambios en los metadatos se escriben
secuencialmente en un registro__footnote:[El registro generalmente se
almacena en el mismo sistema de archivos. Sin embargo también _suele
ser_ posible almacenarlo en otro volumen o incluso en otro disco.]:

// TODO: Usarlo numerado

* _Cada conjunto de operaciones necesario para realizar una tarea
específica sobre el sistema de archivos es una **transacción**_. Por
ejemplo, es una transacción el conjunto de operaciones necesarias para
crear un nuevo archivo.

* _La lista de operaciones necesarias para completar una
transacción se escribe secuencialmente y síncronamente —antes de
terminar la llamada al sistema— en el registro_. Cuando la lista de
operaciones pendientes termina de ser escrita en el registro, se
considera que las operaciones ha sido _confirmadas_ y la llamada al
sistema puede volver al proceso de usuario, permitiendo que continúe con
su ejecución.

* Mientras tanto, el sistema operativo va ejecutando las operaciones
indicadas en el registro sobre las estructuras reales del sistema de
archivos. _A medida que se realizan los cambios se actualiza el
registro para indicar las operaciones completadas_.

* _Cuando todas las operaciones de una transacción se han ejecutado con
éxito, dicha transacción se considera completada y se elimina del
registro_.

En el supuesto de que el sistema falle:

* _Se comprueba el registro durante el montaje del sistema de
archivos, antes de que pueda ser utilizado de nuevo._

* _Todas las transacciones confirmadas que contenga el registro
estarán a medias_, por lo que será necesario terminar de aplicar las
_operaciones pendientes_ antes de finalizar el proceso de montaje.

* Es posible que existan transacciones no confirmadas, es decir,
transacciones que no terminaron de ser escritas en el registro antes del
fallo y, por tanto, cuya lista de operaciones no está completa. En ese
caso, _todos los cambios correspondientes a las transacciones no
confirmadas que hubieran sido aplicados al sistema de archivos, deberán
deshacerse_ para preservar la coherencia.

Esta técnica está empezando a resultar común en muchos sistemas
operativos. Hasta el punto de que es utilizada en sistemas tales como:
ext3, ext4, NTFS, XFS, JFS, ReiserFS, etc.

_Un efecto colateral de la utilización de un registro es la mejora del
rendimiento en el acceso al sistema de archivo_. La razón de esta
mejora es que las costosas escrituras síncronas —es decir, antes de
devolver el control al proceso— de los metadatos en lugares aleatorios
del volumen se transforman en escrituras síncronas secuenciales —que son
mucho más eficientes— en el registro. Mientras que todas las operaciones
indicadas en el registro se aplican asíncronamente mediante escrituras
aleatorias en las estructuras apropiadas, por lo que pueden ser
reordenadas a conveniencia para maximizar el rendimiento. Recordemos que
en el registro pueden haber operaciones de distintos procesos que
afecten a regiones próximas del disco. El resultado global es una
significativa ganancia en la velocidad de las operaciones relativas a
los metadatos, como por ejemplo la creación y borrado de archivos.

El sistema de archivos XFS modifica ligeramente esta técnica,
sustituyendo las escrituras síncronas necesarias para actualizar el
registro por escrituras asíncronas —es decir, el control se devuelve al
proceso antes de terminar de escribir las operaciones en el registro y
confirmar la transacción—. El resultado es:

* Cierta mejora del rendimiento, porque el registro deja de ser el
cuello de botella para las operaciones sobre los metadatos. El registro
es un recurso de acceso exclusivo. Las operaciones de una transacción se
deben escribir antes de devolver el control al proceso y de permitir que
otro proceso a la espera escriba las operaciones de su transacción.

* En el caso de que el sistema fallase, el uso de escrituras asíncronas
podría provocar la corrupción del registro porque pueden ocurrir en
cualquier orden. Para evitarlo, XFS impone cierto orden en las
operaciones de escritura sobre el registro, de forma similar a como se
hace con los _soft updates_, de tal forma que asegura la coherencia
del registro.

==== Sistemas de archivos basados en copia durante la escritura

_Las técnicas anteriores son necesarias para preservar la coherencia
porque la modificación de los metadatos se hace sobrescribiendo los
datos que ya existen_. Es decir, cuando se crea un nuevo archivo el
sistema busca un FCB libre, sobrescribe el bloque del dispositivo donde
lo encuentra para almacenar el nuevo FCB, busca una entrada libre en el
directorio y, nuevamente, sobrescribe el bloque del disco donde se
almacena el directorio para incorporar la nueva entrada. Si algunos de
estos cambios tienen lugar pero otros no, el disco puede quedar
inconsistente.

_Los **sistemas de archivos basados en copia durante la
escritura**_ —o _copy-on-write_— _evitan cambiar los metadatos
sobrescribiendo en el sitio_. En su lugar buscan un hueco libre,
hacen en él una copia del bloque completo con los cambios y después
modifican los metadatos del sistema de archivos que sirven para
localizar el bloque modificado en su nueva ubicación. Estos cambios, a
su vez, tampoco se hacen sobrescribiendo, sino que disparan la creación
de copias modificadas de los bloques afectados, lo que nuevamente va
seguido de cambios en los metadatos que ayudan a localizarlos. El
proceso se repite hasta que se alcanza el _bloque de control de volumen_
y se cambia, momento en el que toda la secuencia de cambios se
consolida.

Los sistemas de archivos basados en copy-on-write suele hacer hacer uso
intensivo de estructuras de datos basadas en árbol porque es muy
sencillo mover un nodo de bloque, con un efecto mínimo en el resto de la
estructura. Por ejemplo, al crear un archivo:

. _Se busca un FCB libre, se lee el bloque que lo contiene en la
memoria principal, se modifica y se escribe en un bloque libre_. El
sistema de archivos debe tener alguna estructura de datos que permita
encontrar el bloque que contiene un FCB a partir de su identificador.
Por lo general esta estructura es algún tipo de árbol. Así que se
modifica el nodo del árbol que señala al bloque con el nuevo FCB para
que conozca la nueva ubicación. Este cambio implica crear un copia del
bloque de dicho nodo con el cambio, lo que a su vez significa modificar
el nodo que señala a este. Y así sucesivamente hasta llegar a la raíz
del árbol de FCB.

. _Se busca una entrada libre en el directorio que va a contener al
archivo y se modifica para añadir el nombre del archivo, el
identificador de su FCB y otras propiedades_. Nuevamente, este cambio
significa crear una copia, con los cambios descritos, del bloque que
contiene la entrada y modificar el FCB del directorio para que contenga
la nueva ubicación del bloque con el contenido del directorio. Como
antes, este cambio en el FCB dispara copias y modificaciones por todo el
árbol de FCB, hasta la raíz

. Una vez la raíz del árbol ha sido copiada a una nueva ubicación con
los cambios, se actualiza su nueva posición en el _bloque de control de
volumen._

Si el sistema falla antes de la modificación del _bloque de control de
volumen_, durante el montaje del sistema de archivos no quedará ni
rastro de ninguno de los cambios porque dicho bloque aun hace referencia
a la antigua raíz del árbol de FCB y, a partir de ellas, a todos los
nodos, bloques y FCB originales. Obviamente los sistemas que implementan
este tipo de sistemas de archivo usan la memoria principal como caché
con el objeto de combinar varias modificaciones sobre un mismo bloque
antes de proceder a su escritura en disco, evitando desencadenar
múltiples veces los cambios posteriores.

Los sistemas de archivos ZFS y Btrfs son los principales ejemplos de
sistemas de archivos basados en _copy-on-write_. Esta solución no sólo
les permite tener las mismas propiedades que el uso de registro en
cuanto a la preservación de la coherencia —con la ventaja de evitar dos
escrituras en disco, una en el registro y otra para el cambio
propiamente dicho— sino que además facilita que puedan ofrecer
características adicionales, como la creación de copias instantáneas del
volumen.

=== Implementación de sistemas de archivos

Como ya se ha comentado, un sistema de archivos suele estar compuesto de
varios niveles diferentes. En la se muestra un ejemplo de la estructura
de un sistema de archivos diseñado en niveles. Cada nivel utiliza las
funciones de los niveles inferiores y proporciona nuevas funciones a los
niveles superiores. Estos niveles han sido descritos en el apartado
<<_estructura_de_un_sistema_de_archivos>>, mientras que las estructuras de metadatos
utilizadas tanto en la memoria como en disco fueron tratadas brevemente
en el <<_estructuras_de_metadatos>>.

A continuación vamos a profundizar aun más en las estructuras y
operaciones utilizadas para implementar los sistemas de archivos

==== Implementación de directorios

Cada directorio suele contener una estructura de datos que relaciona el
nombre de cada archivo que contiene con el identificador de su FCB.
Dicho identificador permite localizar el FCB en la tabla de contenidos
del volumen, que contiene el resto de los atributos del archivo.

En esta sección vamos a estudiar las formas más comunes de implementar
la estructura de datos de un directorio.

===== Lista lineal

El método mas simple para implementar un directorio _consiste en
utilizar una lista lineal o vector de nombres de archivos junto
al identificador al FCB de cada uno_.

Las acciones a realizar, para implementar cada una de las posibles
operaciones sobre el directorio, serían:

* *Crear un archivo*. Primero se explora el directorio para estar
seguros de que no haya ningún archivo con el mismo nombre. Después se
añade una nueva entrada al final del directorio.

* *Borrar un archivo*. Primero se explora la lista en busca del
archivo especificado y una vez localizada se libera la entrada
correspondiente. Para reutilizar la entrada del directorio tenemos
diversas alternativas:

    ** _Se puede marcar la entrada como no utilizada_. Para eso se puede emplear un nombre especial o utilizar algún campo adicional —a parte de nombre de archivo e identificador del FSB— que se ha añadido a la entrada con ese propósito.

    ** _Insertar un puntero a la entrada en una lista de entradas libres_, que se guarda dentro del mismo directorio.

    ** _Copiar la última entrada del directorio en la ubicación que ha quedado libre_ y reducir la longitud del directorio.

La principal desventaja de un directorio implementado como una lista
lineal de entrada es que _para localizar un archivo es necesario
realizar una búsqueda lineal_, lo cual puede resultar muy costoso en
directorios con un número muy grande de archivos. Utilizando una lista
ordenada se puede reducir el tiempo medio de búsqueda, pero eso complica
los procesos de creación y borrado, pues puede que sea necesario mover
cantidades importantes de información para mantener la lista ordenada.
También se _puede utilizar una lista enlazada tanto para reducir el
tiempo necesario para borrar un archivo como para facilitar la tarea de
mantener ordenada la lista_.

Los sistemas de archivos FAT y FAT32 implementan los directorios
utilizando una lista lineal, donde en cada entrada no sólo se almacena
el nombre del archivo sino también el FCB del mismo. Los sistemas de
archivos ext2 y UFS también utilizan una lista lineal no ordenada, donde
sólo se almacena el nombre del archivo o subdirectorio y el
identificador del _inodo_ —el FCB, esos sistemas de archivo—
correspondiente.

===== Tabla de dispersión

En los directorios implementados con una tabla de dispersión también
_se almacenan las entradas de directorio en una lista lineal, pero al
mismo tiempo se utiliza una tabla de dispersión para reducir
enormemente el tiempo de búsqueda en el directorio_. La tabla de
dispersión se indexa con un valor calculado por cierta función de
dispersión a partir del nombre del archivo para obtener la ubicación de
dicho archivo dentro de la lista lineal.

_El único inconveniente es que debemos tratar la posible aparición de
colisiones_, que son aquellas situaciones en las que dos nombres de
archivo proporcionan, al aplicar la función de dispersión, la misma
ubicación en la tabla. Esto se puede resolver utilizando una lista
enlazada en cada entrada de la lista —cada entrada en al lista señalaría
la ubicación de la siguiente entrada de la lista que tiene el mismo
valor para la función de dispersión— a cambio de que las búsquedas sean
un poco más lentas. En cualquier caso, éste método será normalmente más
rápido que una búsqueda lineal por todo el directorio.

===== Árbol B

Para mantener el directorio ordenado, algunos sistemas de archivos
modernos utilizan estructuras de datos en árbol más sofisticadas, como
por ejemplo árboles B.

Un caso concreto es el sistema de archivos NTFS, utilizado por Microsoft
Windows. NTFS utiliza una estructura de datos denominada árbol B+ para
almacenar el índice de los nombres de archivo contenidos en un
directorio. En la entrada en la MFT de cada directorio se almacena un
atributo, denominado _raíz del índice_ que, si el directorio es de
pequeño tamaño, contiene todas las entradas de archivos del directorio.
Pero para un directorio de gran tamaño, la _raíz del índice_ sólo puede
almacenar unas pocas entradas de archivos del directorio. En ese caso la
_raíz del índice_ contiene el nivel superior del árbol B+. Es decir,
cada una de esas entradas de archivos en la _raíz del índice_ incluye
también un puntero al bloque del disco que contiene un nodo del árbol
con las entradas con nombres alfabéticamente anteriores a ese. Si en
dicho nodo tampoco caben todas las entradas, sólo podrá contener algunas
de ellas, por lo que cada una tendrá a su vez un puntero a un nuevo nodo
del árbol; y así sucesivamente

Las _ventajas_ de los árboles B+ son:

* _Eliminan el coste de reordenar las entradas del directorio._

* _La longitud desde la raíz del árbol hasta un nodo hoja es la misma
para todas los caminos por el árbol_.

[NOTE]
====
El sistema de archivos XFS también utiliza un árbol B+, pero en éste
caso la implementación es un poco más compleja:

. Un directorio de pequeño tamaño almacena sus entradas como una lista lineal no ordenada dentro de su mismo _inodo_ o FCB.

. Cuando el directorio no cabe en el _inodo_ se le asigna un bloque propio, donde el directorio es implementado con una tabla de dispersión, tal y como hemos visto anteriormente.

. Cuando el tamaño del directorio excede el tamaño del bloque, la tabla de dispersión se extrae y se almacena en un bloque diferente. La lista lineal también se extrae, pero no tiene que ser almacenada en un único bloque, sino que puede estar repartida por distintos bloques a lo largo del disco.

. Finalmente, cuando la tabla de dispersión excede el tamaño de un bloque, dicha tabla se convierte en un árbol B+.
====

==== Métodos de asignación

El siguiente problema es _cómo_ asignar el espacio disponible en el
disco a los archivos almacenados, de forma que el espacio sea utilizado
de la forma más eficiente y que se pueda acceder a los archivos de la
forma más rápida posible.

Como la unidad mínima de asignación de espacio a un archivo es el
bloque, la fragmentación interna suele ser un problema común a todos los
métodos que veremos a continuación.

===== Asignación contigua

_La *asignación contigua* requiere que cada archivo ocupe un conjunto
contiguo de bloques en el disco_. Esto es muy eficiente, puesto que el
acceso a todos los datos de un archivo requiere un movimiento mínimo del
cabezal del disco.

* _El problema de la asignación contigua puede verse como un caso
concreto del problema de la asignación dinámica del almacenamiento_
(véase el <<_hiperpaginación>>). Es decir, que en un momento
dado tendremos una petición de tamaño _n_ que deberemos satisfacer con
una lista de huecos libres de tamaño variable. Como ya estudiamos
anteriormente, las estrategias más comunes son las de el _primer
ajuste_ y el _mejor ajuste_.

* _La asignación contigua sufre el problema de la **fragmentación
externa**_. La solución sería utilizar alguna forma de
*compactación*, pero esto puede llevar mucho tiempo en discos duros
de gran tamaño y en algunos sistemas esta tarea tiene que realizarse con
el dispositivo desmontado. Por eso es conveniente evitar utilizar
técnicas de compactación en los sistemas en producción. Afortunadamente,
la mayor parte de los sistemas operativos modernos que necesitan
mecanismos de _desfragmentación_ pueden realizar esta tarea sin
detener el sistema, aunque la perdida de rendimiento puede ser
significativa.

* _En la asignación contigua es necesario determinar cuanto espacio
necesita un archivo antes de asignárselo, pero esto no siempre es
posible_. Por ejemplo, si vamos a copiar un archivo, es indudable que
conocemos de antemano cuanto espacio necesita la copia. ¿Pero qué pasa
cuando, por ejemplo, vamos a crear uno nuevo? Entonces cuando se cree el
archivo es necesario que el usuario indique una estimación del espacio
que va necesitar. ¿Y si posteriormente que queremos añadir nuevos datos?
Entonces, si hemos utilizado la estrategia del _mejor ajuste_, lo más
probable es que el espacio situado a ambos lados del archivo ya esté
ocupado. Para resolver esto existen dos posibilidades:

    ** _La primera es terminar el programa de usuario_, emitiendo un error. Entonces, el usuario deberá volver a crear el archivo indicando más espacio y volver a ejecutar el programa. Puesto que las ejecuciones repetidas pueden ser muy costosas, lo más común es que el usuario acabe sobrestimando el espacio, lo que dará como resultado un desperdicio de espacio considerable.

    ** _La segunda es buscar un hueco libre de mayor tamaño y copiar el contenido del archivo al nuevo espacio_. Esto puede hacerse siempre que exista suficiente espacio, aunque puede consumir bastante tiempo.

Para minimizar estos problemas, se puede implementar un esquema de
asignación contigua modificado, donde _se asigna inicialmente un bloque
contiguo de espacio al archivo y, posteriormente, si dicho espacio
resulta no ser lo suficientemente grande, se añade otra área de espacio
contiguo, denominado *extensión*_. La ubicación de los bloques de un
archivo se registra incluyendo en el FCB la dirección del primer bloque
de cada extensión que compone el archivo, así como el número de bloques
que ocupa cada una.

Los sistemas de archivo XFS y ext4 utilizan extensiones para optimizar
su funcionamiento, pues cuantos más bloques contiguos sean asignados a
un archivo, menos reposicionamientos del cabezal del disco son
necesarios para leerlos. Por ejemplo, en ext4 el espacio se asigna a los
archivos en extensiones de hasta 128MB en bloques, generalmente, de 4KB.

===== Asignación enlazada

_En la *asignación enlazada* cada archivo es una lista enlazada de
bloques de disco_, pudiendo estos bloques estar dispersos por todo el
disco:

* _Cada entrada de directorio contiene un puntero al primer_ bloque y,
en ocasiones, al último para facilitar que se puedan añadir nuevos datos
al final

* _Cada bloque contiene un puntero al bloque siguiente_. Por ejemplo,
si cada bloque tiene 512 bytes de tamaño y un puntero requiere 4 bytes,
los bloques de disco tendrán un tamaño efectivo de 508 bytes.

Este mecanismo resuelve todos los problemas de la asignación contigua.
Además:

* _No hay fragmentación externa_, puesto que pueden utilizarse
cualquier bloque libre para satisfacer una solicitud de espacio.

* _No es necesario declarar el espacio del archivo en el momento de
crearlo_, pues siempre podrá seguir creciendo mientras hayan bloques
libres.

Sin embargo, la asignación enlazada también tiene sus desventajas:

* _Sólo resulta eficaz para archivos de acceso secuencial_. Si
queremos ir directamente al bloque i-esimo de un archivo, tendremos que
comenzar desde el principio e ir leyendo cada bloque para obtener el
puntero que nos indica el siguiente bloque. Es muy posible que en
ocasiones esas lecturas deban ir precedidas de un reposicionamiento de
los cabezales del disco.

* _Se pierde cierta cantidad de espacio con los punteros_. Si, por
ejemplo, un puntero ocupa 4 bytes y un bloque tienen un tamaño de 512
bytes, el 0,758% del espacio en disco será utilizado para los punteros,
en lugar de para almacenar información útil. La solución para este
problema consiste en asignar los bloques en grupos, denominados
*clusters*. Así, el primer bloque de cada _cluster_ sólo tendría que
almacenar un puntero al siguiente _clúster_, lo que reduciría la
cantidad de espacio desperdiciada en los punteros y mejoraría la
eficiencia al reducir el número de reposicionamiento del cabezal del
disco. Sin embargo, también incrementaría el grado de fragmentación
interna pues se pierde más espacio cuando un _cluster_ está parcialmente
lleno.

* _Otro problema es la fiabilidad_. Teniendo en cuenta que los
archivos están enlazados mediante punteros, ¿qué sucedería si uno de
esos punteros se pierde o resulta dañado?

[NOTE]
====
El sistema de archivos FAT utiliza una variante del mecanismo de
asignación enlazada en la que se utiliza una *tabla de asignación de
archivo* o *FAT* (_File-Allocation Table_). Éste método consiste
en lo siguiente:

* La FAT es una tabla que contiene una entrada por cada bloque del disco y que se indexa según el número de bloque. Es decir, la entrada 10 de la FAT contiene información del bloque 10 del disco. La FAT se almacena en una sección al principio del volumen.

* Cada entrada de directorio de un archivo contiene, a parte del nombre de dicho archivo y otras atributos, el número de bloque del primer bloque del disco con datos del archivo.

* La entrada de la FAT indexada según ese número de bloque del primer bloque del archivo contiene el número de bloque del siguiente bloque del archivo. Iterando de esa manera se puede conocer los números de bloque de todos los bloques de un archivo.

* El último bloque del archivo se indicar con un valor especial en su entrada en la FAT.

* Los bloques no utilizados se indican con un valor igual a 0 en su entrada en la FAT.

El uso de la FAT puede provocar un número importante de
reposicionamientos del cabezal de disco debido a que siempre es
necesario volver al principio del volumen para leer la FAT. Por eso, es
muy habitual que el sistema operativo intente mantener una copia de la
FAT en la memoria a modo de cache.

Una de las ventajas de este esquema es que mejora el tiempo de acceso
aleatorio, respecto a la asignación enlazada convencional, porque el
cabezal del disco puede encontrar la ubicación de cualquier bloque a
partir de la información en la FAT.
====

===== Asignación indexada

_El mecanismo de *asignación indexada* agrupa todos los punteros de la
asignación enlazada en una única ubicación_: el *bloque de índices*.
Así se resuelve la falta de eficiencia de la asignación enlazada
—convencional, en ausencia de FAT— cuando se realizan accesos
aleatorios:

* _Cada archivo tiene su propio bloque de índices_, que es un vector
de direcciones de bloques de disco.

* _La entrada i-ésima del bloque de índice contiene la dirección del
bloque i-ésimo del archivo_.

* _Cada entrada de directorio contiene la dirección del bloque de
índices del archivo correspondiente_.

Este mecanismo soporta el acceso aleatorio eficiente, además de no
sufrir el problema de la fragmentación externa. Sin embargo, también
tiene sus desventajas:

* _Se pierde más espacio en los punteros que con el mecanismo de
asignación enlazada_. No olvidemos que siempre hay que reservar un
bloque de índices completo para cada archivo, mientras que con la
asignación enlazada sólo se pierde el espacio de los punteros que
realmente es necesario utilizar.

* _Debemos determinar el tamaño del bloque de índices_. Por lo
anterior y puesto que cada archivo debe tener un bloque de índices, ese
bloque debe ser lo más pequeño posible para no perder espacio. Pero si
es demasiado pequeño, no podrá almacenar suficientes punteros para un
archivo de gran tamaño. Entre los mecanismos que pueden utilizarse para
resolver este problema están los siguientes:

    ** _En el *esquema enlazado* se enlazan los bloques de índices_.
Por ejemplo, se puede utilizar el último puntero del bloque de índices
para apuntar al siguiente bloque de índices. Si dicho puntero tiene el
valor nulo, entonces estamos en el último bloque de índices.

    ** _En el *índice multinivel* los punteros del bloque de índices no
señalan a los bloques del archivo, sino a conjunto de bloques de índices
de segundo nivel_. Estos a su vez señalan a los bloques del archivo.
Esta técnica puede puede ampliarse utilizando un tercer o cuarto nivel,
dependiendo del tamaño máximo de archivo que se desee.

    ** _En el *esquema combinado*_ las primeras entradas del bloque de
índices apuntan directamente a los primeros bloques del archivo.
Mientras que las siguientes entradas contiene punteros indirectos, que
apunta a un conjunto de bloques de índices de segundo nivel, seguidos
por entradas que contienen punteros doblemente indirectos, e incluso
triplemente indirectos.

Para mejorar el rendimiento de los mecanismos de asignación indexados,
es muy común que el sistema operativo intente mantener los bloques de
índices en la memoria caché.

[NOTE]
====
Los sistemas de archivos ext2 y ext3 utilizan el mecanismo de
asignación indexada con esquema combinado. Concretamente el mecanismo en
ext2 se implementa de la siguiente manera:

* El disco se divide en múltiples grupos de bloques.

* En cada grupo, los primeros bloques se utilizan para almacenar una tabla de _inodos_ –los FCB de los archivos en el grupo–. El resto de los bloques se intentan utilizar para almacenar los datos de los archivos representados por los _inodos_ del grupo.

* Entre otra información, dentro de cada _inodo_ se almacenan los punteros a los bloques del archivo, en lugar de utilizar un bloque de índices.

* Los primeros 12 punteros en el _inodo_ son directos, seguidos de un puntero indirecto y un puntero doblemente indirecto. Esto permite que el puntero de archivo sea de 64 bits y, por tanto, que se puedan almacenar 264 bytes de información en cada archivo.
====

==== Gestión del espacio libre

Puesto que el espacio en disco es limitado, necesitamos poder reutilizar
el espacio de los archivos borrados. Para controlar el espacio libre en
el disco, _el sistema mantiene una *lista de espacio libre* que
contiene todos los bloques de disco libres_. Para crear un archivo, se
explora la lista de espacio libre hasta obtener la cantidad de espacio
requerida y asignamos ese espacio al nuevo archivo. A continuación
estudiaremos como puede ser implementada esa lista.

===== Vector de bits

_La lista de espacio libre puede ser implementada como un *vector de
bits* o *mapa de bits*, donde cada bloque es representado por un
bit_. Si el bloque está libre, el bit está a 1; mientras que si el
bloque está asignado, el bit está a 0.

* _Este enfoque es relativamente sencillo y eficiente_, puesto que
muchos procesadores disponen de instrucciones para manipulación de bits
que pueden utilizarse para obtener el primer bloque libre. Por ejemplo,
la familia de procesadores x86, a partir del 80386, tiene instrucciones
que devuelven la posición del primer bit a 1 en el valor de un registro.

* Sin embargo, _los vectores de bits son ineficientes a menos que se
mantenga el vector completo en la memoria principal_, escribiéndose
ocasionalmente en el disco. Esto puede ser imposible para los discos de
gran tamaño, en función de la cantidad de memoria principal. Por
ejemplo, un disco de 40 GB con bloques de 1 KB necesitará un mapa de
bits de más de 5 MB, lo que no es un gran requisito para un sistema
moderno pero si lo era hace dos décadas.

El sistema de archivo NTFS y la familia _extended filesystem_ —es
decir, ext, ext2, ext3, etc.— utilizan mapas de bits tanto para
gestionar los bloques de datos libres como las entradas disponibles en
la tabla de _inodos_.

===== Lista enlazada

Otra técnica _consiste en enlazar todos los bloques de disco libres_.
Para eso se puede mantener un puntero al primer bloque libre en una
ubicación especial del disco y que ese bloque contenga un puntero al
siguiente bloque libre del disco. El segundo bloque contendría un
puntero al tercer bloque libre y así sucesivamente.

_El inconveniente es que recorrer la lista no resulta eficiente_, pues
tenemos que leer cada bloque para conocer la dirección del siguiente
bloque libre en disco. Sin embargo, debemos tener en cuenta que no es
frecuente tener que recorrer la lista de espacio libre completa porque,
por lo general, basta con encontrar el primer bloque libre para asignar
el espacio.

El método FAT incorpora el control de bloques libres dentro de la
_tabla de asignación de archivos_, por lo que no se necesita ningún
método adicional

===== Agrupamiento

_Una modificación de la técnica basada en la lista enlazada consiste en
almacenar las direcciones de n bloques libres en el primer bloque
libre_. Los primeros _n — 1_ de esos bloques estarían realmente libres,
pero el último de esos bloques apuntaría a otro bloque con _n_ bloques
libres. Así, podrían localizarse rápidamente las direcciones de un gran
número de bloques libres, lo cual mejora la eficiencia respecto a la
técnica de lista enlazada.

===== Recuento

Generalmente los bloques son asignados o liberados en bloques contiguos,
especialmente si el espacio es asignado mediante asignación contigua o
en _extensiones_ o _clusters_. Esto puede ser aprovechado para
_mantener una lista donde cada entrada almacena la dirección del primer
bloque de un conjunto de bloques libres contiguo, así como el número de
bloques del conjunto_.

Por ejemplo, el sistema de archivos XFS utiliza un árbol B+ para
almacenar las direcciones de las extensiones de bloques libres y
mantenerlas ordenadas por el tamaño de la extensión a la que apuntan.
Así el sistema operativo puede localizar rápidamente el espacio libre
necesario para satisfacer una necesidad de espacio concreta.

==== Sistemas de archivos virtuales

En el <<_montaje_de_sistemas_de_archivos>> vimos cómo el sistema operativo
_monta_ sistemas de archivos de tal forma que aparenten estar
integrados en una única estructura de directorios, permitiendo a los
usuarios moverse de forma transparente entre distintos dispositivos y
tipos de sistemas de archivos. Para hacerlo, un sistema operativo
moderno debe ser capaz de soportar de manera eficiente distintos tipos
de sistemas de archivos, ocultando sus diferencias de cara a los
usuarios.

Un método para implementar múltiples tipos de sistemas de archivos
consiste en escribir diferentes rutinas de acceso, manipulación y
gestión, a los directorios y a los archivos, para cada uno de los tipos
de sistema de archivo existentes. Sin embargo, en lugar de esta
solución, la mayoría de los sistemas operativos utilizan técnicas de
orientación a objetos para implementar diferentes tipos de sistemas de
archivos detrás de una misma interfaz de programación. Es decir, _se
utilizan estructuras de datos y procedimientos comunes para
separar las llamadas al sistema de los detalles de su
implementación real para cada uno de los sistemas de archivos_.

La implementación de un sistema de archivos está compuesta de tres
niveles fundamentales:

. El primer nivel es la _interfaz del sistema de archivos_, a la que
acceden los desarrolladores a través de las llamadas al sistema. Estamos
hablando de las llamadas `open()`, `read()`, `write()` y
`close()`, entre otras, y de los descriptores de archivos. Esta
interfaz es la misma sea cual sea el sistema de archivos al que se esté
intentando acceder.

. El segundo nivel es _la interfaz del *sistema de archivos virtual*
o *VFS* (Virtual File System)_. Este nivel es utilizado por el anterior
para atender las peticiones realizadas. Describe operaciones genéricas
sobre cualquier sistema de archivos y estructuras genéricas como, por
ejemplo, un FCB virtual —que en algunos sistemas operativos se
denomina _vnodo_— _que identifica de forma unívoca a
cada archivo o directorio en uso en el sistema_ —un _inodo_
en los sistemas de archivos de Linux solo identifica a un archivo de
forma unívoca dentro del mismo sistema de archivos— y que da
acceso a sus metadatos. Este nivel cumple con dos importantes
funciones:

    ** _Separa las operaciones genéricas sobre el sistema de archivos con
respecto a su implementación_. VFS define una interfaz muy clara común
para todos los sistemas de archivos. Pero en el mismo sistema existirán
diversas implementaciones de la interfaz VFS, una para cada sistema de
archivos diferente.

    ** _Proporcionar un mecanismo para acceder de forma coherente a los
archivos a través de la red_. Una implementación de VFS no tiene que
estar limitada exclusivamente a ofrecer acceso a archivos en
dispositivos conectados físicamente al sistema. Las operaciones de la
interfaz VFS pueden resolverse utilizando un protocolo de acceso a algún
servidor de archivos conectado a la red.

. El tercer nivel es donde _se implementa cada tipo de sistema de
archivos o los distintos protocolos de los servidores de archivos en
la red_. La interfaz VFS recurre a la implementación correspondiente
para cada tipo de sistema de archivos para satisfacer las solicitudes de
los niveles superiores. Así, por ejemplo, un `read()` puede implicar que
se tenga que recuperar el _vnodo_ del archivo involucrado desde la tabla
del archivos abiertos, usando el descriptor indicado en la llamada al
sistema. Después se _invocaría la operación _VFS_
`read()`, sobre el _vnodo_, en la implementación concreta de
VFS según el tipo de sistema de archivos
involucrado. Será esa implementación quien extraiga del
_vnodo_ la información necesaria —por ejemplo, el
_inodo_ real del archivo en el sistema de archivos— para
llevar acabo la operación indicada, según las especificidades del
sistema de archivos.

=== Planificación de disco

Como ya hemos comentado, es responsabilidad del sistema operativo usar
los recursos del hardware de forma eficiente. Eso incluye planificar los
procesos en la CPU para conseguir el mínimo tiempo de espera que sea
posible o aprovechar de la mejor forma la memoria principal disponible
para atender la demanda de los distintos procesos al mismo tiempo; pero
también, intentar obtener el menor tiempo de acceso y el mayor ancho de
banda posible en el acceso a los discos.

==== Rendimiento del acceso a disco

En un disco duro magnético el *tiempo de acceso al disco*
stem:[T^d] viene determinado por el *tiempo de búsqueda*
stem:[T^b] y la *latencia rotacional* stem:[T^r]:

[stem]
++++
T_d=T_b+T_r
++++

_El tiempo de búsqueda stem:[T^b] es el tiempo que se tarda en
mover el brazo del disco hasta el cilindro deseado. Mientras que
la latencia rotacional stem:[T^r] es el tiempo que hay que
esperar para que el disco gire _asta que la cabeza llegue al sector
deseado_ del cilindro. Por lo tanto, el _tiempo de acceso al disco_
es menor cuando se realizan accesos consecutivos a sectores físicamente
próximos que cuando están dispersos por todo el disco.

_El *ancho de banda* o *tasa de transferencia* del disco
es el número total de bytes transferidos dividido por el tiempo
total que transcurre desde la primera solicitud de servicio a la
terminación de la última transferencia_ con la que se atiende la
petición. Al considerar todo el tiempo necesario para atender la
petición, a más _tiempo de acceso al disco_ menor es el _ancho de
banda_.

En los dispositivos de almacenamiento basados en memorias de estado
sólido (véase el <<_memorias_de_estado_sólido>>) el tiempo de acceso
viene determinado por las características de la memoria, entre otros
factores, lo que hace que las diferencias entre accesos secuenciales y
accesos aleatorios sean mucho menos significativas.

==== Cola de E/S al disco

Cuando se solicita una operación de E/S sobre el almacenamiento el
sistema operativo puede atender la petición sobre la marcha si la
controladora y la unidad de disco están disponibles. Pero si están
ocupadas, la solicitud se almacena en una cola de peticiones pendientes.
Cuando se resuelve una solicitud, el sistema operativo escoge otra de la
cola y se comunica con el hardware para programar la siguiente petición.
La cuestión es ¿cuál es el orden adecuado para escoger la peticiones de
E/S de la cola si se quiere acceder al disco de la forma más eficaz
posible?

==== Planificación FCFS

En la planificación *FCFS* (_First Come, First Served_) o _primero que llega, primero servido_ la cola es
FIFO. Es decir, _se atienden las solicitudes en orden de llegada_.
Es la planificación más simple y es equitativa —pues se atiende a
todos los procesos por igual— pero no proporciona el servicio
más rápido en disco duros magnéticos, donde interesa mover el brazo
del disco lo menos posible.

En los sistemas operativos Linux el FCFS es denominado NOOP y se suele
utilizar en los discos basados en memorias de estado sólido, donde
reordenar las solicitudes no proporciona una mejora significativa del
rendimiento, o cuando se utilizan controladoras de disco inteligentes
que pueden reordenar las solicitudes según su propio criterio.

==== Planificación SSTF

En la planificación *SSTF*
(_Sortest_ _Seek Time First_) o algoritmo de _tiempo de búsqueda
más corto_, de toda cola se selecciona la solicitud con el menor
_tiempo de búsqueda_ desde la posición actual de la cabeza. Como el
_tiempo de búsqueda_ se incrementa a medida que lo hace el número de
cilindros que es necesario recorrer, este algoritmo de planificación
primero da servicio a las solicitudes cercanas a la posición actual de
la cabeza, antes de alejarse para dar servicio a otras solicitudes. Aun
así, la solución no es óptima.

El problema de SSTF es que _puede provocar inanición de algunas
solicitudes_ si van llegando constantemente nuevas solicitudes sobre
regiones cercanas a donde está actualmente la cabeza del disco.

==== Planificación SCAN y C-SCAN

En la planificación *SCAN* o
algoritmo de _exploración_ o del _ascensor_ _el brazo del disco
comienza en un extremo del disco y se mueve hacia el otro atendiendo
solicitudes a medida que pasa por cada cilindro_, hasta llegar al
otro extremo del disco. En el otro extremo la dirección de movimiento de
la cabeza se invierte para recorrer el disco en sentido inverso,
repitiendo el proceso.

Suponiendo que las solicitudes se distribuyen de forma uniforme a lo
largo del disco, es de suponer que cuando se llega a un extremo, antes
de volver, la cantidad de solicitudes en dicho extremo será notablemente
menor que en el otro extremo del disco. Entonces ¿por qué no empezar por
el otro extremo?

_A la variante del SCAN que cuando llega a un extremo vuelve al
inicio, sin atender ninguna solicitud por el camino, para volver a
empezar se la denomina **C-SCAN**_. El resultado es que el tiempo
que tiene que esperar una solicitud para ser atendida es más uniforme
que con el algoritmo SCAN.

==== Planificación LOOK y C-LOOK

En teoría los algoritmos SCAN y C-SCAN hacen que el brazo recorra los
cilindros del primero al último. Sin embargo realmente no se suelen
implementar así. Por lo general, _cuando en el recorrido del brazo,
tras atender una solicitud, se descubre que ya no hay más solicitudes
siguiendo la misma dirección, el brazo invierte la dirección sin llegar
hasta el extremo del disco_. A estas variantes de SCAN y C-SCAN se las
denomina *LOOK* y *C-LOOK*, respecitvamente.

==== Planificación N-Step-SCAN, N-Step-LOOK y FSCAN

Los algoritmos _**N-Step-SCAN** y **N-Step-LOOK**_ son
variantes de los algoritmos SCAN y LOOK, respectivamente, donde _se
limita a N el número de solicitudes que se atenderán en cada barrido del
brazo del disco_. Estos algoritmos funcionan de la siguiente manera:

. Se utiliza una cola con espacio para _N_ solicitudes pendientes
que se van atendiendo mientras el brazo barre el disco.

. Mientras tanto, todas las nuevas solicitudes se incorporan a una
cola diferente.

. Cuando el brazo termina el barrido y las _N_ primeras solicitudes
han sido atendidas, el planificador toma otras _N_ solicitudes de la
segunda cola y las introduce en la primera para repetir el proceso.

Si en lugar de copiar _N_ peticiones de la segunda a la primera cola
se copian todas las solicitudes pendientes, el algoritmo se denomina
F-SCAN.

Estos algoritmos previenen un problema denominado _rigidez del brazo_
—_arm stickiness_, en inglés— a diferencia de los
algoritmos SSTF, SCAN, C-SCAN, LOOK y C-LOOK. El termino _rigidez del
brazo_ hace referencia a cuando hay un flujo continuo de solicitudes
para el mismo cilindro, lo que hace que con los algoritmos anteriores el
brazo no avance por los cilindros hasta llegar la otro extremo. Como
FSCAN, N-Step-SCAN y N-Step-LOOK separan las solicitudes en dos colas,
haciendo que las nuevas tengan que esperar, el brazo siempre continua su
barrido hacia el extremo del disco.

==== Planificación CFQ

_El planificador *CFQ* (Completely Fair Queuing) se diseñó
para compartir de forma equitativa el ancho de banda entre todos los
procesos que solicitan acceso al disco_. Es utilizado actualmente por
defecto en los sistemas Linux modernos y funciona de la siguiente
manera:

* CFQ mantiene una cola de solicitudes para cada proceso y en ella
inserta las solicitudes síncronas de E/S. Cada cola tiene una ventana de
tiempo —o _cuanto_— para acceder al disco. La longitud de
la ventana de tiempo y el tamaño máximo de cada cola dependen de la
prioridad de E/S que tenga el proceso.

* CFQ mantiene una cola de solicitudes por cada prioridad de E/S, donde
se insertan las solicitudes asíncronas de todos los procesos. Una
solicitud asíncronas se inserta en una cola u otra según la prioridad
del proceso que la generó.

* Usando el algoritmo _round-robin_, el planificador CFQ recorre las
colas y extrae de ellas las solicitudes durante el tiempo marcado por el
cuanto de cada una. Las solicitudes extraídas se insertan en la cola de
envío, donde se ordenar para minimizar el _tiempo de búsqueda_,
antes de ser enviadas _al_ dispositivo.

[bibliography]
== Bibliografía

La mayor parte de los contenidos de este documento están basados en las siguientes referencias bibliográficas:

* Silberschatz, A., Galvin, P. y Gagne, G. http://absysnetweb.bbtk.ull.es/cgi-bin/abnetopac?ACC=DOSEARCH&xsqf99=345629.titn./["`Fundamentos de Sistemas Operativos`"]. 7ª ed. McGraw Hill, 2005.
* Silberschatz, A., Galvin, P. y Gagne, G. http://absysnetweb.bbtk.ull.es/cgi-bin/abnetopac?ACC=DOSEARCH&xsqf99=184173.titn./["`Operating System Concepts with Java`"]. 6º ed. John Wiley & Sons Inc., 2004.

Otras referencias bibliográficas utilizadas fueron las siguientes:

* Bavier, A. "`Creating New CPU Schedulers with Virtual Time`". En 21st IEEE Real-Time Systems Symposium (RTSS 2000) WIP Proceedings, 2000.
* Friedman, M. B. "`Windows NT Page Replacement Policies`". En 25th International Computer Measurement Group Conference, December 5-10, 1999, Pag. 234-244.
* Ganger, G. R., McKusick, M. K., Soules, C. A. N. y Patt, Y. N. "`Soft Updates: A Solution to the Metadata Update Problem in File Systems`". En ACM Transactions on Computer Systems, Vol. 18, No. 2, May 2000, Pag. 127—153.
* Gorman, M. "`Understanding the Linux Virtual Memory Manager`". Prentice Hall, 2004.
* Hailperin, M. "`Operating Systems and Middleware: Supporting Controlled Interaction`". Course Technology, 2006.
* Jacob, B y Mudge, T. "Virtual Memory: Issues of Implementation". Computer, 31:33-43, 1998. ISSN 0018-9162. DOI: 10.1109/2.683005. URL http://dx.doi.org/10.1109/2.683005.
* "`Kernel Enhancements for Microsoft Windows Vista and Windows Server Longhorn`" [en línea]. Microsoft Corporation, 2005. URL http://goo.gl/ml8C4.
* "`Kernel Enhancements for Windows XP`" [en línea]. Microsoft Corporation,
2003 [2006]. URL http://goo.gl/ugED.
* "`XFS Filesystem Structure`" [en línea]. Silicon Graphics Inc, 2006 [2007]. URL https://goo.gl/YF82JB
* "`C dynamic memory allocation`" [en línea]. Wikipedia (en), [2011]. URL http://goo.gl/OkFJ3
* "`RAID`" [en línea]. Wikipedia (en), [2011]. URL http://goo.gl/GTQU
