== Gestión de procesos
include::attributes.adoc[]
:imagesdir: {imagesdir}/procesos

=== Procesos

Los primeros sistemas informáticos sólo permitían que un programa se ejecutara de cada vez.
Dicho programa tenía un control completo sobre el sistema y acceso a todos los recursos del mismo.
Por el contrario, los sistemas de tiempo compartido actuales permiten que múltiples programas sean cargados y ejecutados concurrentemente.
Obviamente esta evolución requiere un control más fino y la compartimentación de los diversos programas para que no interfieran unos con otros.
Esto a su vez conduce a la aparición de la noción de _proceso, que no es sino la unidad de trabajo en un sistema operativo moderno de tiempo compartido_.

Por simplicidad, en este tema utilizaremos los términos trabajo y proceso de forma indistinta.
A fin de cuentas tanto los _trabajos_ en los sistemas de procesamiento por lotes como los _procesos_ en los sistemas de tiempo compartido son la unidad de trabajo en sus respectivos sistemas y el origen de toda actividad en la CPU.

Por último, antes de continuar, no debemos olvidar que en un sistema operativo hay:

* _Procesos del sistema_ ejecutando el código del sistema operativo contenido en los _programas del sistema_, que generalmente realizan tareas que es mejor mantener fuera del núcleo.

* _Procesos de usuario_ ejecutando código de usuario contenido en los _programas de aplicación_.

Sin embargo en lo que resta de tema no estableceremos ningún tipo de distinción entre ellos.
Al fin y al cabo todos son simples procesos de cara al resto del sistema.

==== El proceso

Como ya hemos comentado con anterioridad, _un *proceso* es un programa en ejecución_.
Sin embargo los procesos no son sólo el código del programa, sino que _también suelen contener algunos_ otros elementos:

* *El código del programa*, conocido como la sección `text`.

* *La sección de datos* contiene las variables globales.
Se divide entre la sección `data`, donde se almacenan las variables inicializadas, y la sección `bss`, donde se almacenan las variables sin inicializar.

* *La pila* contiene datos temporales como parámetros y direcciones de retorno de las funciones y variables locales.
Es conocida como la sección `stack`.

* *El montón*, que es donde se aloja la memoria que se asigna dinámicamente durante la ejecución del proceso.

* *Información de la actividad actual*, como el _contador de programa_, los _registros de la CPU_, etc.

En la se puede observar la disposición de algunos de estos elementos en la memoria.

En todo caso es importante recordar que _un *proceso* es una entidad activa, con un contador de programa especificando la próxima instrucción a ejecutar y un conjunto de recursos del sistema asociados_.
Mientras que _un *programa* no es un proceso ya que es una entidad pasiva, como un archivo en disco que contiene el código que algún día será ejecutado en la CPU_.
Por lo tanto dos procesos pueden estar asociados al mismo programa pero no por eso dejan de ser distintos procesos (véase el apartado <<_componentes_del_sistema>>).
Ambos tendrán la misma sección _text_ pero el contador de programas, la pila, la sección _data_, etc.
contendrán valores diferentes.

==== Estados de los procesos

Los procesos tienen un *estado* que cambia a lo largo de su ejecución y está definido parcialmente por la actividad actual del propio proceso.
Los estados por los que puede pasar un procesos varían de un sistema operativo a otro, aunque los siguientes son comunes a todos ellos:

* *Nuevo*.
El proceso está siendo creado.

* *Ejecutando*.
El proceso está siendo ejecutado puesto que ha sido escogido por el planificador de la CPU.
_Sólo puede haber un proceso en este estado por CPU en el sistema_.

* *Esperando*.
El proceso está esperando por algún _evento_, como por ejemplo que termine alguna operación de E/S o que se reciba alguna señal.
Obviamente varios procesos pueden estar en este estado.

* *Preparado*.
El proceso está esperando a que se le asigne la CPU.
Varios procesos pueden estar en este estado.

* *Terminado*.
El proceso ha finalizado su ejecución y espera a que se liberen los recursos que le fueron asignados.

El diagrama de estado de los procesos se muestra en la .

==== Bloque de control de proceso

_El *bloque de control de proceso* o *PCB* (Process Control Block) es una estructura de datos que representa a cada proceso en el sistema operativo_.
Sirve de almacén para cualquier información que puede variar de un proceso a otro.

* *Estado del proceso*.
Por ejemplo: nuevo, preparado, esperando, etc.

* *Contador de programa*.
Indica la dirección de la próxima instrucción del proceso que debe ser ejecutada por la CPU.

* *Registros de la CPU*.

* *Información de planificación de la CPU*.
Incluye la información requerida por el planificador de la CPU.
Por ejemplo la prioridad del proceso, punteros a las colas de planificación donde está el proceso, punteros al PCB del proceso padre y de los procesos hijos, etc.

* *Información de gestión de la memoria*.
Incluye la información requerida para la gestión de la memoria.
Por ejemplo los valores de los registros base y límite que definen el área de la memoria física que ocupa el proceso, la tabla de páginas —en el caso de que se use paginación (véase el <<_paginación>>)— o la tabla de segmentos —en el caso de que se utilice segmentación— etc.

* *Información de registro*.
Esta información incluye la cantidad de CPU usada, límites de tiempo en el uso de la CPU, estadísticas de la cuenta del usuario al que pertenece el proceso, estadísticas de la ejecución del proceso, etc.

* *Información de estado de la E/S*.
Incluye la lista de dispositivos de E/S reservados por el proceso, la lista de archivos abiertos, etc.

==== Colas de planificación

En los sistemas operativos hay diferentes *colas de planificación* para los procesos en distintos *estados*.

* *Cola de trabajo*.
_Contiene a todos los procesos en el sistema_ de manera que cuando un proceso entra en el sistema va a esta cola.

* *Cola de preparados*.
_Contiene a los procesos que están cargados en la memoria principal y están preparados para ser ejecutados_.
La cola de preparados es generalmente una lista enlazada de PCB donde cada uno incluye un puntero al PCB del siguiente proceso en la cola.

* *Colas de espera*.
_Contienen a los procesos que están esperando por un evento concreto_, como por ejemplo la finalización de una solicitud de E/S.
Estas colas también suelen ser implementadas como listas enlazadas de PCB y suele existir una por evento, de manera que cuando ocurre algún evento todos los procesos en la cola asociada pasan automáticamente a la cola de preparados.

* *Colas de dispositivo*.
Son un caso particular de cola de espera.
Cada dispositivo de E/S tiene asociada una cola de dispositivo que contiene los procesos que están _esperando_ por ese dispositivo en particular.

Una manera habitual de representar la planificación de procesos es a través de un diagrama de colas como el de la .
Analizándolo podemos tener una idea clara del flujo típico de los procesos dentro del sistema:

. _Un nuevo proceso llega al sistema_.
El proceso es colocado en la _cola de preparados_.
Allí espera hasta que es seleccionado para su ejecución y se le asigna la CPU.
Mientras se ejecuta pueden ocurrir varias cosas:

    ** _El proceso puede solicitar una operación de E/S_ por lo que abandona la CPU y es colocado en la _cola de dispositivo_ correspondiente.
    No debemos olvidar que aunque en nuestro diagrama no exista más que una de estas colas, en un sistema operativo real suele haber una para cada dispositivo.

    ** _El proceso puede querer esperar por un evento_.
    Por ejemplo puede crear un subproceso y esperar a que termine.
En ese caso el proceso hijo es creado mientras el proceso padre abandona la CPU y es colocado en una _cola de espera_ hasta que el proceso hijo termine.
    La terminación del proceso hijo es el evento que espera el proceso padre para continuar su ejecución.

    ** _El proceso puede ser sacado forzosamente de la CPU_ —como resultado de la interrupción del temporizador que indica que lleva demasiado tiempo ejecutándose— y colocado en la _cola de preparados_.

. _Cuando la espera concluye_ los procesos pasan del _estado de espera al de preparado_ y son insertados en la _cola de preparados_.

. _El proceso repite este ciclo hasta que termina_.
En ese momento es eliminado de todas las colas mientras el PCB y los recursos asignados son liberados.

==== Planificación de procesos

Durante su ejecución, los procesos se mueven entre las diversas colas de planificación a criterio del sistema operativo como parte de la tarea de planificación.
Este proceso de selección debe ser realizado por el *planificador* adecuado:

* __En los sistemas de multiprogramados__footnote:[Los sistemas de tiempo compartido como GNU/Linux, Microsoft Windows o cualquier sabor de UNIX carecen de planificador de trabajos.
En estos sistemas simplemente se cargan los procesos en memoria para que sean ejecutados cuando el usuario lo solicita.] el *planificador de largo plazo* —o *planificador de trabajos*— selecciona los trabajos desde la cola de entrada en el almacenamiento secundario, dónde están todos almacenados, y los carga en memoria.

* _El *planificador de corto plazo* o *planificador de CPU* selecciona uno de los procesos en la cola de preparados y lo asigna a la CPU_.
Obviamente este planificador es invocado cuando un proceso en ejecución abandona la CPU por cualquiera de los motivos comentados.

* _Algunos sistemas operativos utilizan el *planificador de medio plazo* para sacar procesos de la memoria y reintroducirlos posteriormente_.
A este esquema se le denomina *intercambio* —o _swapping_— y puede ser necesario utilizarlo cuando escasea la memoria.

==== Cambio de contexto

_El *cambio de contexto* es la tarea de asignar a la CPU un proceso distinto al que la tiene asignada en el momento actual_.
Esto implica salvar el estado del viejo proceso en su PCB y cargar en la CPU el estado del nuevo.
Entre la información que debe ser preservada se incluye:

* Los *registros de la CPU*.

* El *estado del proceso*.

* La *información de gestión de la memoria*.
Por ejemplo la información referente al espacio de direcciones del proceso.

El cambio de contexto es sobrecarga pura puesto que no hace ningún trabajo útil mientras se conmuta.
Su velocidad depende de aspectos tales como: el número de registros, la velocidad de la memoria y la existencia de instrucciones especialesfootnote:[Algunas CPU disponen de instrucciones especiales para salvar y cargar todos los registros de manera eficiente.
Esto reduce el tiempo que la CPU está ocupada en los cambios de contexto.
Otra opción es el uso de juegos de registros, como es el caso de los procesadores Sun UltraSPARC e Intel Itanium.
Con ellos el juegos de registros de la CPU puede ser mapeado sobre un banco de registros mucho más extenso.
Esto permite que la CPU almacene de forma eficiente el valor de los registros de más de un proceso.].

==== Operaciones sobre los procesos

En general los procesos pueden ser creados y eliminados dinámicamente, por lo que los sistemas operativos deben proporcionar mecanismos para la creación y terminación de los mismos.

===== Creación de procesos

_Un proceso —denominado *padre*— puede crear múltiples procesos —los *hijos*— utilizando una llamada al sistema específica para la creación de procesos_.
En general cada proceso se identifica de manera unívoca mediante un *identificador de proceso* o *PID* (_Process Identifier_), que normalmente es un número entero.
Puesto que cada nuevo proceso puede a su vez crear otros procesos, al final se acaba obteniendo un **árbol de procesos**footnote:[En los sistemas UNIX el proceso _init_ es el proceso padre raíz de todos los procesos de usuario.
Su PID siempre es 1 ya que es el primer proceso creado por el sistema operativo al terminar la inicialización del núcleo.
Por lo tanto es el responsable de crear todos los otros procesos que son necesarios para el funcionamiento del sistema.].

Hay varios aspectos en la creación de los procesos que pueden variar de un sistema operativo a otro:

* *¿Cómo obtienen los subprocesos los recursos que necesita para hacer su trabajo?*

    ** _En algunos sistemas operativos los subprocesos sólo puede aspirar a obtener un subconjunto de los recursos de su padre_.
Esto permite evitar, por ejemplo, que un proceso pueda sobrecargar el sistema creando demasiados procesos.

    ** Mientras que _en otros cada subproceso puede solicitar y obtener los recursos directamente del sistema operativo_.

* *¿Qué ocurre con los recursos de un proceso cuando decide crear subprocesos?*

    ** _El proceso puede estar obligado a repartir sus recursos entre sus hijos_.

    ** _O puede que esté en disposición de compartir algunos recursos_ —como memoria y archivos— _con algunos de sus hijos_.

Por ejemplo en POSIX todos los archivos abiertos por un proceso son _heredados_ en ese estado por sus hijos.

* *¿Cómo un proceso puede pasar parámetros de inicialización a sus procesos hijo?* Además de los diversos recursos que un proceso obtiene cuando es creado, el proceso padre suele poder pasar parámetros de inicialización a sus procesos hijo.
Por ejemplo en C y {cpp} se puede obtener acceso a estos parámetros través de los argumentos `argc` y `argv` de la función `main()` del programa.

* *¿Qué ocurre con la ejecución de un proceso cuando crea un subproceso?* Si eso ocurre se suelen contemplar dos posibilidades en términos de la ejecución del padre: ** _El padre continúa ejecutándose concurrentemente con el hijo_.

    ** _El padre decide esperar a que algunos o todos sus hijos terminen_.  

* *¿Cómo se construye el espacio de dirección de los subprocesos?* En general hay dos posibilidades: 

    ** _El espacio de direcciones del proceso hijo es un duplicado del que tiene el padre_.
Es decir, que inicialmente tiene el mismo código y datos que el padre.

    ** _El espacio de direcciones del proceso hijo se crea desde cero y se carga en él un nuevo programa_.

Para ilustrar la diferencia entre estos dos últimos casos, supondremos que tenemos un proceso que quiere crear un subproceso.
En los sistemas operativos POSIX un nuevo proceso siempre se crea con la llamada `fork()` que se encargar de crear el nuevo proceso con una copia del espacio de direcciones del proceso original (véase la ).
Esto facilita la comunicación entre procesos puesto que al copiarse el espacio de direcciones también se copia la tabla de archivos abiertos.
No debemos olvidar que muchos de los recursos de un sistema POSIX se muestran de cara a los procesos que los utilizan como archivos.
Por lo tanto, el proceso hijo no sólo tiene acceso a los archivos abiertos por el padre antes de la llamada al `fork()` sino también a tuberías (véase el apartado <<_ejemplos_de_mecanismos_comunicación_entre_procesos>>), sockets (véase el apartado <<_ejemplos_de_mecanismos_comunicación_entre_procesos>>) y regiones de memoria compartida (véase el apartado <<_memoria_compartida>>), entre otros muchos recursos.
Todos estos son mecanismos que los procesos pueden utilizar para comunicarse.

Tanto padre como hijo continúan la ejecución en la siguiente instrucción después del `fork()`.
La diferencia es que para el padre el valor de retorno de la llamada `fork()` es el identificador de proceso del hijo, mientras que para el hijo el valor de retorno es cero.
De esa forma padre e hijo pueden saber quién es cada uno (véase la ).

En muchas ocasiones lo que se desea es iniciar la ejecución de un nuevo programa.
Como POSIX no dispone de una llamada al sistema para dicha tarea, lo que se debe hacer es invocar la llamada `exec()` en el hijo después del `fork()`.
La llamada `exec()` carga un nuevo archivo ejecutable en el espacio de direcciones del proceso actual e inicia su ejecución, destruyendo la imagen del programa que realizó la llamada a `exec()`.
Mientras tanto el padre puede crear más hijos o si no tiene nada más que hacer puede esperar.
Para ello utiliza la llamada `wait()` que mueve el proceso a una cola de espera hasta que termine el hijo creado previamente (véase la ).

Respecto a la familia de sistemas operativos Microsoft Windows NT, se supone que ambos modelos son soportados a través de la llamada al sistema `NTCreateProcess()`.
Es decir, que el espacio de direcciones del padre puede ser duplicado o se puede indicar el nombre de un programa para que el sistema operativo lo cargue en un nuevo proceso.
Sin embargo el hecho es que los programadores no tienen acceso directo a `NTCreateProcess()` ya que se supone que debe ser utilizada a través de la función `CreateProcess()` de la librería del sistema de la API Win32.
Dicha función no duplica el espacio de direcciones del padre, sino que simplemente carga en un nuevo proceso el programa indicado en los argumentosfootnote:[Esta decisión de diseño en el API principal de Microsoft Windows estuvo motivada por el hecho de que la creación de nuevos procesos tiene un coste alto en ese sistema operativo.
En Microsoft Windows la forma adecuada de realizar varias tareas al mismo tiempo es utilizando hilos (véase el <<_hilos>>).].

===== Terminación de procesos

Un proceso termina cuando se lo indica al sistema operativo con la llamada al sistema `exit()`.
En ese momento puede devolver un valor de estado a su padre, que este puede recuperar a través de la llamada al sistema `wait()`.
Cuando un proceso termina todos los recursos son liberados incluyendo: la memoria física y virtual, archivos abiertos, búferes de E/S, etc.

En todo caso un proceso puede provocar la terminación de otro proceso a través de una llamada al sistema.
Habitualmente el proceso que la invoca es el padre ya que puede que sea el único con permisos para hacerla.
Los motivos pueden ser:

* _El hijo ha excedido el uso de algunos de los recursos reservados_.
Obviamente esto tiene sentido cuando los hijos utilizan un subconjunto de los recursos asignados al padre.

* _La tarea asignada al hijo ya no es necesaria_.

* _El padre termina y el sistema operativo está diseñado para no permite que el hijo pueda seguir ejecutándose si no tiene un padre._ Esto provoca que el sistema operativo inicie lo que se denomina una **terminación en cascada**footnote:[En UNIX si un proceso muere a su hijo se le reasigna como padre el proceso _*init*_ —con PID 1—.
Al menos en GNU/Linux, los intentos de matar a este último son ignorados por el sistema.], donde todos los procesos que cuelgan de uno dado terminan.

=== Procesos cooperativos

Desde el punto de vista de la cooperación podemos clasificar los procesos en dos grupos:

* _Los *procesos independientes*, que no afectan o pueden ser afectados por otros procesos del sistema._ Cualquier proceso que no comparte datos —temporales o persistentes— con otros procesos es independiente.

* _Los *procesos cooperativos*, que pueden afectar o ser afectados por otros procesos ejecutados en el sistema_.
Los procesos que comparten datos siempre son cooperativos.

==== Motivaciones para la colaboración entre procesos

Hay diversos motivos para proporcionar un entorno que permita la cooperación de los procesos:

* *Compartición de información*.
Dado que varios usuarios pueden estar interesados en los mismos bloques de información —por ejemplo en un archivo compartido— el sistema operativo debe proporcionar un entorno que permita el acceso concurrente a este tipo de recursos.

* *Velocidad de cómputo*.
Para que una tarea se ejecute más rápido se puede partir en subtareas que se ejecuten en paralelo.
Es importante destacar que la mejora en la velocidad sólo es posible si el sistema tiene varios componentes de procesamiento —como procesadores o canales E/S—.

* *Modularidad*.
Podemos querer crear nuestro software de forma modular, dividiendo las funciones del programa en procesos separados.

* *Conveniencia*.
Incluso un usuario individual puede querer hacer varias tareas al mismo tiempo.
Por ejemplo editar, imprimir y compilar en paralelo.

Las ejecución concurrente de procesos cooperativos requiere mecanismos tanto para _comunicar unos con otros_ (véase los apartados <<_memoria_compartida>> y <<_microkernel>>) como para _sincronizar sus acciones_ (véase el <<_sincronización>>).

==== Memoria compartida

_La *memoria compartida* es una estrategia para comunicar procesos dónde uno de ellos gana acceso a regiones de la memoria del otro_ (véase la ).
Puesto que normalmente el sistema operativo intenta evitar que un proceso pueda tener acceso a la memoria de otro proceso (véase el apartado <<_protección_de_la_memoria>>), para que pueda haber memoria compartida es necesario que los dos procesos estén de acuerdo en eliminar dicha restricción.

Dos procesos que comparten una región de la memoria pueden intercambiar información leyendo y escribiendo datos en la misma, pero:

* _La estructura de los datos y su localización dentro de la región compartida la determinan los procesos en comunicación_ y no el sistema operativo.

* _Los procesos son responsables de sincronizarse_ (véase el apartado <<_sincronización>>) para no escribir en el mismo sitio al mismo tiempo, lo que que podría generar inconsistencias.

Las principales ventajas de la memoria compartida frente a otros mecanismos de comunicación son:

* _**Eficiencia**, puesto que la comunicación tiene lugar a la velocidad de la memoria principal._

* _**Conveniencia**, puesto que el mecanismo de comunicación sólo requiere leer y escribir de la memoria_.

==== Comunicación entre procesos

_La *comunicación entre procesos* o *IPC* (Interprocess Communication) es un mecanismo para que los procesos puedan compartir información y sincronizar sus acciones sin necesidad de compartir el espacio de direcciones_.
Este mecanismo debe ser proporcionado por el sistema operativo que, a diferencia de cuando se usa memoria compartida, se encarga de la sincronización y así como de establecer el formato que deben tener los datos.
Es particularmente útil en entornos distribuidos dónde los procesos a comunicar residen en ordenadores diferentes conectados a una red.
Por ejemplo se utiliza para comunicar un navegador y un servidor Web en Internet.

La mejor forma de proporcionar IPC es utilizando un sistema de paso de mensajes (véase la ).

===== Sistema de paso de mensajes

_La función de un *sistema de paso de mensajes* es permitir que los procesos se comuniquen sin necesidad de recurrir a la compartición de recursos_ —compartir memoria, archivos, etc.—.

El componente de IPC de cualquier sistema operativo debe proporcionar al menos dos llamadas al sistema similares a:

* _send (message)_ para mandar mensajes a otro proceso.
* _receive (message)_ para recibir mensajes de otro proceso.

Además los diseñadores del sistema operativo deben escoger entre implementar un componentes de IPC con _mensajes de tamaño fijo_ o _mensajes de tamaño variable_.

* *Mensajes de tamaño fijo*.
La implementación del sistema operativo es sencilla pero la programación de aplicaciones es mucho más compleja.

* *Mensajes de tamaño variable*.
La implementación del sistema operativo es más compleja pero la programación de aplicaciones es más simple.

Para que dos procesos se puedan comunicar es necesario que haya un _enlace de comunicaciones_.
No trataremos aquí la implementación física del enlace —que por ejemplo puede ser mediante memoria compartida, un bus hardware, o una red de comunicaciones— sino de su implementación lógica.

En general existen varias opciones a la hora de implementar de manera lógica un enlace y las correspondientes operaciones de envío y recepción:

* _Comunicación directa o indirecta_.
* _Comunicación síncrona o asíncrona_.
* _Buffering explícito o automático_.

===== Referenciación

Los procesos que se quiera comunicar debe tener una forma de referenciarse el uno al otro.
Para ello puede utilizar la comunicación directa o la indirecta.

====== Comunicación directa

En _la *comunicación directa* cada proceso debe nombrar explícitamente al proceso destinatario o receptor de la información_.
Por ejemplo:

* _send (P, message)_ para mandar un mensaje al proceso _P_.
* _receive (Q, message)_ para recibir un mensaje del proceso _Q_.

// TODO: Poner nota de que estas llamadas al sistema no existen.

El esquema anterior se de nomina *direccionamiento simétrico* pero existe una variante de ese mismo esquema denominado *direccionamiento asimétrico*.

* _En el *direccionamiento simétrico* tanto el proceso transmisor como el receptor tienen que nombrar al otro para comunicarse_.

* _En el *direccionamiento asimétrico* sólo el transmisor nombra al receptor, mientras que el receptor no tiene que nombrar al transmisor_.

    ** _send (P, message)_ para mandar un mensaje al proceso _P_.

    ** _receive (&id, message)_ para recibir un mensaje de cualquier proceso.
En este caso el sistema operativo asigna a la variable _id_ el identificador del proceso transmisor del mensaje antes de volver de la llamada al sistema.

La principal desventaja de este tipo de comunicación es que _cambiar el identificador de un proceso requiere actualizar todas las referencias al anterior identificador en todos los procesos que se comunican con el_.
En general cualquier técnica que requiera que los identificadores de los procesos sean establecidos explícitamente en el código de los programas no es deseable.
Esto es así porque en muchos sistemas los identificadores de los procesos cambian de una ejecución a otra.
Por lo tanto lo mejor sería disponer de una solución con un nivel adicional de indirección que evite que los identificadores tenga que ser usados explícitamente

====== Comunicación indirecta

En _la *comunicación indirecta* los mensajes son enviados a *buzones*, *maillox* o *puertos* que son objetos dónde los procesos pueden dejar y recoger mensajes_.

* _send (A, message)_ para mandar un mensaje al puerto _A_.
* _receive (A, message)_ para recibir un mensaje del puerto _A_.

Este tipo de comunicación da lugar a algunas situaciones que deben ser resueltas.
Por ejemplo, *¿qué pasa si los procesos P, Q y R comparten el puerto A, P manda un mensaje, y Q y R invocan la llamada receive() en A?*.
La respuesta correcta dependerá de cuál de los siguientes métodos escogieron los diseñadores del sistema:

* _No permitir que cada enlace esté asociado a más de dos procesos_.

* _No permitir que más de un proceso puedan ejecutar receive() al mismo tiempo_.
Por ejemplo, haciendo que sólo el proceso que crea el puerto tenga permiso para recibir de él.
Los sistemas que optan por esta solución suelen disponer de algún mecanismos para transferir el permiso de recibir a otros procesos.

* _O permitir que el sistema operativo escoja arbitrariamente quién recibe el mensaje_ si dos o más procesos ejecutan `receive()` al mismo tiempo.
La elección puede ser aleatoria o mediante algún algoritmo, por ejemplo por turnos.

===== Buffering

Los mensajes intercambiados durante el proceso de comunicación residen en una cola temporal.
Básicamente hay tres formas de implementar dicha cola:

* _Con *capacidad cero* o *sin buffering* la cola tiene una capacidad máxima de 0 mensajes_, por lo tanto no puede haber ningún mensaje esperando en el enlace.
En este caso el transmisor debe bloquearse hasta que el receptor recibe el mensaje.

* _Con **buffering automático**_:

    ** _Con *capacidad limitada* la cola tiene una capacidad limitada a N mensaje_, por que si la cola no se llena el transmisor no espera.
Sin embargo si la cola se llena, el transmisor debe bloquearse a la espera de haya espacio en la cola.

    ** _Con *capacidad ilimitada* la cola es de longitud potencialmentefootnote:[Las colas de longitud real infinita son imposibles puesto que los recursos son limitados.
La longitud de estas colas viene determinada por la memoria principal disponible, que suele ser lo suficientemente grande para que podamos considerar que las colas son infinitas.] infinita_, lo que permite que el transmisor nunca espere.

===== Sincronización

La comunicación entre dos procesos tiene lugar por medio de las llamadas `send()` y `receive()`; de tal forma que generalmente la primera se bloquea cuando la cola de transmisión se llena —en función del tipo de buffering— mientras que la segunda lo hace cuando la cola de recepción está vacía.

Sin embargo existen diferentes opciones de diseño a la hora de implementar cada una de estas primitivas en función de si se pueden bloquear o no.
Por tanto, el paso de mensajes puede ser *con bloqueo* o *sin bloqueo*, o lo que es lo mismo *síncrono* u *asíncrono*.

* _Cuando el envío es sin bloqueo,_ el proceso transmisor nunca se bloquea.
En caso de que la cola de mensaje esté llena, la solución más común es que la llamada `send()` vuelva con un código de retorno que indique que el proceso debe volver a intentar el envío más tarde.

* _Cuando el envío es con bloqueo_, el proceso transmisor se bloquea cuando no queda espacio en la cola de mensajes y hasta que pueda depositar el mensaje en la misma.

* _Cuando la recepción es sin bloqueo_, el proceso receptor nunca se bloquea.
En caso de que la cola de mensajes esté vacía, el sistema operativo puede indicar al proceso que lo intente más tarde a través de un código de retorno o devolviendo un mensaje nulo.

* _Cuando la recepción es con bloqueo_, el receptor se bloquea cuando no hay mensajes en la cola.

Diferentes combinaciones de `send()` y `receive()` son posibles.
Es decir, transmisión y recepción pueden ser síncronas o asíncronas de manera independiente.

==== Ejemplos de mecanismos comunicación entre procesos

===== Tuberías

Las *tuberías* son un mecanismo de IPC de _comunicación indirecta_ que está incluido en muchos sistemas operativos.
La comunicación es de _capacidad cero_ y _síncrona,_ aunque en algunos sistema operativos también puede ser _asíncrona_.

Conceptualmente cada tubería tiene dos extremos.
Un extremo permite al proceso en ese extremo escribir en la tubería, mientras el otro extremo permite a los procesos leer de la tubería.

Existen dos tipos de tuberías:

* _Las *tuberías anónimas* sólo existen en el espacio de direcciones del proceso que las crea_.

    ** Los procesos hijo pueden heredar las tuberías abiertas por el proceso padre.
Usando esa capacidad de herencia _se puede comunicar un proceso padre con sus hijos de manera privada_ (véase la ).

* _Las *tuberías con nombre* son públicas al resto del sistema_, por lo que teóricamente son accesibles por cualquier proceso.

    ** Se suelen utilizar en aplicaciones _cliente — servidor_ dónde un proceso servidor ofrece algún servicio a otros procesos cliente a través de la tubería.

    ** En POSIX se denominan _FIFO_ y tienen presencia en el sistema de archivos como archivos especiales.

** En Windows las tuberías con nombre son bidireccionales.

Por simplicidad las tuberías son tratadas de forma similar a los archivos por lo que en ambos casos se utilizar las mismas primitivas POSIX de E/S —`read()` y `write()`—.

// TODO: Nota sobre el caso de Windows y Linux.

===== Señales en sistemas operativos POSIX

En POSIX la forma más sencilla de comunicar dos procesos del mismo sistema es mediante el envío de una *señal* de uno al otro.

Los procesos pueden mandar señales utilizando la llamada al sistema `kill()`, que sólo requiere el identificador del proceso de destino y el número de la señal.
Por tanto, _estamos hablando de un mecanismo de comunicación directa_.
Cada señal tiene un efecto particular por defecto —que por lo general es matar al proceso— en el proceso que las recibe.
Sin embargo cada proceso puede declarar un _manejador de señales_ que redefina la acción por defecto para una señal determinada.
Un manejador de señales no es más que una función que es ejecutada asíncronamente cuando la señal es recibida.
En ese sentido _las *señales* en POSIX puede interpretarse como una forma de interrupción por software_.

_Las señales fueron diseñadas originalmente como un mecanismo para que el sistema operativo notificara a los programas ciertos errores y sucesos críticos_, no como un mecanismo de IPC.
Por ejemplo:

* La señal `HUP` o `SIGHUP` es enviada a cada proceso iniciado desde una sesión de terminal cuando dicha sesión termina.

* La señal `INT` o `SIGINT` es enviada al proceso que está enganchado a la consola cuando el usuario pulsa el carácter de interrupción —frecuentemente la combinación de teclas kbd:[CTRL+C]—.

// TODO: Poner el ejemplo de SIGUP.

Sin embargo esto no evita que las señales puedan ser útiles como mecanismo de IPC.
No en vano el estándar POSIX incluye dos señales —`USR1` y `USR2`— especialmente indicadas para este uso.
Además las señales son utilizadas frecuentemente como medio de control de los __demonios__footnote:[Un demonio es un proceso no interactivo que se ejecuta en segundo plano en vez de ser controlado directamente por el usuario.
Este tipo de programas se ejecutan de forma continua y proporcionan servicios específicos, como por ejemplo es el caso de los servidores de correo electrónico, servidores de páginas Web o de bases de datos.] del sistema.
Por ejemplo permiten que un administrador —u otro proceso— le indique a un demonio que debe reinicializarse, empezar a realizar el trabajo para el que fue diseñado o escribir su estado interno en un sitio conocido del almacenamiento.

===== Sockets

_Un *socket* es un punto final en una comunicación bidireccional entre procesos_.
Para que una pareja de procesos se pueda comunicar son necesarios dos _sockets_ —uno para cada proceso— de manera que cada uno de ellos representa un extremo de la conexión.

La API de _sockets_ fue creada por la Universidad de Berkeley para ser la que abstrajera el acceso a la familia de protocolos de Internet (TCP/IP) en el UNIX desarrollado por esa misma universidad.
Sin embargo rápidamente se convirtió en el estándar de facto para la comunicación en red, por lo que todos los sistemas operativos modernos —incluidos los sistemas POSIX y Microsoft Windows— tienen una implementación de la misma.

Pese a sus orígenes, _los sockets se diseñaron para ser independientes de la tecnología de red subyacente_.
Por ejemplo:

* En las redes TCP/IP para crear un _socket_ es necesario indicar la dirección IP y el número de puerto en el que debe de escuchar o desde el que se debe conectar a otro _socket_.
Mientras que en el momento de establecer una conexión con ese otro _socket_, se debe indicar la dirección IP y el número de puerto donde el _socket_ debe estar escuchando.
Esto es así porque la tecnología de red TCP/IP subyacente establece que cada máquina tiene una IP y que los procesos se comunican a través de los puertos en las mismas.

* En los sistemas POSIX es habitual el uso de _sockets de dominio UNIX_ para comunicar procesos dentro de un mismo sistema.
Estos no son más que _sockets_ locales identificados mediante un nombre de archivo y que, por tanto, están representados en el sistema de archivos.
Su principal utilidad están en las aplicaciones que siguen el modelo cliente-servidor pero donde no es interesante —o seguro— que el servicio esté disponible a través de la red.
Por ejemplo se suelen utilizar para conectar gestores de bases de datos con aplicaciones Web servidas desde el mismo equipo.

Si comparamos los ejemplos anteriores, podemos observar que existen grandes diferencias en cuanto a la tecnología de comunicación empleada cuando se trata de comunicar procesos en redes TCP/IP o en un mismo equipo mediante _sockets_ de dominio UNIX.
Sin embargo para ambos casos la API de _sockets_ siempre es la misma.

Los _sockets_ implementan _buffering automático_ y admiten tanto _comunicación síncrona_ como _asíncrona_, aunque el comportamiento final de la interfaz depende de la tecnología de red utilizada.

=== Hilos

Hasta el momento el modelo de proceso que hemos descrito asume que tenemos un sólo *hilo* de ejecución, es decir, que se ejecuta en la CPU una única secuencia de instrucciones.
Un proceso con un hilo de ejecución sólo puede realizar una tarea a la vez.
Por ejemplo, en un procesador de textos con un sólo hilo de ejecución el usuario nunca podría escribir al mismo tiempo que se comprueba la ortografía.
Por eso muchos sistemas operativos modernos han extendido el concepto de proceso para permitir múltiples hilos de ejecución en cada uno.
Los procesos con varios hilos pueden realizar varias tareas a la vez.

==== Introducción

_El hilo es la unidad básica de uso de la CPU en los sistemas operativos multihilo_.
De los recursos de un proceso es privado a cada hilo (véase la ):

* *El identificador del hilo* lo identifica en el sistema de la misma manera que lo hace el identificador de proceso con el proceso.

* *El contador de programa* indica la dirección de la próxima instrucción del proceso que debe ser ejecutada por la CPU.

* *Los registros de la CPU*.

* *La pila* contiene datos temporales como parámetros y direcciones de retorno de las funciones y variables locales.

Sin embargo todos los hilos de un mismo proceso comparten (véase la ):

* *El código del programa*.

* *Otras secciones de datos*, como el montón.

* Y *otros recursos del proceso* como archivos abiertos y señales.

===== Beneficios

Muchos son los beneficios que aporta la programación multihilo:

* *Respuesta*.
Una aplicación multihilo interactiva puede continuar ejecutándose aunque parte de la misma esté bloqueada o realizando una operación lenta, mejorando la _respuesta al usuario_ de la misma.
Por ejemplo, un navegador Web multihilo puede gestionar la interacción del usuario a través de un hilo mientras el contenido solicitado se descarga en otro hilo.

* *Compartición de recursos*.
Por defecto los hilos comparten la memoria y los recursos del proceso al que pertenecen.
El compartir el código es lo que permite a una aplicación tener varios hilos que realizan diferentes actividades dentro del mismo espacio de direcciones.

* *Economía*.
Reservar memoria y otros recursos para la creación de un proceso es costoso.
Puesto que los hilos comparten los recursos de los procesos a los que pertenecen es más económico crearlos.
También es más económico el cambio de contexto entre ellos ya que hay que guardar y recuperar menos información.
Por ejemplo en Oracle/Sun Microsystems Solaris crear un proceso es 30 veces más lento que crear un hilo; y el cambio de contexto es 5 veces más lento.

// Estadísticas más modernas ¿Linux? ¿Windows?

* *Aprovechamiento de las arquitecturas multiprocesador*.
En esas arquitecturas diferentes hilos pueden ejecutarse en paralelo en distintos procesadores.
Por el contrario un proceso monohilo sólo se puede ejecutar en una CPU a la vez, independientemente de cuantas estén disponibles para ejecutarlo.

===== Soporte multihilo

_Las *librerías de hilos* proporcionan al programador la API para crear y gestionar los hilos de su proceso_.
Hay dos formas fundamentales de implementar una librería de hilos:

* La primera forma es _implementar la librería enteramente en el espacio de usuario, sin requerir el soporte del núcleo_:

    ** Los hilos así gestionados no existen para el núcleo.
Sólo existen en el espacio de usuario dentro del proceso que los ha creado.
    Por ese motivo se los denomina *hilos de usuario*.

    ** El código y los datos de la librería residen en el espacio de usuario, por lo que invocar una función de la misma se reduce a una simple llamada a una función, evitando el coste de hacer llamadas al sistema.

* La segunda forma es _implementar la librería en el núcleo_.

    ** Los hilos así gestionados son soportados y gestionados por el núcleo, quien se encarga de planificarlos en la CPU.
Por ese motivo se los denomina *hilos de núcleo*.

    ** El código y los datos de la librería residen en el espacio del núcleo, por lo que invocar una función de la misma requiere frecuentemente hacer una llamada al sistema.

En la actualidad en los diferentes sistemas operativos se pueden encontrar librerías de ambos tipos.
Por ejemplo, la librería de hilos del API Win32 es del segundo tipo mientras que la librería de hilos POSIX Threads —frecuentemente utilizada en los sistemas POSIX— puede ser de ambos tipos, dependiendo solamente del sistema donde se implementefootnote:[POSIX Threads se implementa en el núcleo en los sistemas Linux y en la mayor parte de los UNIX actuales.].

==== Modelos multihilo

Las distintas formas de implementar los hilos comentadas anteriormente —en espacio de usuario o en el núcleo— no son excluyentes ya que en un sistema operativo concreto se pueden implementar ambas, una de las dos o ninguna —esto último en el caso de los sistemas operativos que no soportan de ninguna forma múltiples hilos de ejecución—.
Así que en general debe existir una relación entre los hilos de usuario y los del núcleo.
A continuación veremos tres formas de establecer dicha relación.

===== Muchos a uno

_En un sistema operativo cuyo núcleo no soporta múltiples hilos de ejecución la única posibilidad es utilizar una librería de hilos implementada en el espacio de usuario.
El planificador de dicha librería se encarga de determinar que hilo de usuario se ejecuta en cada momento en el proceso, mientras este es planificado en la CPU por el núcleo, obviamente elegido cuando le corresponda de entre todos los procesos del sistema._

A efectos prácticos un proceso «sin hilos» se puede interpretar como un proceso con «un único hilo» de ejecución en el núcleo.
Por eso se dice que _en el modelo *muchos a uno* se mapean los múltiples hilos de usuario de un proceso en el único hilo de núcleo del mismo_ (véase la ).

Las principales características de este modelo son:

* _La gestión de hilos se hace con una librería en el espacio de usuario_, por lo que puede ser muy eficiente.
Como hemos visto anteriormente la invocación de las funciones de la librería se hace por medio de simples llamadas a funciones.

* _El proceso entero se bloquea si un hilo hace una llamada al sistema que deba ser bloqueada_.
Por ejemplo operaciones de E/S a archivos, esperar a que suceda un evento, etc.

* Como sólo un hilo de usuario puede ser asignado al hilo de núcleo, _los hilos de un mismo proceso no se pueden ejecutar en paralelo en sistemas multiprocesador_.
El planificador de la librería de hilos es el encargado de determinar que hilo de usuario es asignado al único hilo de núcleo del proceso y este sólo puede ejecutarse en una única CPU al mismo tiempo.

El problema del bloqueo de procesos puede ser evitado sustituyendo las funciones de la librería del sistema, de manera que las llamadas al sistema que se pueden bloquear sean sustituidas por versiones con llamadas equivalentes pero no bloqueantes.
Por ejemplo, las llamadas al sistema de E/S se pueden reemplazar por llamadas de E/S asíncrona, que retornan inmediatamente aunque la operación no haya sido completada.
Después de cada una de estas llamadas asíncronas al sistema, la librería del sistema invoca al planificador de la librería de hilos para que bloquee el hilo que ha realizado la llamada y asigne el hilo de núcleo a un nuevo hilo de usuario.
Obviamente el planificador de la librería de hilos debe estar al tanto de cuando las operaciones asíncronas son completadas para poder volver a planificar los hilos de usuario bloqueados.
Este procedimiento es a todas luces bastante complejo y requiere versiones no bloqueantes de todas las llamadas al sistema, así como modificar las funciones bloqueantes de la librería del sistema para implementar el comportamiento descrito.

// TODO: Destacar que esto hay que comprobarlo.

Ejemplos de implementaciones este modelo de hilos son la Green Threads, una de las implementaciones de hilos para Solaris y Java, Stackless Pythonfootnote:[Más información de Stackless Python: http://www.stackless.com/] y GNU Portable Threadsfootnote:[Más información de GNU Pthreads: http://www.gnu.org/software/pth/.].
Estas implementaciones son muy útiles en los sistemas monohilo, de cara a poder ofrecer cierto soporte de hilos a las aplicaciones, pero también en los sistemas multihilo, ya que debido a su bajo coste en recursos y a su alta eficiencia son ideales cuando la cantidad de hilos a crear —el nivel de concurrencia— va a ser previsiblemente muy alta .

===== Uno a uno

_Si el núcleo del sistema operativo soporta hilos de ejecución, lo más común es que estos sean visibles directamente en el espacio de usuario.
Por lo tanto se dice que _en el modelo *uno a uno* se mapea cada hilo de usuario en exactamente un hilo de núcleo_ (véase la ).

Las principales características de este modelo son:

* _Permite a otro hilo del mismo proceso ejecutarse aun cuando un hilo hace una llamada al sistema que debe bloquearse_, ya que el núcleo se encarga de ponerlo en espera y planificar en la CPU a otro de los hilos preparados para ejecutarse de entre todos los existentes en el sistema.

* _Permite paralelismo en sistemas multiprocesador_, ya que diferentes hilos pueden ser planificados por el núcleo en distintos procesadores.

* Crear un hilo de usuario requiere crear el correspondiente hilo de núcleo.
Debido a que la cantidad de memoria disponible para el núcleo suele estar limitada, _muchos sistemas restringen la cantidad máxima de hilos soportados_.

* _Las gestión de los hilos se hace con una librería en el espacio de núcleo_, lo que requiere utilizar llamadas al sistema.

Este modelo se utilizar en la mayor parte de los sistemas operativos multihilo modernos.
Linux, Microsoft Windows 95/98/NT/2000/XP y superiores, y Solaris 9 y superiores, son ejemplos de sistemas operativos que los utilizan.

===== Muchos a muchos

_En teoría debería ser posible aprovechar lo mejor de los dos modelos anteriores.
Por eso _en el modelo *muchos a muchos* se mapean los hilos de usuario en un menor o igual número de hilos de núcleo del proceso_ (véase la ).
Así los desarrolladores pueden utilizar la librería de hilos en el espacio de usuario para crear tantos hilos como quieran.
El planificador de la librería de hilos se encarga de determinar que hilo de usuario es asignado a que hilo de núcleo.
Mientras que el planificador de la CPU asigna la CPU a alguno de los hilos de núcleo del sistema.

* _Los hilos de núcleo pueden ser ejecutados en paralelo en sistemas multiprocesador_.

* _Permite a otro hilo del mismo proceso ejecutarse cuando un hilo hace una llamada al sistema que debe ser bloqueada_, puesto que si un hilo de usuario realiza una llamada al sistema que debe ser bloqueada, el correspondiente hilo de núcleo quedará bloqueado.
Sin embargo, el resto de los hilos de usuario pueden seguir ejecutándose en los otros hilos de núcleo.

Existe una variación del modelo muchos a muchos donde, además de hacer lo comentado anteriormente, se permite que un hilo de usuario quede ligado a un único hilo de núcleo.
Esta variación se denomina en ocasiones modelo de *dos niveles* (véase la ) y es soportada en sistemas operativos como Solaris 8 y anteriores, IRIX, HPUX y Tru64 UNIX.

Tanto en el modelo _muchos a muchos_ como en el de _dos niveles_ es necesario cierto grado de coordinación entre el núcleo y la librería de hilos del espacio de usuario.
Dicha comunicación tiene como objetivo ajustar dinámicamente el número de hilos del núcleo para garantizar la máxima eficiencia.
Uno de los esquemas de comunicación se denomina *activación del planificador* y consiste en que el núcleo informa a la librería de hilos en espacio de usuario del bloqueo de un hilo de un proceso.
Antes de dicha notificación el núcleo se encarga de crear un nuevo hilo de núcleo en el proceso, de manera que el planificador de la librería pueda encargarse de asignarle alguno de los otros hilos de usuario.
Así es como se ajusta el número de hilos dinámicamente de manera que el proceso nunca quede bloqueado.

Debido a la complejidad del mecanismo descrito anteriormente y a la dificultad de coordinar el planificador de la libraría de hilos con el de la CPU para obtener un rendimiento óptimo, sistemas como Linux y Solaris —a partir de la versión 9— han optado por el modelo uno a uno.
Con el objetivo de evitar las penalizaciones de dicho modelo, los desarrolladores de Linux han preferido concentrar sus esfuerzos en conseguir un planificador de CPU más eficiente, así como en reducir los costes de la creación de hilos de núcleo.

==== Sincronización

Hemos comentado anteriormente que los hilos comparten el espacio de direcciones del proceso al que pertenecen.
Al mismo tiempo distintos procesos pueden compartir regiones de la memoria con el objeto de cooperar en las tareas que deben desempeñar.
Ambas posibilidades introducen algunos riesgos, puesto que el acceso concurrente a los datos compartidos puede ocasionar inconsistencias.
En este apartado vamos a discutir _cómo se puede asegurar la ejecución ordenada de hilos o procesos cooperativos que comparten regiones de la memoria, con el fin de mantener la consistencia de los datos_.

// TODO: Usar una nota para decir que cuando hablemos de hilos nos referimos tambien a procesos en sistemas monohilo.

===== El problema de las secciones críticas

_Llamamos *condición de carrera* a la situación donde varios procesos o hilos pueden acceder y manipular los mismos datos concurrentemente, y donde el resultado de la ejecución depende del orden particular en el que tienen lugar dichos accesos_.
Estas situaciones ocurren frecuentemente en los sistemas operativos puesto que diferentes componentes del mismo manipulan los mismos recursos interfiriendo unos con otros.

Para ilustrarlo, supongamos que dos hilos comparten una región de la memoria que contiene un vector de elementos y un contador con el número de elementos del vector:

* El primer hilo realiza varias tareas que no entraremos a describir.
Sin embargo, como resultado de esas tareas en ocasiones añade un elemento al vector e incrementa el contador que indica el número de elementos en el vector.
Es decir, el primer hilo actúa como un *productor* de elementos del vector.
A continuación mostramos una porción de la función del productor:
+
[source, c]
----
while (count == VECTOR_SIZE);

// añadir un elemento al vector
vector[count] = item;
++count;
----

* El segundo hilo también realiza varias tareas que no describiremos.
Pero para realizar esas tareas en ocasiones debe tomar un elemento del vector compartido y decrementar el contador, porque ahora habrá un elemento menos en el vector.
Es decir, el segundo hilo actúa como un *consumidor* de elementos del vector.
A continuación mostramos una porción de la función del consumidor:
+
[source, c]
----
while (count == 0);

// quitar un elemento del vector
--count;
item = vector[count];
----

Aunque las funciones del productor y del consumidor son correctas cuando no coinciden en el tiempo, no funcionan adecuadamente cuando si lo hacen.
El motivo es que los distintos hilos comparten la variable `count` y las sentencias `++count` y `--count` no tiene porque tener una instrucción en lenguaje máquina equivalente.
Por ejemplo, `++count` podría ser traducida de la siguiente manera por el compilador:

.++count
[source, c]
----
registro1 = count;
registro1 = registro1 + 1;
count = registro1;
----

Donde `registro1` representa un registro de la CPU.
De forma parecida la sentencia `--count` puede ser implementada de la siguiente manera:

.--count
[source, c]
----
registro2 = count;
registro2 = registro2 - 1;
count = registro2;
----

Donde nuevamente `registro2` representa un registro de la CPU.
Realmente, aunque `registro1` y `registro2` pueden ser el mismo registro físico, el contenido de los registros se guarda y se recupera durante los cambios de contexto de un hilo al otro, por lo que cada uno ve sus propios valores y no los del otro.

La ejecución concurrente de las sentencias `++count` y `--count` es similar a la ejecución secuencial, pero las instrucciones de lenguaje máquina de ambas sentencias en ambos hilos o procesos pueden ser entrelazadas en algún orden aleatorio.
No olvidemos que la ejecución concurrente se puede dar bien porque:

* Tenemos un sistema multiprocesador, donde ambos hilos se ejecutan a la vez en procesadores diferentes.
* O bien porque tenemos un sistema monoprocesador, donde uno de los hilos puede ser interrumpido por el sistema operativo en cualquier momento (véase el <<_planificación_expropiativa>>) para asignar la CPU al otro.

Un posible entrelazado de las instrucciones en lenguaje máquina entre hilos, suponiendo que inicialmente `count = 5`, podría ser el siguiente:

[source, c]
----
// Entra ++count
registro1 = count;          // registro1 = 5
registro1 = registro1 + 1;  // registro1 = 6
// Sale ++count y entra --count
registro2 = count;          // registro2 = 5
registro2 = registro2 - 1;  // registro2 = 4
// Sale --count y entra ++count
count = registro1;          // count = 6 <2>
// Entra --count
count = registro2;          // count = 4 <1><2>
----
<1> Llegamos al resultado incorrecto `count = 4`, indicando que hay 4 elementos en el vector cuando realmente hay 5.
<2> Si invertimos el orden de las sentencias obtendríamos el resultado, también incorrecto, `count = 6`.

Como se puede apreciar, hemos llegado a estos valores incorrectos porque hemos permitido la manipulación concurrente de la variable `count`.
Según como se entrelacen las instrucciones de `++count` y `--count` en la CPU, el resultado final podría ser: 4, 5 o 6.
Pero el único resultado correcto es 5, que es el que obtendríamos si ejecutamos las sentencias secuencialmente.

Para evitar que estas situaciones lleven a la corrupción de los datos y a caídas de servicios y sistemas debemos asegurarnos que sólo un hilo en cada momento puede manipular recursos y variables compartidas.
Por tanto, necesitamos algún tipo de mecanismo de sincronización para que mientras se ejecuta `++count` no se pueda ejecutar `--count` ni viceversa.

Una forma de controlar el acceso a los recursos compartidos es definiendo en nuestro código _secciones críticas_.
Una *sección crítica* es una porción del código dónde se accede a variables, tablas, listas, archivos y otros recursos compartidos que no deben ser accedidos al mismo tiempo por otros hilos de ejecución.
El acceso a las secciones críticas es controlado de manera que _cuando un hilo se esté ejecutando en una sección de este tipo ningún otro pueda hacerlo en la suya correspondiente para manipular los mismos recursos_.
En estos casos se dice que la ejecución es _mutuamente exclusiva_ en el tiempo.

===== Semáforos, _mutex_ y _spinlocks_

La exclusión mutua en las secciones críticas se asegura utilizando adecuadamente una serie de recursos que para ese fin proporciona el sistema operativo.
Estos recursos utilizan internamente instrucciones y otras características de la CPU incluidas por los diseñadores para resolver este tipo de problemas.
Ese es el caso de los _semáforos_.

_Los *semáforos* son un tipo de objetos del sistema operativo que nos permite controlar el acceso a una sección crítica_, por medio de dos primitivas: `wait()` y `signal()` —o `acquire()` y `release()`, según el libro de texto—.
A continuación describimos el mecanismo de funcionamiento:

[source, cpp]
----
semaphore S(10);    // <1>

S.wait()            // <2>

 ...                // <3>

S.signal();         // <4>
----
<1> Crear el semáforo `S` inicializado a 10. Un semáforo contiene fundamentalmente un contador con el número máximo de hilos que pueden estar ejecutando el código de la sección crítica al mismo tiempo. Los semáforos con contadores inicializados a 1 se denominan *mutex* o *semáforos binarios*.

<2> Intentar entrar en la sección crítica:

    * Si el contador interno del semáforo es mayor que 0, `wait()` lo decrementa y retorna para que la ejecución continue.

    * Si el contador interno del semáforo es igual a 0, `wait()` saca al hilo de la CPU y lo pone en una cola de espera, suspendiendo así su ejecución. Básicamente, hay demasiados hilos dentro de la sección crítica.

<3> Código protegido con el semáforo. Aquí iría el código de la sección crítica en sí.

<4> Salir de la sección crítica:

    * Si el contador interno del semáforo es mayor que 0, `signal()` lo incrementa y retorna para que la ejecución continue.

    * Si el contador interno del semáforo es igual a 0, `signal()` lo incrementa y saca a uno de los hilos en la cola de espera, donde los puso `wait()`, para meterlo en la cola de preparados, dejándolo listo para entrar en la CPU. Cuando ocurra, ese hilo decrementará el contador interno del semáforo y saldrá de `wait()`, donde hasta a hora estaba atrapado. Mientras tanto `signal()` retorna y la ejecución del hilo que sale del sección crítica continua.

[NOTE]
====
Para que funcione correctamente, el semáforo S debe ser el mismo para todos los hilos que tengan secciones críticas cuya ejecución deber ser _mutuamente exclusiva_. Es decir, el semáforo S debe estar compartido entre los hilos de la misma manera que las estructuras de datos, variables y otros recursos que protege.
====

Como hemos comentado anteriormente la implementación del `wait()` y el `signal()` del semáforo debe realizarse utilizando las características proporcionadas por el hardware, de forma que el incremento, decremento y comparación del contador interno se pueda realizar de forma atómicafootnote:[Una operación o conjunto de operaciones es atómica o no interrumpible si de cara al resto del sistema parece que la operación ocurre de forma instantánea e indivisible.].

// TODO: Explicar lo de atómica y porqué.

Por otro lado existen dos alternativas desde el punto de vista de la forma en la que se implementa la espera de los hilos dentro de `wait()`:

* _El hilo puede cambiar su estado a esperado y moverse a una cola de espera asociada al semáforo_, tal y como explicamos antes.
Entonces el planificador de la CPU escogerá a otro proceso para ser ejecutado.

* _El hilo puede iterar comprobado constantemente el contador, esperando a que sea incrementado_.
Este tipo de *espera ocupada* sólo se utiliza en el caso de esperas previsiblemente cortas, puesto que se desperdician ciclos de CPU que otro hilo podría utilizar de forma más productiva.
Por eso, para evitar que las esperas ocupadas sean demasiado largas, los sistema operativos nunca expulsan de la CPU (véase el <<_planificación_expropiativa>>) a hilos que se estén ejecutando dentro de secciones críticas controladas por semáforos con este tipo de espera.

A estos semáforos con *espera ocupada* también se los denomina *spinlocks*.
Los *spinlocks* son utilizados frecuentemente para proteger las estructuras del núcleo en los sistemas multiprocesador, cuando la tarea a realizar dentro de la sección crítica en el núcleo requiere poco tiempo y es mayor el tiempo de CPU que se pierde si se saca al hilo en espera para ejecutar otro en su lugar.

==== Otras consideraciones sobre los hilos

===== Datos específicos de hilo

Los hilos de un mismo proceso comparten los datos del mismo, siendo este uno de los principales beneficios de la programación multihilo.
Por ejemplo todas las variables globales del programa son compartidas por todos los hilos.
Sin embargo en algunas ocasiones puede interesar definir ciertos datos como privados a cada hilo.
A esos datos se los denomina *TSD* o _thread-specific data_ y son soportados por muchas librerías de hilos, incluyendo el API Win32 y Pthreads, aunque no es común que sean soportados directamente por los distintos lenguajes de programación.

// TODO: Un ejemplo.

===== Cancelación de hilos

_La *cancelación* es la operación de terminar un hilo antes de que termine su trabajo_.
Por ejemplo, en un navegador web un hilo se puede encargar de la interfaz de usuario mientras otros hilos se encargan de descargar las páginas y las imágenes de la misma.
Si el usuario pulsa el botón _cancelar_ es necesario que todos los hilos que intervienen en la descarga sean cancelados.
Esto puede ocurrir de dos maneras:

* _En la *cancelación asíncrona* un hilo puede terminar inmediatamente la ejecución de otro_.
Esto puede causar problemas al no liberarse los recursos reservados al proceso por parte del hilo —no se cierran los archivos abiertos, no se libera la memoria, etc.—.
Además si el hilo que termina estaba modificando estructuras de datos que compartía con otros hilos, estas podrían quedar inconsistentes.

* _En la *cancelación en diferido* el hilo comprueba periódicamente cuando debe terminar_.
Esto da al hilo una oportunidad de terminarse así mismo de forma ordenada y en un punto dónde es seguro hacerlo.
En la terminología de Pthreads a estos puntos se los denomina *puntos de cancelación* —o _cancellation points_— y muchas llamadas al sistema lo son por si mismas.

// TODO: C++ no incluye mecanismos de cancelación.
// TODO: Ejemplo en Pthread y API Win 32.

===== Funciones reentrantes y seguras en hilos

A la hora de utilizar una librería en un programa multihilo es necesario que tengamos en cuenta los conceptos de reentrante y de seguridad de hilos:

* _Una funciónfootnote:[De ahora en adelante, cuando usemos el término función nos estaremos refiriendo a cualquier procedimiento, función, método, subprograma, subrutina o rutina del programa.] es *reentrante* si puede ser interrumpida en medio de su ejecución y mientras espera puede volver a ser llamada con total seguridad_.
Obviamente las funciones recursivas deben ser reentrantes para poder llamarse a sí mismas una y otra vez con total seguridad.
+ En el contexto de la programación multihilo ocurre una reentrada cuando, durante la ejecución de una función por parte de un hilo, este es interrumpido por el sistema operativo para planificar posteriormente a otro del mismo proceso que invoca la misma función.
En general una función es reentrante si:

    ** No modifica variables estáticas o globales.
Si lo hiciera sólo puede hacerlo mediante operaciones _leer-modificar-escribir_ que sean ininterrumpibles —es decir, atómicas—.

    ** No modifica su propio código y no llama a otras funciones que no sean reentrantes.

* _Una función es *segura en hilos* o *thread-safe* si al manipular estructuras compartidas de datos lo hace de tal manera que se garantiza la ejecución segura de la misma por múltiples hilos al mismo tiempo_.
Obviamente estamos hablando de un problema de secciones críticas, por lo que se resuelven sincronizando el acceso a estos datos mediante el uso de semáforos, _mutex_ u otros recursos similares ofrecidos por el sistema operativo.

En ocasiones ambos conceptos se confunden porque es bastante común que el código reentrante también sea seguro en hilos.
Sin embargo es posible crear código reentrante que no sea seguro en hilos y viceversa.
Por ejemplo, una función que manipule _datos específicos de hilo_ seguramente no será reentrante aunque si segura en hilos.
Mientras que una función que sólo utilice variables locales y que no invoque a otras funciones seguramente será reentrante y segura en hilos.

===== Las llamadas al sistema fork() y exec() en procesos multihilo

¿Qué debe ocurrir si un hilo de un proceso multihilo ejecuta la llamada `fork()`?:

* ¿El nuevo proceso debe duplicar todos los hilos?.
* ¿O el nuevo proceso debe tener un único hilo copia del que invocó a `fork()`?.

Como hemos comentado anteriormente la llamada al sistema `exec()` sustituye el programa en ejecución con el programa indicado y este inicia su ejecución en `main()`.
Esto incluye liberar toda la memoria reservada y la destrucción de todos los hilos del programa original, por lo que duplicar los hilos en el proceso hijo creado por `fork()` parece algo innecesario.

El estándar POSIX establece que si se utiliza `fork()` en un programa multihilo, el nuevo proceso debe ser creado con un sólo hilo, que será una réplica del que hizo la llamada, así como un duplicado completo del espacio de direcciones del proceso.
Sin embargo algunos sistemas UNIX tienen una segunda llamada no estándar, denominada `forkall()`, capaz de duplicar todos los hilos del proceso padre.
Obviamente sólo resulta conveniente emplearla si no se va a utilizar la llamada `exec()` a continuación.

// TODO: Nota con tener cuidado si exec() falla.

===== Manejo de señales en procesos multihilo

Una señal se utiliza en UNIX para informar a un proceso cuando un evento a ocurrido.
Existen dos tipos de señales:

* Las _**señales síncronas** se deben a alguna acción del propio proceso_.
Ejemplos de señales de este tipo son las originadas por accesos ilegales a memoria o divisiones por 0.
Las señales síncronas son enviadas al mismo proceso que las origina.

* Las _**señales asíncronas** son debidas a procesos externos_.
Un ejemplo de este tipo de señales es la terminación de procesos con teclas especiales como kbd:[CTRL+C] o kbd:[CTRL-D]

Las señales que llegan a un proceso pueden ser interceptadas por una función definida por el programador —que se denominada _manejador de señal_-.
En caso de que esta función no haya sido definido, se utiliza un manejador por defecto cuya acción depende del tipo de evento.

La pregunta entonces es: ¿cuándo se tienen múltiples hilos cuál de ellos debe ser interrumpido para que ejecute el manejador de señales?

* Obviamente las señales síncronas, por su propia naturaleza, deben ser enviadas al hilo que las genera.

* Con las señales asíncronas —las que vienen de fuentes externas— la cosa no está tan clara.
Dependiendo del caso algunas deben ser capturadas por un sólo hilo, mientras que otras —como aquellas que ordenan terminar el proceso— deberían ser enviadas a todos para que sepan lo que va a ocurrir.

// TODO: Y las asíncronas.

La mayor parte de los UNIX multihilo permiten especificar qué señales acepta cada hilo y cuáles no.
Por lo tanto una señal asíncrona sólo será entregada a aquellos hilos que no la bloquean.
Puesto que generalmente las señales necesitan ser manejadas una sola vez, normalmente sólo llegan al primer hilo al que se le asigna la CPU y que no las esté bloqueando.

=== Planificación de la CPU

_El *planificador de la CPU* o *planificador de corto plazo* selecciona de la cola de preparados el siguiente proceso o hilo del núcleo a ejecutar_.
En dicha cola suelen estar los PCB de todos los procesos que esperan una oportunidad para usar la CPU.
Aunque se suelen pensar en la cola de preparados como una cola FIFO, como veremos más adelante, no tiene por qué ser así.
En cualquier caso, sea cual sea el algoritmo de planificación utilizado, éste no debe ser excesivamente lento ya que es ejecutado con mucha frecuencia; aproximadamente una vez cada 100 milisegundos.

// ¿Invertir el orden? ¿Hablar de hilos apartir de ahora?

*Aunque a lo largo de este tema hablaremos de planificar procesos en la CPU, en los sistemas operativos multihilo se planifican los hilos de núcleo y no los procesos*.
Por ello todo lo que comentemos a partir de ahora se aplica de la misma manera a los hilos de núcleo, en aquellos sistemas operativos que los soportan.

==== Planificación expropiativa

Las decisiones de planificación _se deben tomar necesariamente_ en los siguientes casos:

. _Cuando un proceso pasa de *ejecutando* a **esperando**_.
Por ejemplo, por solicitar una operación de E/S, esperar a que un hijo termine, etc.

. _Cuando un proceso termina_.

Cuando el planificador es invocado en alguno de los casos anteriores decimos que tenemos un sistema operativo con *planificación cooperativa* o *no expropiativa*.

En la planificación cooperativa cuando la CPU es asignada a un proceso, dicho proceso la acapara hasta terminar o pasar al estado de _esperando_.
La planificación cooperativa no requiere de ningún hardware especial, por lo que en algunas plataformas puede ser la única opción.
Por ello estaba presente en los sistemas operativos más antiguos, como Microsoft Windows 3.1 y Mac OS.

// TODO: Indicar que hablamos del antiguo sistema operativo de MAC.

Sin embargo, las decisiones de planificación _también pueden ser tomadas en otros casos_:

. _Cuando ocurre una interrupción del temporizador_.

. _Cuando un proceso pasa de *esperando* a **preparado**_.
Por ejemplo porque para un proceso ha terminado la operación de E/S por la que estaba esperando.

Cuando el planificador es invocado en los cuatro casos decimos que tenemos planificación *expropiativa* o *apropiativa*.
La planificación expropiativa si requiere de un soporte adecuado por parte del hardware, por lo que se utiliza en la mayor parte de los sistemas operativos modernos.
Ejemplos de estos sistemas son Microsoft Windows 9x/NT/2000/XP, macOS, GNU/Linux y los UNIX modernos.

// TODO: Actualizar lo de las versiones de Windows.

La utilización de un planificador expropiativo introduce algunas dificultades adicionales:

* Puesto que un proceso puede ser expropiado en cualquier momento, el sistema operativo debe proporcionar _mecanismos de sincronización_ (véase el <<_sincronización>>) para coordinar el acceso a datos compartidos que podrían estar siendo modificados por el proceso que abandona la CPU.

* ¿Qué pasa si un proceso va a ser expropiado cuando se está ejecutando una llamada al sistema? No debemos olvidar que generalmente dentro del núcleo se manipulan datos importantes que deben permanecer consistentes en todo momento.
Para resolver esta cuestión los diseñadores pueden optar por _impedir la expropiación dentro del núcleo_.
Es decir, antes de hacer el cambio de contexto, que sacaría al proceso de la CPU, se espera a que la llamada se complete o se bloquee pasando el proceso al estado de _esperando_.
Esto permite núcleos simples y garantiza que las estructuras del mismo permanezcan consistentes, pero es un modelo pobre en sistemas de tiempo real o multiprocesador.
Exploraremos otras soluciones más adelante (véase el <<_planificación_de_tiempo_real>>).

==== El asignador

_El *asignador* es el componente que da el control de la CPU al proceso seleccionado por el planificador de corto plazo_.
Esta tarea implica realizar las siguientes funciones:

* Cambiar el contexto.

* Cambiar al modo usuario.

* Saltar al punto adecuado del programa para continuar con el proceso.

Puesto que el _asignador_ es invocado para cada conmutación entre procesos, es necesario que el tiempo que tarda en detener un proceso e iniciar otro sea lo más corto posible.
_Al tiempo que transcurre desde que un proceso es escogido para ser planificado en la CPU hasta que es asignado a la misma se lo denomina **latencia de asignación**_.

==== Criterios de planificación

Los diferentes algoritmos de planificación de la CPU tienen diversas propiedades que pueden favorecer a una clase de procesos respecto a otra.
Por ello es interesante disponer de algún criterio para poder comparar dichos algoritmos y determinar cual es el mejor.
Se han sugerido muchos criterios para comparar los algoritmos de planificación de CPU pero la elección de uno u otro puede crear una diferencia sustancial a la hora de juzgar cual es el mejor.
A continuación presentamos los criterios más comunes.

===== Criterios a maximizar

* *Uso de CPU*: Un buen _planificador debería mantener la CPU lo más ocupada posible_.
El uso de CPU es la proporción de tiempo que se usa la CPU en un periodo de tiempo determinado.
Se suele indicar en tanto por cierto.
+
[stem]
++++
bb "uso de CPU" = "tiempo que la CPU permanece ocupada" / "tiempo durante el que se toma la medida" "%"
++++

* *Tasa de procesamiento*: Cuando la CPU está ocupada es porque el trabajo se está haciendo.
Por tanto _una buena medida del volumen de trabajo realizado puede ser el número de tareas o procesos terminados por unidad de tiempo.
_A dicha magnitud es a la que denominamos como _tasa de procesamiento_.
+
[stem]
++++
bb "tasa de procesamiento" = "numero de procesos terminados" / "tiempo durante el que se toma la medida" "procesos/s"
++++

===== Criterios a minimizar

* *Tiempo de ejecución*: Es el _intervalo de tiempo que transcurre desde que el proceso es cargado hasta que termina_.

* *Tiempo de espera*: Es la _suma de tiempos que el proceso permanece a la espera en la cola de preparados_.
Evidentemente esta medida de tiempo no incluye el tiempo de espera debido a las operaciones de E/S.

* *Tiempo de respuesta*: Es _el intervalo de tiempo que transcurre desde que se le lanza un evento —se pulsa una tecla, se hace clic con el ratón o llega un paquete por la interfaz de red— hasta que se produce la primera respuesta del proceso_.
Evidentemente esto mide el tiempo que se tarda en responder y no el tiempo de E/S, mientras que el tiempo de ejecución sí suele estar limitado por la velocidad de los dispositivos E/S.

===== Elección del criterio adecuado

En función del tipo de sistema o de la clase de trabajos que se van a ejecutar puede ser conveniente medir la eficiencia del sistema usando un criterio u otro.
Esto a su vez beneficiará a unos algoritmos de planificación frente a otros, indicándonos cuáles son los más eficientes para nuestra clase de trabajos en particular.

En general podemos encontrar dos clases de trabajos para los que puede ser necesario evaluar la eficiencia del sistema de manera diferente.:

* En los sistemas interactivos —ya sean sistemas de escritorio o _mainframes_ de tiempo compartido— los procesos pasan la mayor parte del tiempo esperando algún tipo de entrada por parte de los usuarios.
En este tipo de sistemas el tiempo de ejecución no suele ser el mejor criterio para determinar la bondad de un algoritmo de planificación, ya que vendrá determinado en gran medida por la velocidad de la entrada de los usuarios.
Por el contrario se espera que el sistema reaccione lo antes posible a las órdenes recibidas, lo que hace que _el tiempo de respuesta se el criterio más adecuado_ para evaluar al planificador de la CPU.
Además el tiempo de respuesta se reduce generalmente cuando el tiempo que pasan los procesos interactivos en la cola de preparados también lo hace —tras haber sido puestos ahí por la ocurrencia de algún evento— por lo que también _puede ser una buena idea utilizar como criterio el tiempo de espera_.
Esta selección de criterios no sólo es adecuada para los sistemas interactivos, ya que existen muchos otros casos donde es interesante seleccionar un planificador de la CPU que minimice el tiempo de respuesta.
Esto por ejemplo ocurre con algunos servicios en red como: sistemas de mensajería instantánea, chats, servidores de videojuegos, etc.

* Por el contrario en los _mainframes_ de procesamiento por lotes y multiprogramados, en los superordenadores que realizan complejas simulaciones físicas y en los grandes centros de datos de proveedores de Internet como Google, lo de menos es el tiempo de respuesta y lo realmente importante es realizar cada tarea en el menor tiempo posible.
Por eso en ese tipo de sistemas _es aconsejable utilizar criterios tales como el tiempo de ejecución o la tasa de procesamiento_.

Obviamente estos criterios varían de un proceso a otro, por lo que normalmente lo que se busca es optimizar los valores promedios en el sistema.
Sin embargo no debemos olvidar que _en muchos casos puede ser más conveniente optimizar el máximo y mínimo de dichos valores antes que el promedio_.
Por ejemplo, en los sistemas interactivos es más importante minimizar la varianza en el tiempo de respuesta que el tiempo de respuesta promedio, puesto que para los usuarios un sistema con un tiempo de respuesta predecible es más deseable que uno muy rápido en promedio pero con una varianza muy alta.

==== Ciclo de ráfagas de CPU y de E/S

El éxito de la planificación de CPU depende en gran medida de la siguiente propiedad que podemos observar en los procesos: _La ejecución de un proceso consiste de ciclos de CPU y esperas de E/S, de forma que alternan entre estos dos estados.
La ejecución empieza con una ráfaga de CPU, seguida por una ráfaga de E/S, que a su vez es seguida por otra de CPU y así sucesivamente.
Finalmente la última ráfaga de CPU finaliza con una llamada al sistema —generalmente exit()— para terminar la ejecución del proceso_.

La curva que relaciona la frecuencia de las ráfagas de CPU con la duración de las mismas tiende a ser exponencial o hiper-exponencial (véase la ) aunque varía enormemente entre procesos y sistemas informáticos distintos.
Esto significa que los procesos se pueden clasificar entre aquellos que presentan un gran número de ráfagas de CPU cortas o aquellos con un pequeño número de ráfagas de CPU largas.
Concretamente:

* Decimos que un _proceso es *limitado por la E/S* cuando presenta muchas ráfagas de CPU cortas, debido a que si es así pasa la mayor parte del tiempo esperando por la E/S_.

* Decimos que un _proceso está *limitado por la CPU* cuando presenta pocas ráfagas de CPU largas, debido a que si es así hace un uso intensivo de la misma y a penas pasa tiempo esperando por la E/S_.

Esta distinción entre tipos de procesos puede ser importante en la selección de un algoritmo de planificación de CPU adecuado.
En general:

* _El algoritmo escogido debe favorecer —planificándolos antes— a los procesos limitados por la E/S_, evitando así que los procesos limitados por la CPU —que son los que tienden a usarla más tiempo— la acaparen.
Si eso ocurriera, los procesos limitados por la E/S se acumularían en la cola de preparados, dejando vacías las colas de dispositivos.
A este _fenómeno tan negativo que provoca una infrautilización de los dispositivos de E/S se lo denomina **efecto convoy**_.

* Además planificar primero a los procesos limitados por la E/S tiene dos efectos muy positivos:

    ** _Los procesos interactivos son generalmente procesos limitados por la E/S, por lo que planificarlos primero hace que mejore el tiempo de respuesta_.

    ** __Generalmente el tiempo de espera promedio se reduce cuando se planifican primero los procesos con ráfagas de CPU cortas__footnote:[En la literatura sobre algoritmos de planificación de la CPU se indica que SJF (_Shortest-Job First_) y SRTF (_Shortest-Remaing-Time First_) son los óptimos respecto al tiempo de espera promedio precisamente porque siempre escogen al proceso con la ráfaga de CPU más corta de entre los que esperan en la cola de preparados.], Según las definiciones anteriores, estos procesos son precisamente los limitados por la E/S.

==== Planificación

Hasta el momento hemos considerado la cola de preparados como una estructura donde los procesos que están preparados para ser ejecutados se ordenan y se escogen según el criterio del algoritmo de planificación.
Aunque a lo largo de todo el tema <<_gestión_de_procesos>> se puede haber intuido que dicha cola es de tipo FIFO —lo que se conoce como algoritmo de planificación FCFS o _First Come, First Served_— ya al principio del <<_planificación_de_la_cpu>> indicamos que no tiene porqué ser así pues existen muchos otros algoritmos —SJF o _Shortest-Job First_, SRTF o _Shortest-Remaing-Time First_, RR o _Round-Robin_, por prioridades, etc.— que pueden ser preferibles en función del criterio que utilicemos para evaluar la eficiencia de los mismos.

Sin embargo en los sistemas operativos modernos realmente las cosas son un poco más complejas ya que generalmente se utiliza algún tipo de *planificación con colas multinivel*.
_En este tipo de planificación _no existe una única cola de preparados sobre la que se utiliza un único algoritmo de planificación sino que_:

* _La cola de preparados se divide en varias colas separadas_ y los procesos son asignados a alguna de dichas colas en base a características de los mismos.

* _Cada cola puede tener un algoritmo de planificación de la CPU distinto_.
Es decir, alguno de los que hemos mencionado anteriormente y que se estudiarán en las clases de problemas.

* _Mediante un algoritmo determinado se debe seleccionar la cola que debe escoger al siguiente proceso a ejecutar._

Precisamente una cuestión interesante es la indicada en éste último punto ¿cómo seleccionar la cola que debe escoger al siguiente proceso que debe ser ejecutado?.

===== Prioridad fija

Aunque existen muchas maneras de clasificar los procesos entre las diferentes colas, lo más común en los sistemas operativos modernos es hacerlo en base a la prioridad de los procesos (véase la ):

* _A cada proceso se le asigna una prioridad_.

* _En la cola de preparados hay una cola para cada nivel de prioridad_.

* _Los procesos, al entrar en la cola de preparados, son insertados en aquella cola que coincide con su prioridad_.

* _El planificador escoge primero siempre la cola de prioridad más alta que no esté vacía_.

====== Definición de las prioridades

Las prioridades se suelen indicar con números enteros en un rango fijo.
Por ejemplo [0-7], [0-31], [0-139] o [0-4095].
En algunos sistemas operativos los números más grandes representan mayor prioridad, mientras que en otros son los procesos con números más pequeños los que se planifican primero.
_En éste curso utilizaremos la convención de que a menor valor mayor prioridad_.

En los sistemas con prioridad fija:

* Una vez se asigna una prioridad a un proceso ésta nunca cambia.

* _Las prioridades normalmente vienen determinadas por criterios ajenos al sistema operativo_.
Por ejemplo: la importancia del proceso, la cantidad de dinero pagada para el uso del sistema u otros factores políticos.
_A este tipo de prioridades se las denomina definidas externamente_.

====== Planificación expropiativa o cooperativa

La planificación con prioridades puede ser expropiativa o cooperativa.
_En el caso expropiativo cuando un proceso llega a la cola de preparados su prioridad es comparada con la del proceso en ejecución, de manera que el segundo es expulsado si la prioridad del primero es superior a la suya_.
Obviamente en la planificación cooperativa los nuevos procesos simplemente son insertados en la cola que les corresponde en base a su prioridad, independientemente de si tienen o no mayor prioridad que el que se esté ejecutando.

====== Planificación entre procesos con la misma prioridad

Cada cola en cada nivel de prioridad puede tener cualquier algoritmo de planificación de CPU, lo que virtualmente significa que el abanico de posibilidad es muy amplio.
Sin embargo lo más común es que los diseñadores del sistema opten por utilizar o bien el planificador FCFS o bien el RRfootnote:[Los algoritmos FCFS y RR se pueden combinar de múltiples maneras.
En algunos sistemas todas las colas son o bien FCFS o bien RR, mientras que en otros unas colas pueden ser de un tipo y otras del otro.
Por ejemplo, en el núcleo Linux las prioridades más altas —las etiquetadas como de tiempo real— tienen tanto una cola FCFS como una cola RR.
En cada prioridad primero se planifican los procesos de la cola FCFS y después lo de la cola RR.].

En la planificación *FCFS* (_First Come, First Served_) o _primero que llega, primero servido_ la cola es FIFO:

* _Los procesos que llegan se colocan al final de la cola que les corresponde_.

* _El proceso asignado a la CPU se coge siempre del principio de la cola seleccionada_.

El algoritmo *RR* (_Round-Robin_) es similar al FCFS pero utilizando el temporizador para expropiar la CPU a los procesos a intervalos regulares, alternando así entre ellos de manera que se da a todos los procesos la oportunidad de ejecutarse.
Como se puede intuir, fue diseñado para los sistemas de tiempo compartido, siendo ampliamente utilizado en cualquier sistema operativo de propósito general moderno.

El algoritmo RR requiere los siguientes elementos:

* _Se define una ventana de tiempo o *cuanto*_, generalmente entre 10 y 100 ms.

* _La cola RR se define como una cola circular dónde el planificador asigna la CPU a cada proceso en intervalos de tiempo de hasta un cuanto_.

Cuando se utilizar la planificación RR el tamaño del cuanto es un factor clave en la eficiencia del planificador:

* _Cuando se reduce el tiempo del cuanto, el tiempo de respuesta y el tiempo de espera promedio tienden a mejorar_.
Sin embargo el número de cambios de contexto será mayor, por lo que la ejecución de los procesos será mas lenta.
Además es importante tener en cuenta que interesa que el tiempo del cuanto sea mucho mayor que el tiempo del cambio de contexto; pues si por ejemplo el tiempo del cambio de contexto es un 10% del tiempo del cuanto, entonces alrededor del 10% de CPU se perdería en cambios de contexto.

* _Cuando se incrementa el tiempo del cuanto, el tiempo de espera promedio se incrementa_ dado que entonces el RR tiende a comportarse como un FCFS, que suele tener grandes tiempos de espera promedio.
Además se puede observar experimentalmente que el tiempo de ejecución promedio generalmente mejora cuantos más procesos terminan su próxima ráfaga de CPU dentro del tiempo del cuantofootnote:[Por ejemplo, dados tres procesos con una duración cada uno de ellos de 10 unidades de tiempo y cuanto igual a 1, el tiempo de ejecución promedio será de 29 unidades.
Sin embargo si el cuanto de tiempo fuera 10, el tiempo de ejecución promedio caería a 20 unidades de tiempo.].
Por lo tanto nos interesan un cuanto grande para que más procesos terminen su siguiente ráfaga dentro del mismo.

La _regla general que siguen los diseñadores es intentar que el 80% de las ráfagas de CPU sean menores que el tiempo de cuanto_.
Se busca así equilibrar los criterios anteriores, evitando que el tiempo de cuanto sea demasiado grande o demasiado cortofootnote:[De manera práctica actualmente se utilizan tiempos de cuanto de entre 10 y 100 ms.
Estos tiempos son mucho mayores que los tiempos de cambios de contexto, que generalmente son inferiores a 10µs.].

// TODO: Ejemplo con Windows

====== Muerte por inanición y otros inconvenientes

El principal problema de este tipo de planificación es el _bloqueo indefinido_ o *muerte por inanición*, puesto que el algoritmo puede dejar a los procesos de baja prioridad esperando indefinidamente si hay un conjunto de procesos de mayor prioridad demandando CPU continuamente.

Además, como vimos en el <<_ciclo_de_ráfagas_de_cpu_y_de_e_s>>, es conveniente favorecer a los procesos limitados por la E/S frente a los procesos limitados por la CPU para evitar el _efecto convoy_ y para mejorar los tiempos tanto de espera como de respuesta promedio.
Lamentablemente este tipo de planificación con _prioridad fija no es capaz de hacerlo ya que la prioridad de los procesos viene determinada exclusivamente por criterios externos al funcionamiento del sistema operativo_.

===== Prioridad dinámica

La mayor parte de los sistemas operativos modernos de propósito generalfootnote:[Microsoft Windows, macOS, Oracle/Sun Microsystems Solaris, las versiones de Linux anteriores a la 2.6.23 y, en general, casi la totalidad de los sistemas operativos modernos de propósito general utilizan este tipo de planificación de prioridades dinámicas con RR como planificador en cada prioridad.] _solucionan los inconvenientes de la planificación con prioridad fija permitiendo que la prioridad de los procesos se ajuste dinámicamente_ bajo su propio criterio:

* Por ejemplo, _una solución al problema de la muerte por inanición es utilizar un mecanismo de **envejecimiento**_ que aumente gradualmente la prioridad de los procesos mientras están esperando en la cola de preparados —por ejemplo 1 nivel de prioridad cada 15 minutos—.
De esta manera los procesos de baja prioridad tarde o temprano tendrán oportunidad de ejecutarse.
Con este mecanismo una vez consiguen ejecutarse, se les restablece su prioridad original.

* _Para favorecer en la planificación a los procesos limitados por la E/S el sistema puede añadir o quitar prioridad a los procesos, respecto a su prioridad fija, en función de medidas internas del sistema operativo_.
Por ejemplo se puede tomar en consideración: límites de tiempo, necesidades de memoria, número de archivos abiertos, la proporción entre el tiempo de ráfaga de E/S promedio y el de ráfaga de CPU promedio del proceso, etc.
Obviamente el objetivo suele ser mejorar el rendimiento del sistema priorizando unos procesos respecto a otros.

El resultado de estas políticas es que la prioridad que finalmente utiliza el sistema operativo para planificar los procesos en un valor calculado dinámicamente a partir de intereses externos y medidas internas.
Por lo tanto los procesos pueden cambiar múltiples veces de cola durante su tiempo de vida.
_A la planificación de múltiples niveles donde los procesos pueden cambiar de una cola a otra se la denomina **planificación con colas multinivel realimentadas**_.

// TODO: Más del ejemplo de Windows.

===== Planificación por reparto proporcional

Hasta el momento hemos hablado de planificadores que se concentran en cuál es el proceso más importante que debe ser ejecutado en cada instante.
Sin embargo otra opción, desde el punto de vista de la planificación ,es repartir el tiempo de CPU entre los procesos a un ritmo controlado.
Esto es precisamente lo que hace _la *planificación equitativa* (Fair Scheduling) que intenta repartir por igual el tiempo de CPU entre los procesos de la cola de preparados_.
Por ejemplo, si 4 procesos compiten por el uso de la CPU, el planificador asignará un 25%
del tiempo de la misma a cada uno.
Si a continuación un usuario iniciase un nuevo proceso, el planificador tendría que ajustar el reparto asignando un 20% del tiempo a cada uno.
El algoritmo de planificación equitativa es muy similar al algoritmo RR pero, a diferencia de este último en el que se utiliza un cuanto de tamaño fijo, _la ventana de tiempo se calcula de dinámicamente para garantizar el reparto equitativo de la CPU_.

Al igual que en los algoritmos anteriores, en ocasiones puede ser interesante priorizar unos procesos frente a otros, tanto por motivos ajenos al sistema operativo como por motivos internos.
Por ejemplo se puede querer favorecer a los procesos limitados por la E/S para mejorar la eficiencia del sistema, tal y como comentamos en el apartado <<_ciclo_de_ráfagas_de_cpu_y_de_e_s>>.
La _planificación equitativa_ resuelve este problema asignando proporcionalmente más tiempo de CPU a los procesos con mayor prioridad.
__A esta generalización del planificador equitativo se la conoce como **planificador equitativo ponderado**__footnote:[Linux desde la versión 2.6.23 utiliza un tipo de *planificador equitativo ponderado* denominado *CFS* (_Completely Fair Scheduler_) o *planificador completamente justo.*].

// TODO: Ejemplo de Linux.

==== Planificación de tiempo real

En el <<_sistemas_de_tiempo_real>> discutimos la importancia de los sistemas de tiempo real.
A continuación, describiremos las funcionalidades necesarias para soportar la ejecución de procesos en tiempo real dentro de un sistema operativo de propósito general.

===== Tiempo real estricto

Los sistemas de *tiempo real estricto* son necesarios para realizar tareas críticas que deben ser completadas dentro de unos márgenes de tiempo preestablecidos.
Generalmente las tareas son entregas al sistema operativo junto con una declaración de las restricciones de tiempo —periodicidad y límite de tiempo— y la cantidad de tiempo que necesitan para ejecutarse.
El planificador sólo admitirá las tareas si puede garantizar el cumplimiento de las restricciones de tiempo, rechazándolas en caso contrario.
El proporcionar estas garantías requiere que el planificador conozca exactamente el tiempo máximo que se tarda en realizar todas y cada una de las funciones del sistema operativo.
Esto es imposible en sistemas con almacenamiento secundario o memoria virtual, ya que introducen variaciones no controladas en la cantidad de tiempo necesario para ejecutar una tarea.
Por tanto, el _tiempo real estricto no es compatible con los sistemas operativos de propósito general_, como los de tiempo compartido.

===== Tiempo real flexible

La ejecución de procesos de *tiempo real flexible* es menos restrictiva.
Tan sólo requiere que los procesos críticos reciban mayor prioridad que los que no lo son.
Esto es compatible con los sistemas de tiempo compartido, aunque _puede generar excesos en la cantidad de recursos asignados a los procesos de tiempo real, así como inanición y grandes retardos en la ejecución del resto de los procesos_.
Sin embargo esto nos permite conseguir sistemas de propósito general que soporten multimedia, videojuegos y otras tareas que no funcionarían de manera aceptable en un entorno que no implementara tiempo real flexible.
Por ello la mayor parte de los sistemas operativos modernos soportan este tipo de tiempo real.

Implementar el soporte de tiempo real flexible en un sistema operativo de propósito general requiere:

    * Sistema operativo con planificación con prioridades.
__Los procesos de tiempo real deben tener la mayor prioridad.
Además, no deben ser afectados por ningún mecanismo de envejecimiento o bonificación__footnote:[Linux, Microsoft Windows y la mayor parte de los sistemas operativos modernos de propósito general dividen el rango de prioridades en dos partes.
El conjunto de prioridades más altas son prioridades de tiempo real y por tanto son fijas.
Mientras que el grupo de prioridades más bajas son de tiempo no real y dinámicas.
Además el planificador se implementa de tal manera que un proceso con prioridad dinámica nunca puede alcanzar el rango de prioridades de tiempo real.], que sí puede afectar a los procesos de tiempo no real.

    * _Baja latencia de asignación_.
Cuanto menor es la latencia más rápido comenzará a ejecutarse el proceso de tiempo real después de ser seleccionado por el planificador de la CPU.

Mientras que el primer requerimiento es bastante sencillo de conseguir, el segundo es mucho más complejo.
Muchos sistemas operativos tienen un núcleo no expropiable.
Estos núcleos no pueden realizar un cambio de contexto mientras se está ejecutando código del núcleo —por ejemplo debido a una llamada al sistema— por lo que se ven obligados a esperar hasta que la tarea que se esté realizando se termine antes de asignar la CPU a otro proceso.
Esto aumenta la _latencia de asignación_ dado que algunas llamadas al sistema pueden ser muy complejas y requerir mucho tiempo para ser completadas.
Con el objetivo de resolverlo existen diversas alternativas:

====== Puntos de expropiación

Una posibilidad es _hacer que el código del núcleo sea expropiable_.
Esto se consigue introduciendo *puntos de expropiación* en diversos lugares _seguros_ dentro del código.
En dichos puntos se comprueba si algún proceso de prioridad más alta está en la cola de preparados.
En caso de que sea así se expropia la CPU al proceso actual y se le asigna al proceso de más alta prioridad.

Debido a la función que realizan los puntos de expropiación, sólo pueden ser colocados en lugares seguros del código del núcleo.
Es decir, sólo pueden estar situados allí donde no se interrumpe la modificación de estructuras de datos.
Sin embargo esto limita el número de puntos que pueden ser colocados, por lo que la latencia de asignación puede seguir siendo muy alta para algunas tareas muy complejas del código del núcleo.

// TODO: Ejemplo de Linux.

====== Núcleo expropiable

Otra posibilidad es _diseñar un núcleo completamente expropiable_.
Puesto que en este caso la ejecución de cualquier tarea en el núcleo puede ser interrumpida en cualquier momento por procesos de mayor prioridad —que el que actualmente tiene asignada la CPU— es necesario proteger las estructuras de datos del núcleo con mecanismos de sincronización, lo que hace que el diseño de un núcleo de estas características sea mucho más complejo.

Supongamos que un proceso de baja prioridad es interrumpido, porque hay un proceso de alta prioridad en la cola de preparados, mientras accede a una importante estructura de datos del núcleo.
Durante su ejecución el proceso de alta prioridad podría intentar acceder a la misma estructura que manipulaba el proceso de baja prioridad cuando fue interrumpido.
Debido al uso de mecanismos de sincronización el proceso de alta prioridad tendría que abandonar la CPU a la espera de que el de baja libere el acceso.
Sin embargo este tardará en ser asignado a la CPU mientras haya algún otro proceso de alta prioridad en la cola de preparados.
Además otros procesos puede irse añadiendo a la cola de espera del mecanismo de sincronización que regula el acceso a la estructura de datos del núcleo.
Al hecho de que un proceso de alta prioridad tenga que esperar por uno de baja se le conoce como *inversión de la prioridad*.
Para resolverlo se utiliza un *protocolo de herencia de la prioridad* dónde un proceso de baja prioridad hereda la prioridad del proceso de más alta prioridad que espera por un recurso al que el primero está accediendo.
En el momento en que el proceso de baja prioridad libere el acceso a dicho recurso, su prioridad retornará a su valor original.

Linux 2.6, Solaris y Microsoft Windows NT/2000/XP son algunos ejemplos de sistemas operativos con núcleos expropiables.
En el caso concreto de Solaris la latencia de asignación es inferior a 1 ms.
mientras que con la expropiación del núcleo desactivada ésta puede superar los 100 ms.

Lamentablemente el _conseguir baja latencia de asignación no tiene coste cero_.
El hecho de que el núcleo sea expropiable aumenta el número de cambios de contexto, lo que reduce el rendimiento del sistema a cambio de una mejor respuesta.
Por ello resulta muy interesante para aplicaciones de tiempo real, multimedia y sistemas interactivos pero es poco adecuado para servidores y computación de alto rendimiento.
Es por eso que Linux 2.6 permite escoger entre tener un núcleo expropiativo, usar puntos de expropiación o nada de lo anterior.
De esta forma Linux está preparado tanto para servidores como para sistemas de escritorio o de tiempo real.

// TODO: Ejemplo de Linux

==== Planificación en sistemas multiprocesador

Para tratar el problema de la planificación en los sistemas multiprocesador nos limitaremos al caso de los __sistemas homogéneos__footnote:[Un ejemplo de lo contrario —de sistema heterogéneo— se puede observar en los PC modernos donde muchos disponen tanto de una CPU como de una GPU especializada en el procesamiento de gráficos y en las operaciones de coma flotante.].
En dichos sistemas los procesadores son idénticos, por lo que cualquiera de ellos puede ejecutar cualquier proceso.
Esto es bastante común y simplifica el problema de la planificación.
Aun así no debemos olvidar que incluso en el caso de los sistemas homogéneos pueden aparecer limitaciones en la planificación.
Por ejemplo:

    * Un dispositivo de E/S puede estar conectado mediante un bus privado a un procesador en particular.
En ese caso los procesos que quieren utilizar ese dispositivo deben ejecutarse en dicho procesador.

* Los procesadores SMTfootnote:[El _HyperThreading_ disponible en algunos procesadores de Intel es una implementación de la tecnología _Simultaneous Multithreading_.] (_Simultaneous Multithreading_) permiten la ejecución concurrente de varios hilos como si de varias CPU se tratara.
Sin embargo, al no disponer cada hilo de una CPU completa es posible que algunos deban esperar a que algún otro libere unidades de la CPU que le son necesarias.
Eso debe ser tenido en cuenta por el planificador con el fin de optimizar el rendimiento del sistema.

Al margen de estas cuestiones, existen diversas posibilidades a la hora de enfrentar el problema de la planificación en un sistema multiprocesador:

    * Cuando utilizamos **multiprocesamiento asimétrico**footnote:[En los sistemas de _multiprocesamiento asimétrico_ hay una CPU maestra y varias esclavas a quienes la primera entrega el trabajo.
En ocasiones las CPU esclavas se distinguen por haber sido diseñadas para realizar algún tipo de trabajo de forma eficiente —como es el caso las GPU, que no son sino CPU diseñadas para el procesamiento de gráficos— o por el hardware al que están conectadas —como por ejemplo las CPU unidas a discos para gestionarlos—.] todas las decisiones de planificación, procesamiento de E/S y otras actividades son gestionadas por un único procesador, el _servidor_ o _maestro_.
El resto de procesadores se limitan a ejecutar el código de usuarios que les es asignado.
Este esquema _es sencillo puesto que evita la necesidad de compartir estructuras de datos entre el código que se ejecuta en los procesadores_.

* Cuando utilizamos **multiprocesamiento simétrico**footnote:[En los sistemas de _multiprocesamiento simétrico_ o _SMP_ (_Symmetric Multiprocessing_) todos los procesadores son iguales.
Todos comparten los mismos recursos, pueden acceder a los mismos dispositivos y cada uno ejecuta una copia del núcleo del sistema operativo.
Por lo tanto el sistema operativo debe saber compartir los recursos y repartir la carga entre las CPU.
Casi todos los sistemas multiprocesador modernos son de este tipo.] o _SMP_ cada procesador ejecuta su propia copia del núcleo del sistema operativo y se auto-planifica mediante su propio planificador de CPU.
En estos sistemas nos podemos encontrar con varias alternativas:

    ** Algunos sistemas disponen de _una cola de preparados común para todos los procesadores_.
Puesto que se mira en una única cola, _todos los procesos pueden ser planificados en cualquier procesador_.
Este esquema requiere el uso mecanismos de sincronización debido a que hay estructuras de datos que se comparten entre todos los núcleos.
En caso contrario varios procesadores podrían escoger y ejecuta el mismo proceso a la vez.

    ** Por el contrario otros sistemas disponen de _una cola de preparados para cada procesador_.
El mayor inconveniente de esta solución es que puede generar desequilibrios entre los procesadores, ya que un procesador puede acabar desocupado —con la cola de preparados vacía— mientras otro está muy ocupado.

Muchos sistemas operativos modernos implementan el esquema SMP con una cola de preparados común.
Esto incluye Microsoft Windows NT/2000/XP, Solaris, macOS y versiones anteriores a Linux 2.6.
Sin embargo, esta solución presenta algunos inconvenientes:

* La posibilidad de que un proceso se pueda ejecutar en cualquier CPU —aunque parezca beneficiosa— es negativa desde el punto de vista de que dejan de ser útiles las cachés de los procesadores, penalizando notablemente el rendimiento del sistema.
Por eso realmente la mayoría de los sistemas operativos de este tipo intenta evitar la migración de procesos de un procesador a otro.
A esto se lo conoce con el nombre de *afinidad al procesador*.

* Los mecanismos de sincronización requeridos para controlar el acceso a la cola de preparados pueden mantener a los procesadores mucho tiempo desocupados —mientras esperan— en sistemas con un gran número de procesadores y con muchos procesos a la espera de ser ejecutados.

Cada vez más sistemas modernos —incluido Linux 2.6— están optando por utilizar el esquema SMP con una cola de preparados por procesador.
De esta manera, al no utilizar mecanismos de sincronización, se eliminan los tiempos de espera para acceder a la cola de preparados y escoger un nuevo proceso.
Sin embargo, con el fin de mantener la carga de trabajo equilibrada entre todos los procesadores es necesario disponer de algunos mecanismos de *balanceo de carga*.
Por ejemplo:

* En la *migración comandada* o _push migration_ un tarea específica —que se ejecuta con menor frecuencia que el planificador de la CPU— estima la carga de trabajo de cada CPU y en caso de encontrar algún desequilibrio mueve algunos procesos de la cola de preparados de unos procesadores a la de los otros * En la *migración solicitada* o _pull migration_ un procesador inactivo extrae de la cola de preparados de un procesador ocupado alguna tarea que esté esperando.

Tanto el planificador de Linux 2.6 como el planificador ULE, disponible en los sistemas FreeBSD, implementan ambas técnicas.
Mientras que en Microsoft Windows, a partir de Windows Vista, sólo se hace uso de la _migración solicitada_.

// TODO: Para integrar con lo anterior.

Para ilustrar los visto hasta el momento sobre la planificación de la CPU en sistemas operativos modernos, vamos a estudiar las principales características de las últimas versiones de Microsoft Windows a este respecto.

Las actuales versiones de sistemas operativos Windows se agrupan dentro de la familia Microsoft Windows NT; que nació con el sistema operativo Windows NT 3.1 en 1993 y que llega hasta hoy en día con Microsoft Windows 8.1 y Windows Server 2012 R2 —que se corresponden con la versión 6.3 de dicha familia Windows NT—

El núcleo de la familia _Windows NT_ es multihilo e internamente implementa un algoritmo de planificación expropiativa con colas multinivel realimentadas basado en prioridades:

Como cualquier sistema operativo moderno, el núcleo de Windows es expropiable —lo que sabemos que ofrece latencias de asignación más bajas que si no lo fuera— y soporta tiempo real flexible:

Respecto a esto último, en Windows los programadores o administradores del sistema pueden utilizar el API para establecer la prioridad de los hilos.
Sin embargo sobre estas preferencias el núcleo aplica ciertas bonificaciones para obtener la prioridad real; combinando diferentes criterios para reducir la latencia, mejorar la respuesta —obviamente a través de beneficiar a los hilos limitados por E/S— evitar la muerte por inanición y la inversión de prioridad.
Estas bonificaciones pueden ocurrir en los siguientes casos:

Respecto al tiempo de cuanto, desde Windows Vista —NT 6.0— no se usa el temporizador para controlarlo sino un contador de ciclos de reloj de la CPUfootnote:[Desde el Intel Pentium las CPU de la familia x86 incorporan un contador de marca de tiempo (Time Stamp Counter o TSC) de 64 bits que indica el número de ciclos transcurridos desde el último _reset_ del procesador.].
Así el sistema puede determinar con precisión el tiempo que se hay estado ejecutando un hilo, sin incluir los tiempos dedicados a otras cuestiones, como por ejemplo a manejar interrupciones.

En Windows los hilos se insertan en la cabeza de su cola —no en el final— y conservan lo que les queda de cuanto, cuando son expropiados.
Mientras que se insertan por el final con el valor de cuanto reiniciado, cuando abandonan la CPU por haber agotado el cuanto anterior.

En Windows las prioridades de los procesos se pueden ver desde dos perspectivas: la del API de Windows y la del núcleo.
Esta última es la que hemos estudiado en el apartado anterior.
Mientras que el API tiene una organización muy diferente que en última instancia debe ser mapeada a las prioridades numéricas del núcleo de Windows.

El API organiza los procesos por clases de prioridad: Tiempo real (15), Alta (10), Arriba de lo normal (9), Normal (8), Debajo de lo normal (7), Baja (6) y Reposo (1) .
Al tiempo que cada hilo tiene una prioridad relativa: De tiempo crítico (15), Más alta (2), Arriba de lo normal (1), Normal (0), Debajo de lo normal (—1), Más baja (—2) y Reposo (—15).
Por lo que la prioridad interna de cada hilo, desde el punto de vista del núcleo, es el resultado de sumar la prioridad base obtenida a partir de la clase de prioridad del proceso con la prioridad relativa del hilo en cuestión.
